"""
ëŒ€ì‹œë³´ë“œ ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸

ë‹´ë‹¹ ê¸°ëŠ¥:
- ì „êµ­ í‰ë‹¹ê°€ ë° ê±°ë˜ëŸ‰ ì¶”ì´ ì¡°íšŒ
- ì›”ê°„ ì•„íŒŒíŠ¸ ê°’ ì¶”ì´ ì¡°íšŒ
- ë­í‚¹ ì¡°íšŒ (ìš”ì¦˜ ê´€ì‹¬ ë§ì€ ì•„íŒŒíŠ¸, ìƒìŠ¹ë¥ , í•˜ë½ë¥  TOP 5)
"""
import logging
import asyncio
from datetime import date, datetime, timedelta
from typing import Optional, List, Dict, Any
from fastapi import APIRouter, Depends, HTTPException, Query, status
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func, and_, or_, case, desc, text
from sqlalchemy.orm import selectinload

from app.api.v1.deps import get_db
from app.models.apartment import Apartment
from app.models.sale import Sale
from app.models.rent import Rent
from app.models.state import State
from app.utils.cache import get_from_cache, set_to_cache, build_cache_key

logger = logging.getLogger(__name__)

router = APIRouter()


@router.get(
    "/regional-heatmap",
    response_model=dict,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Dashboard (ëŒ€ì‹œë³´ë“œ)"],
    summary="ì§€ì—­ë³„ ìƒìŠ¹ë¥  íˆíŠ¸ë§µ ë°ì´í„° ì¡°íšŒ",
    description="""
    ë„/íŠ¹ë³„ì‹œ/ê´‘ì—­ì‹œ ë‹¨ìœ„ë¡œ ì§€ì—­ë³„ ê°€ê²© ìƒìŠ¹ë¥ ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ### ì œê³µ ë°ì´í„°
    - ì§€ì—­ëª… (ë„/íŠ¹ë³„ì‹œ/ê´‘ì—­ì‹œ)
    - ê°€ê²© ìƒìŠ¹ë¥  (%)
    - í‰ê·  ê°€ê²© (ë§Œì›/í‰)
    
    ### Query Parameters
    - `transaction_type`: ê±°ë˜ ìœ í˜• (sale: ë§¤ë§¤, jeonse: ì „ì„¸, ê¸°ë³¸ê°’: sale)
    - `months`: ë¹„êµ ê¸°ê°„ (ê°œì›”, ê¸°ë³¸ê°’: 3)
    """
)
async def get_regional_heatmap(
    transaction_type: str = Query("sale", description="ê±°ë˜ ìœ í˜•: sale(ë§¤ë§¤), jeonse(ì „ì„¸)"),
    months: int = Query(3, ge=1, le=12, description="ë¹„êµ ê¸°ê°„ (ê°œì›”)"),
    db: AsyncSession = Depends(get_db)
):
    """
    ì§€ì—­ë³„ ìƒìŠ¹ë¥  íˆíŠ¸ë§µ ë°ì´í„° ì¡°íšŒ
    
    ë„/íŠ¹ë³„ì‹œ/ê´‘ì—­ì‹œ ë‹¨ìœ„ë¡œ ê°€ê²© ìƒìŠ¹ë¥ ì„ ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    cache_key = build_cache_key("dashboard", "regional-heatmap", transaction_type, str(months))
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        return cached_data
    
    try:
        logger.info(f"ğŸ” [Dashboard Heatmap] ì§€ì—­ë³„ íˆíŠ¸ë§µ ë°ì´í„° ì¡°íšŒ ì‹œì‘ - transaction_type: {transaction_type}, months: {months}")
        
        trans_table = get_transaction_table(transaction_type)
        price_field = get_price_field(transaction_type, trans_table)
        date_field = get_date_field(transaction_type, trans_table)
        
        # í•„í„° ì¡°ê±´
        if transaction_type == "sale":
            base_filter = and_(
                trans_table.is_canceled == False,
                (trans_table.is_deleted == False) | (trans_table.is_deleted.is_(None)),
                trans_table.trans_price.isnot(None),
                trans_table.exclusive_area.isnot(None),
                trans_table.exclusive_area > 0
            )
        else:  # jeonse
            base_filter = and_(
                or_(
                    trans_table.monthly_rent == 0,
                    trans_table.monthly_rent.is_(None)
                ),
                (trans_table.is_deleted == False) | (trans_table.is_deleted.is_(None)),
                trans_table.deposit_price.isnot(None),
                trans_table.exclusive_area.isnot(None),
                trans_table.exclusive_area > 0
            )
        
        # ì‹¤ì œ ë°ì´í„°ì˜ ë‚ ì§œ ë²”ìœ„ í™•ì¸
        date_range_stmt = select(
            func.min(date_field).label('min_date'),
            func.max(date_field).label('max_date')
        ).where(
            and_(
                base_filter,
                date_field.isnot(None)
            )
        )
        date_range_result = await db.execute(date_range_stmt)
        date_range = date_range_result.first()
        
        if not date_range or not date_range.min_date or not date_range.max_date:
            logger.warning(f"âš ï¸ [Dashboard Heatmap] ë‚ ì§œ ë²”ìœ„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - ë¹ˆ ë°ì´í„° ë°˜í™˜")
            return {
                "success": True,
                "data": []
            }
        
        max_date = date_range.max_date
        min_date = date_range.min_date
        
        # ìµœê·¼ ê¸°ê°„: ìµœëŒ€ ë‚ ì§œë¡œë¶€í„° months ê°œì›” ì „
        recent_start = max_date - timedelta(days=months * 30)
        # ì´ì „ ê¸°ê°„: recent_startë¡œë¶€í„° months ê°œì›” ì „
        previous_start = recent_start - timedelta(days=months * 30)
        
        # ë‚ ì§œ ë²”ìœ„ê°€ ë°ì´í„° ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ ì¡°ì •
        if previous_start < min_date:
            previous_start = min_date
        if recent_start < min_date:
            recent_start = min_date + timedelta(days=months * 30)
            previous_start = min_date
        
        logger.info(f"ğŸ“… [Dashboard Heatmap] ë‚ ì§œ ë²”ìœ„ - min_date: {min_date}, max_date: {max_date}, previous_start: {previous_start}, recent_start: {recent_start}, recent_end: {max_date}")
        
        # ë„/íŠ¹ë³„ì‹œ/ê´‘ì—­ì‹œ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™” (city_name ì‚¬ìš©)
        # ìµœê·¼ ê¸°ê°„ í‰ê·  ê°€ê²©
        recent_prices_stmt = (
            select(
                State.city_name,
                func.avg(price_field / trans_table.exclusive_area * 3.3).label('avg_price_per_pyeong'),
                func.count(trans_table.trans_id).label('transaction_count')
            )
            .join(Apartment, trans_table.apt_id == Apartment.apt_id)
            .join(State, Apartment.region_id == State.region_id)
            .where(
                and_(
                    base_filter,
                    date_field.isnot(None),
                    date_field >= recent_start,
                    date_field <= max_date,
                    (Apartment.is_deleted == False) | (Apartment.is_deleted.is_(None)),
                    trans_table.exclusive_area.isnot(None),
                    trans_table.exclusive_area > 0
                )
            )
            .group_by(State.city_name)
            .having(func.count(trans_table.trans_id) >= 5)  # ìµœì†Œ 5ê±´ ì´ìƒ
        )
        
        # ì´ì „ ê¸°ê°„ í‰ê·  ê°€ê²©
        previous_prices_stmt = (
            select(
                State.city_name,
                func.avg(price_field / trans_table.exclusive_area * 3.3).label('avg_price_per_pyeong')
            )
            .join(Apartment, trans_table.apt_id == Apartment.apt_id)
            .join(State, Apartment.region_id == State.region_id)
            .where(
                and_(
                    base_filter,
                    date_field.isnot(None),
                    date_field >= previous_start,
                    date_field < recent_start,
                    (Apartment.is_deleted == False) | (Apartment.is_deleted.is_(None)),
                    trans_table.exclusive_area.isnot(None),
                    trans_table.exclusive_area > 0
                )
            )
            .group_by(State.city_name)
            .having(func.count(trans_table.trans_id) >= 5)  # ìµœì†Œ 5ê±´ ì´ìƒ
        )
        
        recent_result, previous_result = await asyncio.gather(
            db.execute(recent_prices_stmt),
            db.execute(previous_prices_stmt)
        )
        
        # ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì„¸ì…˜ ì¢…ë£Œ ì „ì— ë°ì´í„° ê°€ì ¸ì˜¤ê¸°)
        recent_rows = recent_result.fetchall()
        previous_rows = previous_result.fetchall()
        
        # ì´ì „ ê¸°ê°„ ê°€ê²© ë”•ì…”ë„ˆë¦¬
        previous_prices = {row.city_name: float(row.avg_price_per_pyeong or 0) for row in previous_rows}
        
        # ìµœê·¼ ê¸°ê°„ ë°ì´í„° ì²˜ë¦¬ ë° ìƒìŠ¹ë¥  ê³„ì‚°
        heatmap_data = []
        for row in recent_rows:
            city_name = row.city_name
            recent_avg = float(row.avg_price_per_pyeong or 0)
            transaction_count = row.transaction_count or 0
            
            if city_name not in previous_prices or previous_prices[city_name] == 0:
                continue
            
            previous_avg = previous_prices[city_name]
            change_rate = ((recent_avg - previous_avg) / previous_avg) * 100
            
            heatmap_data.append({
                "region": city_name,
                "change_rate": round(change_rate, 2),
                "avg_price_per_pyeong": round(recent_avg, 1),
                "transaction_count": transaction_count
            })
        
        # ìƒìŠ¹ë¥  ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ê³  TOP 5ë§Œ ë°˜í™˜
        heatmap_data.sort(key=lambda x: x["change_rate"], reverse=True)
        heatmap_data = heatmap_data[:5]  # TOP 5ë§Œ ë°˜í™˜
        
        logger.info(f"âœ… [Dashboard Heatmap] íˆíŠ¸ë§µ ë°ì´í„° ìƒì„± ì™„ë£Œ - ì§€ì—­ ìˆ˜: {len(heatmap_data)} (TOP 5)")
        
        response_data = {
            "success": True,
            "data": heatmap_data
        }
        
        # ìºì‹œì— ì €ì¥ (TTL: 30ë¶„)
        if len(heatmap_data) > 0:
            await set_to_cache(cache_key, response_data, ttl=1800)
        
        return response_data
        
    except Exception as e:
        logger.error(f"âŒ [Dashboard Heatmap] ì§€ì—­ë³„ íˆíŠ¸ë§µ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/regional-trends",
    response_model=dict,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Dashboard (ëŒ€ì‹œë³´ë“œ)"],
    summary="ì§€ì—­ë³„ ì§‘ê°’ ë³€í™” ì¶”ì´ ì¡°íšŒ",
    description="""
    ë„/íŠ¹ë³„ì‹œ/ê´‘ì—­ì‹œ ë‹¨ìœ„ë¡œ ì§€ì—­ë³„ ì§‘ê°’ ë³€í™” ì¶”ì´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ### ì œê³µ ë°ì´í„°
    - ì§€ì—­ëª…ë³„ ì›”ë³„ í‰ê·  ê°€ê²© ì¶”ì´
    
    ### Query Parameters
    - `transaction_type`: ê±°ë˜ ìœ í˜• (sale: ë§¤ë§¤, jeonse: ì „ì„¸, ê¸°ë³¸ê°’: sale)
    - `months`: ì¡°íšŒ ê¸°ê°„ (ê°œì›”, ê¸°ë³¸ê°’: 12)
    """
)
async def get_regional_trends(
    transaction_type: str = Query("sale", description="ê±°ë˜ ìœ í˜•: sale(ë§¤ë§¤), jeonse(ì „ì„¸)"),
    months: int = Query(12, ge=1, le=24, description="ì¡°íšŒ ê¸°ê°„ (ê°œì›”)"),
    db: AsyncSession = Depends(get_db)
):
    """
    ì§€ì—­ë³„ ì§‘ê°’ ë³€í™” ì¶”ì´ ì¡°íšŒ
    
    ë„/íŠ¹ë³„ì‹œ/ê´‘ì—­ì‹œ ë‹¨ìœ„ë¡œ ì›”ë³„ í‰ê·  ê°€ê²© ì¶”ì´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    cache_key = build_cache_key("dashboard", "regional-trends", transaction_type, str(months))
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        return cached_data
    
    try:
        logger.info(f"ğŸ” [Dashboard Trends] ì§€ì—­ë³„ ì¶”ì´ ë°ì´í„° ì¡°íšŒ ì‹œì‘ - transaction_type: {transaction_type}, months: {months}")
        
        trans_table = get_transaction_table(transaction_type)
        price_field = get_price_field(transaction_type, trans_table)
        date_field = get_date_field(transaction_type, trans_table)
        
        # í•„í„° ì¡°ê±´
        if transaction_type == "sale":
            base_filter = and_(
                trans_table.is_canceled == False,
                (trans_table.is_deleted == False) | (trans_table.is_deleted.is_(None)),
                trans_table.trans_price.isnot(None),
                trans_table.exclusive_area.isnot(None),
                trans_table.exclusive_area > 0
            )
        else:  # jeonse
            base_filter = and_(
                or_(
                    trans_table.monthly_rent == 0,
                    trans_table.monthly_rent.is_(None)
                ),
                (trans_table.is_deleted == False) | (trans_table.is_deleted.is_(None)),
                trans_table.deposit_price.isnot(None),
                trans_table.exclusive_area.isnot(None),
                trans_table.exclusive_area > 0
            )
        
        # ì‹¤ì œ ë°ì´í„°ì˜ ë‚ ì§œ ë²”ìœ„ í™•ì¸
        date_range_stmt = select(
            func.min(date_field).label('min_date'),
            func.max(date_field).label('max_date')
        ).where(
            and_(
                base_filter,
                date_field.isnot(None)
            )
        )
        date_range_result = await db.execute(date_range_stmt)
        date_range = date_range_result.first()
        
        if not date_range or not date_range.min_date or not date_range.max_date:
            logger.warning(f"âš ï¸ [Dashboard Trends] ë‚ ì§œ ë²”ìœ„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - ë¹ˆ ë°ì´í„° ë°˜í™˜")
            return {
                "success": True,
                "data": []
            }
        
        # ë°ì´í„°ê°€ ìˆëŠ” ê¸°ê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚ ì§œ ë²”ìœ„ ì„¤ì •
        end_date = date_range.max_date
        # ìµœëŒ€ 1ë…„ ì „ë¶€í„°, ë˜ëŠ” ë°ì´í„°ì˜ ì‹œì‘ì¼ë¶€í„°
        start_date = max(
            date_range.min_date,
            end_date - timedelta(days=365)
        )
        
        logger.info(f"ğŸ“… [Dashboard Trends] ë‚ ì§œ ë²”ìœ„ - min_date: {date_range.min_date}, max_date: {date_range.max_date}, start_date: {start_date}, end_date: {end_date}")
        
        # ì›”ë³„ ê·¸ë£¹í™” í‘œí˜„ì‹
        month_expr = func.to_char(date_field, 'YYYY-MM')
        
        # ì§€ì—­ë³„ ì›”ë³„ í‰ê·  ê°€ê²© ì¡°íšŒ (1ë…„ ì „ë¶€í„° ì˜¤ëŠ˜ê¹Œì§€)
        regional_trends_stmt = (
            select(
                State.city_name,
                month_expr.label('month'),
                func.avg(price_field / trans_table.exclusive_area * 3.3).label('avg_price_per_pyeong'),
                func.count(trans_table.trans_id).label('transaction_count')
            )
            .join(Apartment, trans_table.apt_id == Apartment.apt_id)
            .join(State, Apartment.region_id == State.region_id)
            .where(
                and_(
                    base_filter,
                    date_field.isnot(None),
                    date_field >= start_date,  # 1ë…„ ì „ë¶€í„°
                    date_field <= end_date,  # ì˜¤ëŠ˜ê¹Œì§€
                    (Apartment.is_deleted == False) | (Apartment.is_deleted.is_(None)),
                    trans_table.exclusive_area.isnot(None),
                    trans_table.exclusive_area > 0
                )
            )
            .group_by(State.city_name, month_expr)
            .having(func.count(trans_table.trans_id) >= 3)  # ìµœì†Œ 3ê±´ ì´ìƒ
            .order_by(State.city_name, month_expr)  # ì§€ì—­ë³„, ì›”ë³„ ì •ë ¬
        )
        
        result = await db.execute(regional_trends_stmt)
        rows = result.fetchall()
        
        # ì§€ì—­ ê·¸ë£¹í™” í•¨ìˆ˜ (ë” í° ê·¸ë£¹ìœ¼ë¡œ ë¬¶ê¸°)
        def get_region_group(city_name: str) -> str:
            """ë„/íŠ¹ë³„ì‹œ/ê´‘ì—­ì‹œë¥¼ ì§€ì—­ ê·¸ë£¹ìœ¼ë¡œ ë³€í™˜"""
            if not city_name:
                return "ê¸°íƒ€"
            
            # ì„œìš¸
            if "ì„œìš¸" in city_name:
                return "ì„œìš¸"
            # ê²½ê¸°
            elif "ê²½ê¸°" in city_name:
                return "ê²½ê¸°"
            # ì¸ì²œ
            elif "ì¸ì²œ" in city_name:
                return "ì¸ì²œ"
            # ì¶©ì²­ (ì¶©ë¶, ì¶©ë‚¨, ëŒ€ì „ í¬í•¨)
            elif "ì¶©ë¶" in city_name or "ì¶©ì²­ë¶" in city_name:
                return "ì¶©ì²­"
            elif "ì¶©ë‚¨" in city_name or "ì¶©ì²­ë‚¨" in city_name:
                return "ì¶©ì²­"
            elif "ëŒ€ì „" in city_name:
                return "ì¶©ì²­"
            # ë¶€ìš¸ê²½ (ë¶€ì‚°, ìš¸ì‚°, ê²½ìƒ, ëŒ€êµ¬ ëª¨ë‘ í¬í•¨)
            elif "ë¶€ì‚°" in city_name:
                return "ë¶€ìš¸ê²½"
            elif "ìš¸ì‚°" in city_name:
                return "ë¶€ìš¸ê²½"
            elif "ëŒ€êµ¬" in city_name:
                return "ë¶€ìš¸ê²½"
            elif "ê²½ë¶" in city_name or "ê²½ìƒë¶" in city_name:
                return "ë¶€ìš¸ê²½"
            elif "ê²½ë‚¨" in city_name or "ê²½ìƒë‚¨" in city_name:
                return "ë¶€ìš¸ê²½"
            # ì „ë¼ (ì „ë¶, ì „ë‚¨, ê´‘ì£¼ í¬í•¨)
            elif "ì „ë¶" in city_name or "ì „ë¼ë¶" in city_name:
                return "ì „ë¼"
            elif "ì „ë‚¨" in city_name or "ì „ë¼ë‚¨" in city_name:
                return "ì „ë¼"
            elif "ê´‘ì£¼" in city_name:
                return "ì „ë¼"
            # ì œì£¼
            elif "ì œì£¼" in city_name:
                return "ì œì£¼"
            # ê¸°íƒ€ (ê°•ì› ë“±)
            else:
                return "ê¸°íƒ€"
        
        # ì§€ì—­ ê·¸ë£¹ë³„ë¡œ ë°ì´í„° ê·¸ë£¹í™”
        regional_groups_dict: Dict[str, Dict[str, Any]] = {}
        for row in rows:
            city_name = row.city_name
            region_group = get_region_group(city_name)
            
            if region_group not in regional_groups_dict:
                regional_groups_dict[region_group] = {}
            
            month = row.month
            avg_price = round(float(row.avg_price_per_pyeong or 0), 1)
            transaction_count = row.transaction_count or 0
            
            # ê°™ì€ ì›”ì˜ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í‰ê·  ê³„ì‚° (ê°€ì¤‘ í‰ê· )
            if month in regional_groups_dict[region_group]:
                existing = regional_groups_dict[region_group][month]
                total_count = existing["transaction_count"] + transaction_count
                if total_count > 0:
                    # ê°€ì¤‘ í‰ê·  ê³„ì‚°
                    existing["avg_price_per_pyeong"] = round(
                        (existing["avg_price_per_pyeong"] * existing["transaction_count"] + 
                         avg_price * transaction_count) / total_count, 1
                    )
                existing["transaction_count"] = total_count
            else:
                regional_groups_dict[region_group][month] = {
                    "month": month,
                    "avg_price_per_pyeong": avg_price,
                    "transaction_count": transaction_count
                }
        
        # ê° ì§€ì—­ ê·¸ë£¹ë³„ ë°ì´í„°ë¥¼ ì›”ë³„ë¡œ ì •ë ¬í•˜ê³  ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        regional_trends = []
        for region_group, month_data in regional_groups_dict.items():
            data_list = list(month_data.values())
            data_list.sort(key=lambda x: x["month"])
            
            regional_trends.append({
                "region": region_group,
                "data": [
                    {
                        "month": item["month"],
                        "avg_price_per_pyeong": item["avg_price_per_pyeong"],
                        "transaction_count": item["transaction_count"]
                    }
                    for item in data_list
                ]
            })
        
        # ì§€ì—­ ê·¸ë£¹ ìˆœì„œëŒ€ë¡œ ì •ë ¬
        region_order = ["ì„œìš¸", "ê²½ê¸°", "ì¸ì²œ", "ì¶©ì²­", "ë¶€ìš¸ê²½", "ì „ë¼", "ì œì£¼", "ê¸°íƒ€"]
        regional_trends.sort(key=lambda x: region_order.index(x["region"]) if x["region"] in region_order else 999)
        
        logger.info(f"âœ… [Dashboard Trends] ì§€ì—­ë³„ ì¶”ì´ ë°ì´í„° ìƒì„± ì™„ë£Œ - ì§€ì—­ ìˆ˜: {len(regional_trends)}")
        
        response_data = {
            "success": True,
            "data": regional_trends
        }
        
        # ìºì‹œì— ì €ì¥ (TTL: 30ë¶„)
        if len(regional_trends) > 0:
            await set_to_cache(cache_key, response_data, ttl=1800)
        
        return response_data
        
    except Exception as e:
        logger.error(f"âŒ [Dashboard Trends] ì§€ì—­ë³„ ì¶”ì´ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


def get_transaction_table(transaction_type: str):
    """ê±°ë˜ ìœ í˜•ì— ë”°ë¥¸ í…Œì´ë¸” ë°˜í™˜"""
    if transaction_type == "sale":
        return Sale
    elif transaction_type == "jeonse":
        return Rent
    else:
        return Sale


def get_price_field(transaction_type: str, table):
    """ê±°ë˜ ìœ í˜•ì— ë”°ë¥¸ ê°€ê²© í•„ë“œ ë°˜í™˜"""
    if transaction_type == "sale":
        return table.trans_price
    elif transaction_type == "jeonse":
        return table.deposit_price
    else:
        return table.trans_price


def get_date_field(transaction_type: str, table):
    """ê±°ë˜ ìœ í˜•ì— ë”°ë¥¸ ë‚ ì§œ í•„ë“œ ë°˜í™˜"""
    if transaction_type == "sale":
        return table.contract_date
    elif transaction_type == "jeonse":
        return table.deal_date
    else:
        return table.contract_date


@router.get(
    "/advanced-charts/price-distribution",
    response_model=dict,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Dashboard (ëŒ€ì‹œë³´ë“œ)"],
    summary="ê°€ê²©ëŒ€ë³„ ì•„íŒŒíŠ¸ ë¶„í¬ (íˆìŠ¤í† ê·¸ë¨ìš©)",
    description="""
    ê°€ê²©ëŒ€ë³„ ì•„íŒŒíŠ¸ ë¶„í¬ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤. HighChart íˆìŠ¤í† ê·¸ë¨ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
    
    ### ì œê³µ ë°ì´í„°
    - ê°€ê²©ëŒ€ êµ¬ê°„ë³„ ì•„íŒŒíŠ¸ ìˆ˜ ë° í‰ê·  ê°€ê²©
    """
)
async def get_price_distribution(
    transaction_type: str = Query("sale", description="ê±°ë˜ ìœ í˜•: sale(ë§¤ë§¤), jeonse(ì „ì„¸)"),
    db: AsyncSession = Depends(get_db)
):
    """ê°€ê²©ëŒ€ë³„ ì•„íŒŒíŠ¸ ë¶„í¬ ì¡°íšŒ (íˆìŠ¤í† ê·¸ë¨ìš©)"""
    cache_key = build_cache_key("dashboard", "price-distribution", transaction_type)
    
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        return cached_data
    
    try:
        logger.info(f"ğŸ” [Dashboard Advanced] ê°€ê²© ë¶„í¬ ì¡°íšŒ ì‹œì‘ - transaction_type: {transaction_type}")
        
        trans_table = get_transaction_table(transaction_type)
        price_field = get_price_field(transaction_type, trans_table)
        
        # í•„í„° ì¡°ê±´
        if transaction_type == "sale":
            base_filter = and_(
                trans_table.is_canceled == False,
                (trans_table.is_deleted == False) | (trans_table.is_deleted.is_(None)),
                trans_table.trans_price.isnot(None),
                trans_table.exclusive_area.isnot(None),
                trans_table.exclusive_area > 0
            )
        else:  # jeonse
            base_filter = and_(
                or_(
                    trans_table.monthly_rent == 0,
                    trans_table.monthly_rent.is_(None)
                ),
                (trans_table.is_deleted == False) | (trans_table.is_deleted.is_(None)),
                trans_table.deposit_price.isnot(None),
                trans_table.exclusive_area.isnot(None),
                trans_table.exclusive_area > 0
            )
        
        # ê°€ê²©ëŒ€ êµ¬ê°„ë³„ ë¶„ë¥˜ (ë§Œì› ë‹¨ìœ„)
        price_ranges = case(
            (price_field < 10000, "1ì–µ ë¯¸ë§Œ"),
            (and_(price_field >= 10000, price_field < 30000), "1ì–µ~3ì–µ"),
            (and_(price_field >= 30000, price_field < 50000), "3ì–µ~5ì–µ"),
            (and_(price_field >= 50000, price_field < 70000), "5ì–µ~7ì–µ"),
            (and_(price_field >= 70000, price_field < 100000), "7ì–µ~10ì–µ"),
            (and_(price_field >= 100000, price_field < 150000), "10ì–µ~15ì–µ"),
            else_="15ì–µ ì´ìƒ"
        )
        
        stmt = (
            select(
                price_ranges.label('price_range'),
                func.count(trans_table.trans_id).label('count'),
                func.avg(price_field).label('avg_price')
            )
            .join(Apartment, trans_table.apt_id == Apartment.apt_id)
            .where(
                and_(
                    base_filter,
                    (Apartment.is_deleted == False) | (Apartment.is_deleted.is_(None)),
                    trans_table.exclusive_area.isnot(None),
                    trans_table.exclusive_area > 0
                )
            )
            .group_by(price_ranges)
            .order_by(price_ranges)
        )
        
        result = await db.execute(stmt)
        rows = result.fetchall()
        
        data = [
            {
                "price_range": row.price_range,
                "count": row.count or 0,
                "avg_price": round(float(row.avg_price or 0) / 10000, 1)  # ì–µì› ë‹¨ìœ„
            }
            for row in rows
        ]
        
        response_data = {
            "success": True,
            "data": data
        }
        
        if len(data) > 0:
            await set_to_cache(cache_key, response_data, ttl=1800)
        
        return response_data
        
    except Exception as e:
        logger.error(f"âŒ [Dashboard Advanced] ê°€ê²© ë¶„í¬ ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/advanced-charts/regional-price-correlation",
    response_model=dict,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Dashboard (ëŒ€ì‹œë³´ë“œ)"],
    summary="ì§€ì—­ë³„ ê°€ê²© ìƒê´€ê´€ê³„ (ë²„ë¸” ì°¨íŠ¸ìš©)",
    description="""
    ì§€ì—­ë³„ í‰ê·  ê°€ê²©, ê±°ë˜ëŸ‰, ìƒìŠ¹ë¥ ì„ ì¡°íšŒí•©ë‹ˆë‹¤. HighChart ë²„ë¸” ì°¨íŠ¸ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
    
    ### ì œê³µ ë°ì´í„°
    - ì§€ì—­ë³„ í‰ê·  ê°€ê²©, ê±°ë˜ëŸ‰, ìƒìŠ¹ë¥ 
    """
)
async def get_regional_price_correlation(
    transaction_type: str = Query("sale", description="ê±°ë˜ ìœ í˜•: sale(ë§¤ë§¤), jeonse(ì „ì„¸)"),
    months: int = Query(3, ge=1, le=12, description="ë¹„êµ ê¸°ê°„ (ê°œì›”)"),
    db: AsyncSession = Depends(get_db)
):
    """ì§€ì—­ë³„ ê°€ê²© ìƒê´€ê´€ê³„ ì¡°íšŒ (ë²„ë¸” ì°¨íŠ¸ìš©)"""
    cache_key = build_cache_key("dashboard", "price-correlation", transaction_type, str(months))
    
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        return cached_data
    
    try:
        logger.info(f"ğŸ” [Dashboard Advanced] ê°€ê²© ìƒê´€ê´€ê³„ ì¡°íšŒ ì‹œì‘ - transaction_type: {transaction_type}, months: {months}")
        
        trans_table = get_transaction_table(transaction_type)
        price_field = get_price_field(transaction_type, trans_table)
        date_field = get_date_field(transaction_type, trans_table)
        
        # í•„í„° ì¡°ê±´
        if transaction_type == "sale":
            base_filter = and_(
                trans_table.is_canceled == False,
                (trans_table.is_deleted == False) | (trans_table.is_deleted.is_(None)),
                trans_table.trans_price.isnot(None),
                trans_table.exclusive_area.isnot(None),
                trans_table.exclusive_area > 0
            )
        else:  # jeonse
            base_filter = and_(
                or_(
                    trans_table.monthly_rent == 0,
                    trans_table.monthly_rent.is_(None)
                ),
                (trans_table.is_deleted == False) | (trans_table.is_deleted.is_(None)),
                trans_table.deposit_price.isnot(None),
                trans_table.exclusive_area.isnot(None),
                trans_table.exclusive_area > 0
            )
        
        # ì‹¤ì œ ë°ì´í„°ì˜ ë‚ ì§œ ë²”ìœ„ í™•ì¸
        date_range_stmt = select(
            func.min(date_field).label('min_date'),
            func.max(date_field).label('max_date')
        ).where(
            and_(
                base_filter,
                date_field.isnot(None)
            )
        )
        date_range_result = await db.execute(date_range_stmt)
        date_range = date_range_result.first()
        
        if not date_range or not date_range.min_date or not date_range.max_date:
            logger.warning(f"âš ï¸ [Dashboard Advanced] ë‚ ì§œ ë²”ìœ„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - ë¹ˆ ë°ì´í„° ë°˜í™˜")
            return {
                "success": True,
                "data": []
            }
        
        max_date = date_range.max_date
        min_date = date_range.min_date
        
        # ìµœê·¼ ê¸°ê°„: ìµœëŒ€ ë‚ ì§œë¡œë¶€í„° months ê°œì›” ì „
        recent_start = max_date - timedelta(days=months * 30)
        # ì´ì „ ê¸°ê°„: recent_startë¡œë¶€í„° months ê°œì›” ì „
        previous_start = recent_start - timedelta(days=months * 30)
        
        # ë‚ ì§œ ë²”ìœ„ê°€ ë°ì´í„° ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ ì¡°ì •
        if previous_start < min_date:
            previous_start = min_date
        if recent_start < min_date:
            recent_start = min_date + timedelta(days=months * 30)
            previous_start = min_date
        
        logger.info(f"ğŸ“… [Dashboard Advanced] ë‚ ì§œ ë²”ìœ„ - min_date: {min_date}, max_date: {max_date}, previous_start: {previous_start}, recent_start: {recent_start}, recent_end: {max_date}")
        
        # ìµœê·¼ ê¸°ê°„ í‰ê·  ê°€ê²© ë° ê±°ë˜ëŸ‰
        recent_stmt = (
            select(
                State.city_name,
                func.avg(price_field / trans_table.exclusive_area * 3.3).label('avg_price_per_pyeong'),
                func.count(trans_table.trans_id).label('transaction_count')
            )
            .join(Apartment, trans_table.apt_id == Apartment.apt_id)
            .join(State, Apartment.region_id == State.region_id)
            .where(
                and_(
                    base_filter,
                    date_field.isnot(None),
                    date_field >= recent_start,
                    date_field <= max_date,
                    (Apartment.is_deleted == False) | (Apartment.is_deleted.is_(None)),
                    trans_table.exclusive_area.isnot(None),
                    trans_table.exclusive_area > 0
                )
            )
            .group_by(State.city_name)
            .having(func.count(trans_table.trans_id) >= 5)
        )
        
        # ì´ì „ ê¸°ê°„ í‰ê·  ê°€ê²©
        previous_stmt = (
            select(
                State.city_name,
                func.avg(price_field / trans_table.exclusive_area * 3.3).label('avg_price_per_pyeong')
            )
            .join(Apartment, trans_table.apt_id == Apartment.apt_id)
            .join(State, Apartment.region_id == State.region_id)
            .where(
                and_(
                    base_filter,
                    date_field.isnot(None),
                    date_field >= previous_start,
                    date_field < recent_start,
                    (Apartment.is_deleted == False) | (Apartment.is_deleted.is_(None)),
                    trans_table.exclusive_area.isnot(None),
                    trans_table.exclusive_area > 0
                )
            )
            .group_by(State.city_name)
            .having(func.count(trans_table.trans_id) >= 5)
        )
        
        recent_result, previous_result = await asyncio.gather(
            db.execute(recent_stmt),
            db.execute(previous_stmt)
        )
        
        recent_rows = recent_result.fetchall()
        previous_rows = previous_result.fetchall()
        
        # ì´ì „ ê¸°ê°„ ê°€ê²© ë”•ì…”ë„ˆë¦¬
        previous_prices = {row.city_name: float(row.avg_price_per_pyeong or 0) for row in previous_rows}
        
        # ìµœê·¼ ê¸°ê°„ ë°ì´í„° ì²˜ë¦¬ ë° ìƒìŠ¹ë¥  ê³„ì‚°
        data = []
        for row in recent_rows:
            city_name = row.city_name
            recent_avg = float(row.avg_price_per_pyeong or 0)
            transaction_count = row.transaction_count or 0
            
            if city_name not in previous_prices or previous_prices[city_name] == 0:
                continue
            
            previous_avg = previous_prices[city_name]
            change_rate = ((recent_avg - previous_avg) / previous_avg) * 100 if previous_avg > 0 else 0
            
            data.append({
                "region": city_name,
                "avg_price_per_pyeong": round(recent_avg, 1),
                "transaction_count": transaction_count,
                "change_rate": round(change_rate, 2)
            })
        
        response_data = {
            "success": True,
            "data": data
        }
        
        if len(data) > 0:
            await set_to_cache(cache_key, response_data, ttl=1800)
        
        return response_data
        
    except Exception as e:
        logger.error(f"âŒ [Dashboard Advanced] ê°€ê²© ìƒê´€ê´€ê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/summary",
    response_model=dict,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Dashboard (ëŒ€ì‹œë³´ë“œ)"],
    summary="ëŒ€ì‹œë³´ë“œ ìš”ì•½ ë°ì´í„° ì¡°íšŒ",
    description="""
    ì „êµ­ í‰ë‹¹ê°€ ë° ê±°ë˜ëŸ‰ ì¶”ì´, ì›”ê°„ ì•„íŒŒíŠ¸ ê°’ ì¶”ì´ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ### ì œê³µ ë°ì´í„°
    1. **ì „êµ­ í‰ë‹¹ê°€ ì¶”ì´**: ìµœê·¼ 6ê°œì›”ê°„ ì›”ë³„ í‰ë‹¹ê°€ í‰ê· 
    2. **ì „êµ­ ê±°ë˜ëŸ‰ ì¶”ì´**: ìµœê·¼ 6ê°œì›”ê°„ ì›”ë³„ ê±°ë˜ ê±´ìˆ˜
    3. **ì›”ê°„ ì•„íŒŒíŠ¸ ê°’ ì¶”ì´**: ì „êµ­ vs ì£¼ìš” ì§€ì—­ ë¹„êµ (ìµœê·¼ 12ê°œì›”)
    
    ### Query Parameters
    - `transaction_type`: ê±°ë˜ ìœ í˜• (sale: ë§¤ë§¤, jeonse: ì „ì„¸, ê¸°ë³¸ê°’: sale)
    - `months`: ì¡°íšŒ ê¸°ê°„ (ê°œì›”, ê¸°ë³¸ê°’: 6, ìµœëŒ€: 12)
    """
)
async def get_dashboard_summary(
    transaction_type: str = Query("sale", description="ê±°ë˜ ìœ í˜•: sale(ë§¤ë§¤), jeonse(ì „ì„¸)"),
    months: int = Query(6, ge=1, le=12, description="ì¡°íšŒ ê¸°ê°„ (ê°œì›”)"),
    db: AsyncSession = Depends(get_db)
):
    """
    ëŒ€ì‹œë³´ë“œ ìš”ì•½ ë°ì´í„° ì¡°íšŒ
    
    ì „êµ­ í‰ë‹¹ê°€ ë° ê±°ë˜ëŸ‰ ì¶”ì´, ì›”ê°„ ì•„íŒŒíŠ¸ ê°’ ì¶”ì´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ìºì‹œ í‚¤ ìƒì„±
    cache_key = build_cache_key("dashboard", "summary", transaction_type, str(months))
    
    # 1. ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        return cached_data
    
    try:
        # 2. ìºì‹œ ë¯¸ìŠ¤: ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒ
        logger.info(f"ğŸ” [Dashboard] ìš”ì•½ ë°ì´í„° ì¡°íšŒ ì‹œì‘ - transaction_type: {transaction_type}, months: {months}")
        
        trans_table = get_transaction_table(transaction_type)
        price_field = get_price_field(transaction_type, trans_table)
        date_field = get_date_field(transaction_type, trans_table)
        
        logger.info(f"ğŸ“Š [Dashboard] í…Œì´ë¸” ì •ë³´ - trans_table: {trans_table.__tablename__}, price_field: {price_field}, date_field: {date_field}")
        
        # í•„í„° ì¡°ê±´ (trans_table ì‚¬ìš©)
        if transaction_type == "sale":
            base_filter = and_(
                trans_table.is_canceled == False,
                (trans_table.is_deleted == False) | (trans_table.is_deleted.is_(None)),
                trans_table.trans_price.isnot(None),
                trans_table.exclusive_area.isnot(None),
                trans_table.exclusive_area > 0
            )
        else:  # jeonse
            base_filter = and_(
                or_(
                    trans_table.monthly_rent == 0,
                    trans_table.monthly_rent.is_(None)
                ),
                (trans_table.is_deleted == False) | (trans_table.is_deleted.is_(None)),
                trans_table.deposit_price.isnot(None),
                trans_table.exclusive_area.isnot(None),
                trans_table.exclusive_area > 0
            )
        
        logger.info(f"ğŸ”§ [Dashboard] base_filter ì„¤ì • ì™„ë£Œ - transaction_type: {transaction_type}")
        
        # ë¨¼ì € ì „ì²´ ë°ì´í„° ê°œìˆ˜ í™•ì¸ (ë‚ ì§œ í•„í„° ì—†ì´)
        total_count_stmt = select(func.count(trans_table.trans_id)).where(base_filter)
        total_count_result = await db.execute(total_count_stmt)
        total_count = total_count_result.scalar() or 0
        
        # ë‚ ì§œê°€ ìˆëŠ” ë°ì´í„° ê°œìˆ˜ í™•ì¸
        date_count_stmt = select(func.count(trans_table.trans_id)).where(
            and_(base_filter, date_field.isnot(None))
        )
        date_count_result = await db.execute(date_count_stmt)
        date_count = date_count_result.scalar() or 0
        
        # ë‚ ì§œê°€ NULLì¸ ë°ì´í„° ê°œìˆ˜ í™•ì¸
        null_date_count_stmt = select(func.count(trans_table.trans_id)).where(
            and_(base_filter, date_field.is_(None))
        )
        null_date_count_result = await db.execute(null_date_count_stmt)
        null_date_count = null_date_count_result.scalar() or 0
        
        logger.info(f"ğŸ“ˆ [Dashboard] ë°ì´í„° ê°œìˆ˜ í™•ì¸ - ì „ì²´: {total_count}, ë‚ ì§œ ìˆìŒ: {date_count}, ë‚ ì§œ NULL: {null_date_count}")
        
        if total_count == 0:
            logger.warning(f"âš ï¸ [Dashboard] {transaction_type} í…Œì´ë¸”ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        elif date_count == 0 and null_date_count == 0:
            logger.warning(f"âš ï¸ [Dashboard] {transaction_type} í…Œì´ë¸”ì— í•„í„° ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        
        # ì‹¤ì œ ë°ì´í„°ì˜ ë‚ ì§œ ë²”ìœ„ í™•ì¸
        date_range_stmt = select(
            func.min(date_field).label('min_date'),
            func.max(date_field).label('max_date')
        ).where(
            and_(
                base_filter,
                date_field.isnot(None)
            )
        )
        date_range_result = await db.execute(date_range_stmt)
        date_range = date_range_result.first()
        
        if not date_range or not date_range.min_date or not date_range.max_date:
            logger.warning(f"âš ï¸ [Dashboard] ë‚ ì§œ ë²”ìœ„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - ë¹ˆ ë°ì´í„° ë°˜í™˜")
            return {
                "success": True,
                "data": {
                    "price_trend": [],
                    "volume_trend": [],
                    "monthly_trend": {
                        "national": [],
                        "regional": []
                    }
                }
            }
        
        # ë°ì´í„°ê°€ ìˆëŠ” ê¸°ê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚ ì§œ ë²”ìœ„ ì„¤ì •
        end_date = date_range.max_date
        # ìµœëŒ€ months ê°œì›” ì „ë¶€í„°, ë˜ëŠ” ë°ì´í„°ì˜ ì‹œì‘ì¼ë¶€í„°
        start_date = max(
            date_range.min_date,
            end_date - timedelta(days=months * 30)
        )
        
        logger.info(f"ğŸ“… [Dashboard] ë‚ ì§œ ë²”ìœ„ - min_date: {date_range.min_date}, max_date: {date_range.max_date}, start_date: {start_date}, end_date: {end_date}")
        
        # ì›”ë³„ ê·¸ë£¹í™”ë¥¼ ìœ„í•œ í‘œí˜„ì‹
        month_expr = func.to_char(date_field, 'YYYY-MM')
        
        # 1. ì „êµ­ í‰ë‹¹ê°€ ì¶”ì´ (ì›”ë³„)
        # exclusive_areaê°€ 0ì´ê±°ë‚˜ NULLì¸ ê²½ìš°ë¥¼ ëª…ì‹œì ìœ¼ë¡œ í•„í„°ë§í•˜ê³ ,
        # í‰ë‹¹ê°€ ê³„ì‚° ì‹œ NULL ê°’ì´ ë°œìƒí•˜ì§€ ì•Šë„ë¡ ì²˜ë¦¬
        price_trend_where_conditions = [
            base_filter,
            date_field.isnot(None),  # ì›”ë³„ ê·¸ë£¹í™”ë¥¼ ìœ„í•´ ë‚ ì§œëŠ” í•„ìˆ˜
            date_field >= start_date,  # ë°ì´í„°ê°€ ìˆëŠ” ê¸°ê°„ì˜ ì‹œì‘ì¼ë¶€í„°
            date_field <= end_date,  # ë°ì´í„°ê°€ ìˆëŠ” ê¸°ê°„ì˜ ì¢…ë£Œì¼ê¹Œì§€
            (Apartment.is_deleted == False) | (Apartment.is_deleted.is_(None)),
            trans_table.exclusive_area.isnot(None),
            trans_table.exclusive_area > 0
        ]
        
        price_trend_stmt = (
            select(
                month_expr.label('month'),
                func.avg(
                    case(
                        (trans_table.exclusive_area.isnot(None), price_field / trans_table.exclusive_area * 3.3),
                        else_=None
                    )
                ).label('avg_price_per_pyeong'),  # í‰ë‹¹ê°€ (ë§Œì›)
                func.count(trans_table.trans_id).label('transaction_count')
            )
            .join(Apartment, trans_table.apt_id == Apartment.apt_id)
            .where(and_(*price_trend_where_conditions))
            .group_by(month_expr)
            .order_by(month_expr)
        )
        
        # 3. ì›”ê°„ ì•„íŒŒíŠ¸ ê°’ ì¶”ì´ (ì „êµ­ vs ì£¼ìš” ì§€ì—­) - ìµœê·¼ 12ê°œì›”
        # ì›”ë³„ ê·¸ë£¹í™”ë¥¼ ìœ„í•œ í‘œí˜„ì‹ (ì›”ê°„ ì¶”ì´ìš©)
        monthly_month_expr = func.to_char(date_field, 'YYYY-MM')
        
        # ì›”ê°„ ì¶”ì´ìš© where ì¡°ê±´ (price_trendì™€ ë™ì¼)
        monthly_trend_where_conditions = list(price_trend_where_conditions)
        
        # ì „êµ­ í‰ê· 
        national_trend_stmt = (
            select(
                monthly_month_expr.label('month'),
                func.avg(price_field).label('avg_price')
            )
            .join(Apartment, trans_table.apt_id == Apartment.apt_id)
            .where(and_(*monthly_trend_where_conditions))
            .group_by(monthly_month_expr)
            .order_by(monthly_month_expr)
        )
        
        # ì§€ì—­ë³„ ì¶”ì´ (ì£¼ìš” ë„ì‹œ: ì„œìš¸, ë¶€ì‚°, ëŒ€êµ¬, ì¸ì²œ, ê´‘ì£¼, ëŒ€ì „, ìš¸ì‚°)
        major_cities = ['ì„œìš¸íŠ¹ë³„ì‹œ', 'ë¶€ì‚°ê´‘ì—­ì‹œ', 'ëŒ€êµ¬ê´‘ì—­ì‹œ', 'ì¸ì²œê´‘ì—­ì‹œ', 'ê´‘ì£¼ê´‘ì—­ì‹œ', 'ëŒ€ì „ê´‘ì—­ì‹œ', 'ìš¸ì‚°ê´‘ì—­ì‹œ']
        
        # ì§€ì—­ë³„ ì¶”ì´ìš© where ì¡°ê±´ (ì›”ê°„ ì¶”ì´ + ì§€ì—­ í•„í„°)
        regional_trend_where_conditions = list(monthly_trend_where_conditions)
        regional_trend_where_conditions.append(State.city_name.in_(major_cities))
        
        regional_trend_stmt = (
            select(
                State.city_name,
                monthly_month_expr.label('month'),
                func.avg(price_field).label('avg_price')
            )
            .join(Apartment, trans_table.apt_id == Apartment.apt_id)
            .join(State, Apartment.region_id == State.region_id)
            .where(and_(*regional_trend_where_conditions))
            .group_by(State.city_name, monthly_month_expr)
            .order_by(State.city_name, monthly_month_expr)
        )
        
        # 2. ì „êµ­ ê±°ë˜ëŸ‰ ì¶”ì´ (ì›”ë³„) - price_trendì™€ ë™ì¼í•œ ì¡°ê±´ ì‚¬ìš©
        volume_trend_stmt = (
            select(
                month_expr.label('month'),
                func.count(trans_table.trans_id).label('transaction_count')
            )
            .join(Apartment, trans_table.apt_id == Apartment.apt_id)
            .where(and_(*price_trend_where_conditions))
            .group_by(month_expr)
            .order_by(month_expr)
        )
        
        # ì¿¼ë¦¬ ë³‘ë ¬ ì‹¤í–‰ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
        logger.info("ğŸš€ [Dashboard] ì¿¼ë¦¬ ì‹¤í–‰ ì‹œì‘")
        price_trend_result, volume_trend_result, national_trend_result, regional_trend_result = await asyncio.gather(
            db.execute(price_trend_stmt),
            db.execute(volume_trend_stmt),
            db.execute(national_trend_stmt),
            db.execute(regional_trend_stmt)
        )
        
        logger.info(f"âœ… [Dashboard] ì¿¼ë¦¬ ì‹¤í–‰ ì™„ë£Œ")
        
        # ê²°ê³¼ ì²˜ë¦¬
        price_trend_data = []
        for row in price_trend_result:
            price_trend_data.append({
                "month": row.month,
                "avg_price_per_pyeong": round(float(row.avg_price_per_pyeong or 0), 1),
                "transaction_count": row.transaction_count or 0
            })
        
        logger.info(f"ğŸ“Š [Dashboard] price_trend_data ê°œìˆ˜: {len(price_trend_data)}, ë°ì´í„°: {price_trend_data}")
        
        volume_trend_data = []
        for row in volume_trend_result:
            volume_trend_data.append({
                "month": row.month,
                "count": row.transaction_count or 0
            })
        
        logger.info(f"ğŸ“Š [Dashboard] volume_trend_data ê°œìˆ˜: {len(volume_trend_data)}, ë°ì´í„°: {volume_trend_data}")
        
        national_trend = []
        for row in national_trend_result:
            national_trend.append({
                "month": row.month,
                "avg_price": round(float(row.avg_price or 0), 0)
            })
        
        logger.info(f"ğŸ“Š [Dashboard] national_trend ê°œìˆ˜: {len(national_trend)}, ë°ì´í„°: {national_trend}")
        
        regional_trend_dict: Dict[str, List[Dict[str, Any]]] = {}
        for row in regional_trend_result:
            city = row.city_name
            if city not in regional_trend_dict:
                regional_trend_dict[city] = []
            regional_trend_dict[city].append({
                "month": row.month,
                "avg_price": round(float(row.avg_price or 0), 0)
            })
        
        logger.info(f"ğŸ“Š [Dashboard] regional_trend_dict: {regional_trend_dict}")
        
        # ì§€ì—­ë³„ ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        regional_trend = [
            {
                "region": city,
                "data": data
            }
            for city, data in regional_trend_dict.items()
        ]
        
        logger.info(f"ğŸ“Š [Dashboard] regional_trend ê°œìˆ˜: {len(regional_trend)}, ë°ì´í„°: {regional_trend}")
        
        response_data = {
            "success": True,
            "data": {
                "price_trend": price_trend_data,  # í‰ë‹¹ê°€ ì¶”ì´
                "volume_trend": volume_trend_data,  # ê±°ë˜ëŸ‰ ì¶”ì´
                "monthly_trend": {
                    "national": national_trend,  # ì „êµ­ ì¶”ì´
                    "regional": regional_trend  # ì§€ì—­ë³„ ì¶”ì´
                }
            }
        }
        
        logger.info(f"âœ… [Dashboard] ì‘ë‹µ ë°ì´í„° ìƒì„± ì™„ë£Œ - price_trend: {len(price_trend_data)}, volume_trend: {len(volume_trend_data)}, national: {len(national_trend)}, regional: {len(regional_trend)}")
        
        # ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ìºì‹œì— ì €ì¥ (ë¹ˆ ë°°ì—´ì€ ìºì‹œí•˜ì§€ ì•ŠìŒ)
        has_data = (len(price_trend_data) > 0 or 
                    len(volume_trend_data) > 0 or 
                    len(national_trend) > 0 or 
                    len(regional_trend) > 0)
        
        if has_data:
            logger.info(f"ğŸ’¾ [Dashboard] ë°ì´í„°ê°€ ìˆìœ¼ë¯€ë¡œ ìºì‹œì— ì €ì¥")
            # 3. ìºì‹œì— ì €ì¥ (TTL: 30ë¶„ = 1800ì´ˆ) - ë” ê¸´ ìºì‹œë¡œ ì„±ëŠ¥ í–¥ìƒ
            await set_to_cache(cache_key, response_data, ttl=1800)
        else:
            logger.warning(f"âš ï¸ [Dashboard] ë°ì´í„°ê°€ ì—†ìœ¼ë¯€ë¡œ ìºì‹œì— ì €ì¥í•˜ì§€ ì•ŠìŒ")
        
        return response_data
        
    except Exception as e:
        logger.error(f"âŒ [Dashboard] ëŒ€ì‹œë³´ë“œ ìš”ì•½ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        logger.error(f"âŒ [Dashboard] ì—ëŸ¬ ìƒì„¸ ì •ë³´:", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/rankings",
    response_model=dict,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Dashboard (ëŒ€ì‹œë³´ë“œ)"],
    summary="ëŒ€ì‹œë³´ë“œ ë­í‚¹ ë°ì´í„° ì¡°íšŒ",
    description="""
    ìš”ì¦˜ ê´€ì‹¬ ë§ì€ ì•„íŒŒíŠ¸, ìƒìŠ¹ë¥  TOP 5, í•˜ë½ë¥  TOP 5ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ### ì œê³µ ë°ì´í„°
    1. **ìš”ì¦˜ ê´€ì‹¬ ë§ì€ ì•„íŒŒíŠ¸**: ìµœê·¼ 7ì¼ê°„ ê±°ë˜ëŸ‰ ê¸°ì¤€ TOP 10
    2. **ìƒìŠ¹ë¥  TOP 5**: ìµœê·¼ 3ê°œì›”ê°„ ê°€ê²© ìƒìŠ¹ë¥ ì´ ë†’ì€ ì•„íŒŒíŠ¸
    3. **í•˜ë½ë¥  TOP 5**: ìµœê·¼ 3ê°œì›”ê°„ ê°€ê²© í•˜ë½ë¥ ì´ ë†’ì€ ì•„íŒŒíŠ¸
    
    ### Query Parameters
    - `transaction_type`: ê±°ë˜ ìœ í˜• (sale: ë§¤ë§¤, jeonse: ì „ì„¸, ê¸°ë³¸ê°’: sale)
    - `trending_days`: ê´€ì‹¬ ë§ì€ ì•„íŒŒíŠ¸ ì¡°íšŒ ê¸°ê°„ (ì¼, ê¸°ë³¸ê°’: 7)
    - `trend_months`: ìƒìŠ¹/í•˜ë½ë¥  ê³„ì‚° ê¸°ê°„ (ê°œì›”, ê¸°ë³¸ê°’: 3)
    """
)
async def get_dashboard_rankings(
    transaction_type: str = Query("sale", description="ê±°ë˜ ìœ í˜•: sale(ë§¤ë§¤), jeonse(ì „ì„¸)"),
    trending_days: int = Query(7, ge=1, le=30, description="ê´€ì‹¬ ë§ì€ ì•„íŒŒíŠ¸ ì¡°íšŒ ê¸°ê°„ (ì¼)"),
    trend_months: int = Query(3, ge=1, le=12, description="ìƒìŠ¹/í•˜ë½ë¥  ê³„ì‚° ê¸°ê°„ (ê°œì›”)"),
    db: AsyncSession = Depends(get_db)
):
    """
    ëŒ€ì‹œë³´ë“œ ë­í‚¹ ë°ì´í„° ì¡°íšŒ
    
    ìš”ì¦˜ ê´€ì‹¬ ë§ì€ ì•„íŒŒíŠ¸, ìƒìŠ¹ë¥  TOP 5, í•˜ë½ë¥  TOP 5ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ìºì‹œ í‚¤ ìƒì„±
    cache_key = build_cache_key("dashboard", "rankings", transaction_type, str(trending_days), str(trend_months))
    
    # 1. ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        return cached_data
    
    try:
        # 2. ìºì‹œ ë¯¸ìŠ¤: ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒ
        logger.info(f"ğŸ” [Dashboard Rankings] ë­í‚¹ ë°ì´í„° ì¡°íšŒ ì‹œì‘ - transaction_type: {transaction_type}, trending_days: {trending_days}, trend_months: {trend_months}")
        
        trans_table = get_transaction_table(transaction_type)
        price_field = get_price_field(transaction_type, trans_table)
        date_field = get_date_field(transaction_type, trans_table)
        
        logger.info(f"ğŸ“Š [Dashboard Rankings] í…Œì´ë¸” ì •ë³´ - trans_table: {trans_table.__tablename__}, price_field: {price_field}, date_field: {date_field}")
        
        # í•„í„° ì¡°ê±´ (trans_table ì‚¬ìš©)
        if transaction_type == "sale":
            base_filter = and_(
                trans_table.is_canceled == False,
                (trans_table.is_deleted == False) | (trans_table.is_deleted.is_(None)),
                trans_table.trans_price.isnot(None),
                trans_table.exclusive_area.isnot(None),
                trans_table.exclusive_area > 0
            )
        else:  # jeonse
            base_filter = and_(
                or_(
                    trans_table.monthly_rent == 0,
                    trans_table.monthly_rent.is_(None)
                ),
                (trans_table.is_deleted == False) | (trans_table.is_deleted.is_(None)),
                trans_table.deposit_price.isnot(None),
                trans_table.exclusive_area.isnot(None),
                trans_table.exclusive_area > 0
            )
        
        logger.info(f"ğŸ”§ [Dashboard Rankings] base_filter ì„¤ì • ì™„ë£Œ")
        
        # ì‹¤ì œ ë°ì´í„°ì˜ ë‚ ì§œ ë²”ìœ„ í™•ì¸
        date_range_stmt = select(
            func.min(date_field).label('min_date'),
            func.max(date_field).label('max_date')
        ).where(
            and_(
                base_filter,
                date_field.isnot(None)
            )
        )
        date_range_result = await db.execute(date_range_stmt)
        date_range = date_range_result.first()
        
        if not date_range or not date_range.min_date or not date_range.max_date:
            logger.warning(f"âš ï¸ [Dashboard Rankings] ë‚ ì§œ ë²”ìœ„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - ë¹ˆ ë°ì´í„° ë°˜í™˜")
            return {
                "success": True,
                "data": {
                    "trending": [],
                    "rising": [],
                    "falling": []
                }
            }
        
        # ë°ì´í„°ê°€ ìˆëŠ” ê¸°ê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚ ì§œ ë²”ìœ„ ì„¤ì •
        max_date = date_range.max_date
        min_date = date_range.min_date
        
        # ë°ì´í„° ê¸°ê°„ ê³„ì‚° (ì¼ ë‹¨ìœ„)
        data_span_days = (max_date - min_date).days
        
        # ìµœê·¼ ê¸°ê°„: ìµœëŒ€ ë‚ ì§œë¡œë¶€í„° trend_months ê°œì›” ì „
        recent_start = max_date - timedelta(days=trend_months * 30)
        # ì´ì „ ê¸°ê°„: recent_startë¡œë¶€í„° trend_months ê°œì›” ì „
        previous_start = recent_start - timedelta(days=trend_months * 30)
        
        # ë‚ ì§œ ë²”ìœ„ê°€ ë°ì´í„° ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ ì¡°ì •
        # ë°ì´í„° ê¸°ê°„ì´ ë¶€ì¡±í•œ ê²½ìš°, ê°€ëŠ¥í•œ ë²”ìœ„ ë‚´ì—ì„œ ìµœëŒ€í•œ í™•ì¥
        if data_span_days < trend_months * 30 * 2:
            # ë°ì´í„° ê¸°ê°„ì´ ë¶€ì¡±í•˜ë©´, ìµœê·¼ ê¸°ê°„ì„ ì „ì²´ ë°ì´í„°ì˜ ì ˆë°˜ìœ¼ë¡œ ì„¤ì •
            logger.warning(f"âš ï¸ [Dashboard Rankings] ë°ì´í„° ê¸°ê°„ì´ ë¶€ì¡±í•¨ ({data_span_days}ì¼). ë‚ ì§œ ë²”ìœ„ ì¡°ì •")
            if data_span_days >= trend_months * 30:
                # ìµœì†Œí•œ trend_months ê°œì›”ì˜ ë°ì´í„°ëŠ” ìˆëŠ” ê²½ìš°
                recent_start = max_date - timedelta(days=trend_months * 30)
                previous_start = min_date
            else:
                # ë°ì´í„° ê¸°ê°„ì´ trend_months ê°œì›”ë³´ë‹¤ ì§§ì€ ê²½ìš°
                mid_date = min_date + timedelta(days=data_span_days // 2)
                recent_start = mid_date
                previous_start = min_date
        else:
            # ë‚ ì§œ ë²”ìœ„ê°€ ë°ì´í„° ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ ì¡°ì •
            if previous_start < min_date:
                previous_start = min_date
            if recent_start < min_date:
                recent_start = min_date + timedelta(days=trend_months * 30)
                previous_start = min_date
        
        logger.info(f"ğŸ“… [Dashboard Rankings] ë‚ ì§œ ë²”ìœ„ - min_date: {min_date}, max_date: {max_date}, data_span_days: {data_span_days}, previous_start: {previous_start}, recent_start: {recent_start}, recent_end: {max_date}")
        
        trending_stmt = (
            select(
                Apartment.apt_id,
                Apartment.apt_name,
                State.city_name,
                State.region_name,
                func.count(trans_table.trans_id).label('transaction_count'),
                func.avg(price_field / trans_table.exclusive_area * 3.3).label('avg_price_per_pyeong')
            )
            .join(Apartment, trans_table.apt_id == Apartment.apt_id)
            .join(State, Apartment.region_id == State.region_id)
            .where(
                and_(
                    base_filter,
                    date_field.isnot(None),
                    date_field >= recent_start,
                    date_field <= max_date,
                    (Apartment.is_deleted == False) | (Apartment.is_deleted.is_(None)),
                    trans_table.exclusive_area.isnot(None),
                    trans_table.exclusive_area > 0
                )
            )
            .group_by(Apartment.apt_id, Apartment.apt_name, State.city_name, State.region_name)
            .having(func.count(trans_table.trans_id) >= 2)  # ìµœì†Œ ê±°ë˜ ê±´ìˆ˜ ì™„í™”: 3 -> 2
            .order_by(desc('transaction_count'))
            .limit(5)
        )
        
        # ì•„íŒŒíŠ¸ë³„ ì´ì „ ê¸°ê°„ í‰ê·  ê°€ê²©
        previous_prices_stmt = (
            select(
                Apartment.apt_id,
                Apartment.apt_name,
                State.city_name,
                State.region_name,
                func.avg(price_field / trans_table.exclusive_area * 3.3).label('avg_price_per_pyeong')
            )
            .join(Apartment, trans_table.apt_id == Apartment.apt_id)
            .join(State, Apartment.region_id == State.region_id)
            .where(
                and_(
                    base_filter,
                    date_field.isnot(None),
                    date_field >= previous_start,
                    date_field < recent_start,
                    (Apartment.is_deleted == False) | (Apartment.is_deleted.is_(None)),
                    trans_table.exclusive_area.isnot(None),
                    trans_table.exclusive_area > 0
                )
            )
            .group_by(Apartment.apt_id, Apartment.apt_name, State.city_name, State.region_name)
            .having(func.count(trans_table.trans_id) >= 2)  # ìµœì†Œ ê±°ë˜ ê±´ìˆ˜ ì™„í™”: 3 -> 2
        )
        
        # ì•„íŒŒíŠ¸ë³„ ìµœê·¼ ê¸°ê°„ í‰ê·  ê°€ê²©
        recent_prices_stmt = (
            select(
                Apartment.apt_id,
                Apartment.apt_name,
                State.city_name,
                State.region_name,
                func.avg(price_field / trans_table.exclusive_area * 3.3).label('avg_price_per_pyeong')
            )
            .join(Apartment, trans_table.apt_id == Apartment.apt_id)
            .join(State, Apartment.region_id == State.region_id)
            .where(
                and_(
                    base_filter,
                    date_field.isnot(None),
                    date_field >= recent_start,
                    date_field <= max_date,
                    (Apartment.is_deleted == False) | (Apartment.is_deleted.is_(None)),
                    trans_table.exclusive_area.isnot(None),
                    trans_table.exclusive_area > 0
                )
            )
            .group_by(Apartment.apt_id, Apartment.apt_name, State.city_name, State.region_name)
            .having(func.count(trans_table.trans_id) >= 2)  # ìµœì†Œ ê±°ë˜ ê±´ìˆ˜ ì™„í™”: 3 -> 2
        )
        
        # ì¿¼ë¦¬ ë³‘ë ¬ ì‹¤í–‰
        logger.info("ğŸš€ [Dashboard Rankings] ë­í‚¹ ì¿¼ë¦¬ ì‹¤í–‰ ì‹œì‘")
        trending_result, previous_prices_result, recent_prices_result = await asyncio.gather(
            db.execute(trending_stmt),
            db.execute(previous_prices_stmt),
            db.execute(recent_prices_stmt)
        )
        
        logger.info(f"âœ… [Dashboard Rankings] ë­í‚¹ ì¿¼ë¦¬ ì‹¤í–‰ ì™„ë£Œ")
        
        # ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì„¸ì…˜ ì¢…ë£Œ ì „ì— ë°ì´í„° ê°€ì ¸ì˜¤ê¸°)
        trending_rows = trending_result.fetchall()
        previous_prices_rows = previous_prices_result.fetchall()
        recent_prices_rows = recent_prices_result.fetchall()
        
        logger.info(f"ğŸ“Š [Dashboard Rankings] ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ - trending: {len(trending_rows)}, previous: {len(previous_prices_rows)}, recent: {len(recent_prices_rows)}")
        
        # ìš”ì¦˜ ê´€ì‹¬ ë§ì€ ì•„íŒŒíŠ¸ ì²˜ë¦¬
        trending_apartments = []
        for row in trending_rows:
            trending_apartments.append({
                "apt_id": row.apt_id,
                "apt_name": row.apt_name or "-",
                "region": f"{row.city_name} {row.region_name}" if row.city_name and row.region_name else "-",
                "transaction_count": row.transaction_count or 0,
                "avg_price_per_pyeong": round(float(row.avg_price_per_pyeong or 0), 1)
            })
        
        logger.info(f"ğŸ“Š [Dashboard Rankings] trending_apartments ê°œìˆ˜: {len(trending_apartments)}, ë°ì´í„°: {trending_apartments}")
        
        # ì´ì „ ê¸°ê°„ ê°€ê²© ì²˜ë¦¬
        previous_prices: Dict[int, Dict[str, Any]] = {}
        for row in previous_prices_rows:
            previous_prices[row.apt_id] = {
                "apt_name": row.apt_name or "-",
                "region": f"{row.city_name} {row.region_name}" if row.city_name and row.region_name else "-",
                "avg_price_per_pyeong": float(row.avg_price_per_pyeong or 0)
            }
        
        logger.info(f"ğŸ“Š [Dashboard Rankings] previous_prices ê°œìˆ˜: {len(previous_prices)}, apt_ids: {list(previous_prices.keys())[:10]}")
        
        rising_apartments = []
        falling_apartments = []
        
        recent_prices_count = 0
        skipped_no_previous = 0
        skipped_zero_previous = 0
        
        for row in recent_prices_rows:
            recent_prices_count += 1
            apt_id = row.apt_id
            recent_avg = float(row.avg_price_per_pyeong or 0)
            
            if apt_id not in previous_prices:
                skipped_no_previous += 1
                logger.debug(f"âš ï¸ [Dashboard Rankings] ì•„íŒŒíŠ¸ {apt_id}ëŠ” ì´ì „ ê¸°ê°„ ë°ì´í„°ê°€ ì—†ì–´ ê±´ë„ˆëœ€")
                continue
            
            previous_avg = previous_prices[apt_id]["avg_price_per_pyeong"]
            
            if previous_avg == 0:
                skipped_zero_previous += 1
                logger.debug(f"âš ï¸ [Dashboard Rankings] ì•„íŒŒíŠ¸ {apt_id}ëŠ” ì´ì „ ê¸°ê°„ í‰ê·  ê°€ê²©ì´ 0ì´ì–´ì„œ ê±´ë„ˆëœ€")
                continue
            
            if recent_avg == 0:
                logger.debug(f"âš ï¸ [Dashboard Rankings] ì•„íŒŒíŠ¸ {apt_id}ëŠ” ìµœê·¼ ê¸°ê°„ í‰ê·  ê°€ê²©ì´ 0ì´ì–´ì„œ ê±´ë„ˆëœ€")
                continue
            
            change_rate = ((recent_avg - previous_avg) / previous_avg) * 100
            
            apt_data = {
                "apt_id": apt_id,
                "apt_name": row.apt_name or previous_prices[apt_id]["apt_name"],
                "region": f"{row.city_name} {row.region_name}" if row.city_name and row.region_name else previous_prices[apt_id]["region"],
                "change_rate": round(change_rate, 2),
                "recent_avg": round(recent_avg, 1),
                "previous_avg": round(previous_avg, 1)
            }
            
            if change_rate > 0:
                rising_apartments.append(apt_data)
            elif change_rate < 0:
                falling_apartments.append(apt_data)
        
        logger.info(f"ğŸ“Š [Dashboard Rankings] recent_prices_count: {recent_prices_count}, skipped_no_previous: {skipped_no_previous}, skipped_zero_previous: {skipped_zero_previous}, rising: {len(rising_apartments)}, falling: {len(falling_apartments)}")
        
        # ì •ë ¬ ë° TOP 5 ì„ íƒ
        rising_apartments.sort(key=lambda x: x["change_rate"], reverse=True)
        falling_apartments.sort(key=lambda x: x["change_rate"])
        
        # ìµœì†Œ 1ê°œ ì´ìƒì˜ ê²°ê³¼ë¥¼ ë³´ì¥í•˜ê¸° ìœ„í•´, ë°ì´í„°ê°€ ë¶€ì¡±í•œ ê²½ìš° ë” ë§ì€ ì•„íŒŒíŠ¸ë¥¼ í¬í•¨
        if len(rising_apartments) < 5 and len(recent_prices_rows) > len(rising_apartments):
            logger.info(f"âš ï¸ [Dashboard Rankings] ìƒìŠ¹ ì•„íŒŒíŠ¸ê°€ ë¶€ì¡±í•¨ ({len(rising_apartments)}ê°œ). ì¶”ê°€ ë°ì´í„° í¬í•¨ ì‹œë„")
        
        if len(falling_apartments) < 5 and len(recent_prices_rows) > len(falling_apartments):
            logger.info(f"âš ï¸ [Dashboard Rankings] í•˜ë½ ì•„íŒŒíŠ¸ê°€ ë¶€ì¡±í•¨ ({len(falling_apartments)}ê°œ). ì¶”ê°€ ë°ì´í„° í¬í•¨ ì‹œë„")
        
        rising_apartments = rising_apartments[:5]
        falling_apartments = falling_apartments[:5]
        
        logger.info(f"ğŸ“Š [Dashboard Rankings] ìµœì¢… ê²°ê³¼ - trending: {len(trending_apartments)}, rising: {len(rising_apartments)}, falling: {len(falling_apartments)}")
        
        # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ìƒì„¸ ë¡œê·¸ ì¶œë ¥
        if len(rising_apartments) == 0:
            logger.warning(f"âš ï¸ [Dashboard Rankings] ìƒìŠ¹ ì•„íŒŒíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. recent_prices_rows: {len(recent_prices_rows)}, previous_prices: {len(previous_prices)}, skipped_no_previous: {skipped_no_previous}, skipped_zero_previous: {skipped_zero_previous}")
        
        if len(falling_apartments) == 0:
            logger.warning(f"âš ï¸ [Dashboard Rankings] í•˜ë½ ì•„íŒŒíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. recent_prices_rows: {len(recent_prices_rows)}, previous_prices: {len(previous_prices)}, skipped_no_previous: {skipped_no_previous}, skipped_zero_previous: {skipped_zero_previous}")
        
        response_data = {
            "success": True,
            "data": {
                "trending": trending_apartments,  # ìš”ì¦˜ ê´€ì‹¬ ë§ì€ ì•„íŒŒíŠ¸
                "rising": rising_apartments,  # ìƒìŠ¹ë¥  TOP 5
                "falling": falling_apartments  # í•˜ë½ë¥  TOP 5
            }
        }
        
        logger.info(f"âœ… [Dashboard Rankings] ì‘ë‹µ ë°ì´í„° ìƒì„± ì™„ë£Œ")
        
        # ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ìºì‹œì— ì €ì¥ (ë¹ˆ ë°°ì—´ì€ ìºì‹œí•˜ì§€ ì•ŠìŒ)
        has_data = (len(trending_apartments) > 0 or 
                    len(rising_apartments) > 0 or 
                    len(falling_apartments) > 0)
        
        if has_data:
            logger.info(f"ğŸ’¾ [Dashboard Rankings] ë°ì´í„°ê°€ ìˆìœ¼ë¯€ë¡œ ìºì‹œì— ì €ì¥")
            # 3. ìºì‹œì— ì €ì¥ (TTL: 30ë¶„ = 1800ì´ˆ)
            await set_to_cache(cache_key, response_data, ttl=1800)
        else:
            logger.warning(f"âš ï¸ [Dashboard Rankings] ë°ì´í„°ê°€ ì—†ìœ¼ë¯€ë¡œ ìºì‹œì— ì €ì¥í•˜ì§€ ì•ŠìŒ")
        
        return response_data
        
    except Exception as e:
        logger.error(f"âŒ [Dashboard Rankings] ëŒ€ì‹œë³´ë“œ ë­í‚¹ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        logger.error(f"âŒ [Dashboard Rankings] ì—ëŸ¬ ìƒì„¸ ì •ë³´:", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/rankings_region",
    response_model=dict,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Dashboard (ëŒ€ì‹œë³´ë“œ)"],
    summary="ì§€ì—­ë³„ ëŒ€ì‹œë³´ë“œ ë­í‚¹ ë°ì´í„° ì¡°íšŒ",
    description="""
    ì§€ì •í•œ ì‹œë„ ë‚´ì—ì„œ ìš”ì¦˜ ê´€ì‹¬ ë§ì€ ì•„íŒŒíŠ¸, ìƒìŠ¹ë¥  TOP 5, í•˜ë½ë¥  TOP 5ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ### ì œê³µ ë°ì´í„°
    1. **ìš”ì¦˜ ê´€ì‹¬ ë§ì€ ì•„íŒŒíŠ¸**: ìµœê·¼ 7ì¼ê°„ ì§€ì •í•œ ì§€ì—­ ê±°ë˜ëŸ‰ ê¸°ì¤€ TOP 10
    2. **ìƒìŠ¹ë¥  TOP 5**: ìµœê·¼ 3ê°œì›”ê°„ ì§€ì •í•œ ì§€ì—­ ê°€ê²© ìƒìŠ¹ë¥ ì´ ë†’ì€ ì•„íŒŒíŠ¸
    3. **í•˜ë½ë¥  TOP 5**: ìµœê·¼ 3ê°œì›”ê°„ ì§€ì •í•œ ì§€ì—­ ê°€ê²© í•˜ë½ë¥ ì´ ë†’ì€ ì•„íŒŒíŠ¸
    
    ### Query Parameters
    - `transaction_type`: ê±°ë˜ ìœ í˜• (sale: ë§¤ë§¤, jeonse: ì „ì„¸, ê¸°ë³¸ê°’: sale)
    - `trending_days`: ê´€ì‹¬ ë§ì€ ì•„íŒŒíŠ¸ ì¡°íšŒ ê¸°ê°„ (ì¼, ê¸°ë³¸ê°’: 7)
    - `trend_months`: ìƒìŠ¹/í•˜ë½ë¥  ê³„ì‚° ê¸°ê°„ (ê°œì›”, ê¸°ë³¸ê°’: 3)
    - `region_name`: ì§€ì—­ëª… (ì „êµ­ 7ë„ë§Œ ì…ë ¥ ê°€ëŠ¥, ì˜ˆ: "ê²½ê¸°ë„", "ì„œìš¸íŠ¹ë³„ì‹œ", "ë¶€ì‚°ê´‘ì—­ì‹œ" ë“±)
    """
)
async def get_dashboard_rankings_region(
    transaction_type: str = Query("sale", description="ê±°ë˜ ìœ í˜•: sale(ë§¤ë§¤), jeonse(ì „ì„¸)"),
    trending_days: int = Query(7, ge=1, le=30, description="ê´€ì‹¬ ë§ì€ ì•„íŒŒíŠ¸ ì¡°íšŒ ê¸°ê°„ (ì¼)"),
    trend_months: int = Query(3, ge=1, le=12, description="ìƒìŠ¹/í•˜ë½ë¥  ê³„ì‚° ê¸°ê°„ (ê°œì›”)"),
    region_name: Optional[str] = Query(None, description="ì§€ì—­ëª… (ì‹œë„ ë ˆë²¨ë§Œ, ì˜ˆ: 'ê²½ê¸°ë„', 'ì„œìš¸íŠ¹ë³„ì‹œ', 'ë¶€ì‚°ê´‘ì—­ì‹œ' ë“±)"),
    db: AsyncSession = Depends(get_db)
):
    """
    ì§€ì—­ë³„ ëŒ€ì‹œë³´ë“œ ë­í‚¹ ë°ì´í„° ì¡°íšŒ
    
    ê° ì§€ì—­ë³„ë¡œ ìš”ì¦˜ ê´€ì‹¬ ë§ì€ ì•„íŒŒíŠ¸, ìƒìŠ¹ë¥  TOP 5, í•˜ë½ë¥  TOP 5ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ì§€ì—­ í•„í„° íŒŒì‹±
    region_filter_city_name = None
    region_filter_region_name = None
    
    if region_name:
        # ì§€ì—­ëª… íŒŒì‹± (ì˜ˆ: "ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬", "ê²½ê¸°ë„ ê³ ì–‘ì‹œ", "ê°•ë‚¨êµ¬", "ê²½ê¸°ë„", "ëŒ€í•œë¯¼êµ­")
        parts = region_name.strip().split()
        
        # city_name ì •ê·œí™” ë§¤í•‘
        city_mapping = {
            "ì„œìš¸": "ì„œìš¸íŠ¹ë³„ì‹œ",
            "ë¶€ì‚°": "ë¶€ì‚°ê´‘ì—­ì‹œ",
            "ëŒ€êµ¬": "ëŒ€êµ¬ê´‘ì—­ì‹œ",
            "ì¸ì²œ": "ì¸ì²œê´‘ì—­ì‹œ",
            "ê´‘ì£¼": "ê´‘ì£¼ê´‘ì—­ì‹œ",
            "ëŒ€ì „": "ëŒ€ì „ê´‘ì—­ì‹œ",
            "ìš¸ì‚°": "ìš¸ì‚°ê´‘ì—­ì‹œ",
            "ì„¸ì¢…": "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ",
            "ê²½ê¸°": "ê²½ê¸°ë„",
            "ê°•ì›": "ê°•ì›íŠ¹ë³„ìì¹˜ë„",
            "ì¶©ë¶": "ì¶©ì²­ë¶ë„",
            "ì¶©ë‚¨": "ì¶©ì²­ë‚¨ë„",
            "ì „ë¶": "ì „ë¶íŠ¹ë³„ìì¹˜ë„",
            "ì „ë‚¨": "ì „ë¼ë‚¨ë„",
            "ê²½ë¶": "ê²½ìƒë¶ë„",
            "ê²½ë‚¨": "ê²½ìƒë‚¨ë„",
            "ì œì£¼": "ì œì£¼íŠ¹ë³„ìì¹˜ë„"
        }
        
        # ì „ì²´/ëŒ€í•œë¯¼êµ­ í‚¤ì›Œë“œ ì²˜ë¦¬
        if region_name.strip() in ["ëŒ€í•œë¯¼êµ­", "ì „êµ­", "ì „ì²´", "all", "ì „ì²´ì§€ì—­"]:
            # ì „ì²´ ì§€ì—­ ì¡°íšŒ (í•„í„° ì—†ìŒ)
            region_filter_city_name = None
            region_filter_region_name = None
            logger.info(f"ğŸ” [Dashboard Rankings Region] ì „ì²´ ì§€ì—­ ì¡°íšŒ ëª¨ë“œ")
        elif len(parts) == 1:
            # ë‹¨ì¼ ë‹¨ì–´ ì…ë ¥: ì‹œë„ëª…ì¸ì§€ í™•ì¸ (ì‹œë„ ë ˆë²¨ë§Œ í—ˆìš©)
            input_name = parts[0]
            
            # ì‹œë„ëª…ìœ¼ë¡œ ëë‚˜ëŠ”ì§€ í™•ì¸ (ë„, íŠ¹ë³„ì‹œ, ê´‘ì—­ì‹œ, íŠ¹ë³„ìì¹˜ì‹œ, íŠ¹ë³„ìì¹˜ë„)
            is_city_level = input_name.endswith(("ë„", "íŠ¹ë³„ì‹œ", "ê´‘ì—­ì‹œ", "íŠ¹ë³„ìì¹˜ì‹œ", "íŠ¹ë³„ìì¹˜ë„"))
            
            if is_city_level:
                # ì‹œë„ëª…ë§Œ ì œê³µëœ ê²½ìš° (ì˜ˆ: "ê²½ê¸°ë„", "ì„œìš¸íŠ¹ë³„ì‹œ")
                city_name_normalized = city_mapping.get(input_name, input_name)
                if not city_name_normalized.endswith(("ì‹œ", "ë„", "íŠ¹ë³„ì‹œ", "ê´‘ì—­ì‹œ", "íŠ¹ë³„ìì¹˜ì‹œ", "íŠ¹ë³„ìì¹˜ë„")):
                    city_name_normalized = city_mapping.get(input_name, f"{input_name}ì‹œ")
                
                region_filter_city_name = city_name_normalized
                region_filter_region_name = None  # ì‹œë„ ì „ì²´
                logger.info(f"ğŸ” [Dashboard Rankings Region] ì‹œë„ëª…ë§Œ ì œê³µë¨ - {input_name} â†’ {region_filter_city_name}")
            else:
                # ì‹œë„ ë ˆë²¨ì´ ì•„ë‹Œ ê²½ìš° ì—ëŸ¬
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"ì‹œë„ ë ˆë²¨ë§Œ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤. (ì˜ˆ: 'ê²½ê¸°ë„', 'ì„œìš¸íŠ¹ë³„ì‹œ', 'ë¶€ì‚°ê´‘ì—­ì‹œ' ë“±). ì…ë ¥ëœ ê°’: '{input_name}'"
                )
        elif len(parts) >= 2:
            # ì‹œë„ëª… + ì‹œêµ°êµ¬ëª… í˜•ì‹ì€ í—ˆìš©í•˜ì§€ ì•ŠìŒ (ì‹œë„ ë ˆë²¨ë§Œ í—ˆìš©)
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"ì‹œë„ ë ˆë²¨ë§Œ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤. (ì˜ˆ: 'ê²½ê¸°ë„', 'ì„œìš¸íŠ¹ë³„ì‹œ', 'ë¶€ì‚°ê´‘ì—­ì‹œ' ë“±). ì…ë ¥ëœ ê°’: '{region_name}'"
            )
        
        logger.info(f"ğŸ” [Dashboard Rankings Region] ì§€ì—­ í•„í„° - city_name: {region_filter_city_name}, region_name: {region_filter_region_name}")
    
    # ìºì‹œ í‚¤ ìƒì„± (ì§€ì—­ í•„í„° í¬í•¨)
    cache_key_parts = ["dashboard", "rankings_region", transaction_type, str(trending_days), str(trend_months)]
    if region_name:
        cache_key_parts.append(region_name)
    cache_key = build_cache_key(*cache_key_parts)
    
    # 1. ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        # ìºì‹œ ë°ì´í„°ê°€ ë¹ˆ ë°°ì—´ì¸ì§€ í™•ì¸ (ë°ì´í„°ê°€ ì‹¤ì œë¡œ ì—†ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ DB ì¬ì¡°íšŒ)
        data = cached_data.get("data", {})
        trending = data.get("trending", [])
        rising = data.get("rising", [])
        falling = data.get("falling", [])
        
        # ëª¨ë“  ë°ì´í„°ê°€ ë¹ˆ ë°°ì—´ì´ë©´ DBì—ì„œ ë‹¤ì‹œ ì¡°íšŒ
        if len(trending) == 0 and len(rising) == 0 and len(falling) == 0:
            logger.info(f"âš ï¸ [Dashboard Rankings Region] ìºì‹œ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŒ. DBì—ì„œ ì¬ì¡°íšŒ ì‹œë„ - cache_key: {cache_key}")
        else:
            # ë°ì´í„°ê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ìºì‹œ ë°ì´í„° ë°˜í™˜
            return cached_data
    
    try:
        # 2. ìºì‹œ ë¯¸ìŠ¤: ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒ
        logger.info(f"ğŸ” [Dashboard Rankings Region] ì§€ì—­ë³„ ë­í‚¹ ë°ì´í„° ì¡°íšŒ ì‹œì‘ - transaction_type: {transaction_type}, trending_days: {trending_days}, trend_months: {trend_months}, region_name: {region_name}")
        
        trans_table = get_transaction_table(transaction_type)
        price_field = get_price_field(transaction_type, trans_table)
        date_field = get_date_field(transaction_type, trans_table)
        
        logger.info(f"ğŸ“Š [Dashboard Rankings Region] í…Œì´ë¸” ì •ë³´ - trans_table: {trans_table.__tablename__}, price_field: {price_field}, date_field: {date_field}")
        
        # í•„í„° ì¡°ê±´
        if transaction_type == "sale":
            base_filter = and_(
                trans_table.is_canceled == False,
                (trans_table.is_deleted == False) | (trans_table.is_deleted.is_(None)),
                trans_table.trans_price.isnot(None),
                trans_table.exclusive_area.isnot(None),
                trans_table.exclusive_area > 0
            )
        else:  # jeonse
            base_filter = and_(
                or_(
                    trans_table.monthly_rent == 0,
                    trans_table.monthly_rent.is_(None)
                ),
                (trans_table.is_deleted == False) | (trans_table.is_deleted.is_(None)),
                trans_table.deposit_price.isnot(None),
                trans_table.exclusive_area.isnot(None),
                trans_table.exclusive_area > 0
            )
        
        logger.info(f"ğŸ”§ [Dashboard Rankings Region] base_filter ì„¤ì • ì™„ë£Œ")
        
        # ì§€ì—­ í•„í„° ì¡°ê±´ êµ¬ì„± (ë‚ ì§œ ë²”ìœ„ í™•ì¸ìš©)
        region_filter_conditions = []
        if region_filter_city_name:
            region_filter_conditions.append(State.city_name == region_filter_city_name)
        if region_filter_region_name:
            region_filter_conditions.append(State.region_name == region_filter_region_name)
        
        # ì‹¤ì œ ë°ì´í„°ì˜ ë‚ ì§œ ë²”ìœ„ í™•ì¸ (ì§€ì—­ í•„í„° ì ìš©)
        date_range_where_conditions = [
            base_filter,
            date_field.isnot(None)
        ]
        
        # ì§€ì—­ í•„í„°ê°€ ìˆìœ¼ë©´ ë‚ ì§œ ë²”ìœ„ í™•ì¸ì—ë„ ì ìš©
        if region_filter_conditions:
            # Stateì™€ Apartmentë¥¼ ì¡°ì¸í•˜ì—¬ ì§€ì—­ í•„í„° ì ìš©
            date_range_stmt = (
                select(
                    func.min(date_field).label('min_date'),
                    func.max(date_field).label('max_date')
                )
                .select_from(trans_table)
                .join(Apartment, trans_table.apt_id == Apartment.apt_id)
                .join(State, Apartment.region_id == State.region_id)
                .where(
                    and_(
                        *date_range_where_conditions,
                        *region_filter_conditions,
                        (Apartment.is_deleted == False) | (Apartment.is_deleted.is_(None)),
                        (State.is_deleted == False) | (State.is_deleted.is_(None))
                    )
                )
            )
        else:
            # ì§€ì—­ í•„í„°ê°€ ì—†ìœ¼ë©´ ì „ì²´ ë°ì´í„° ë²”ìœ„ í™•ì¸
            date_range_stmt = select(
                func.min(date_field).label('min_date'),
                func.max(date_field).label('max_date')
            ).where(
                and_(*date_range_where_conditions)
            )
        
        date_range_result = await db.execute(date_range_stmt)
        date_range = date_range_result.first()
        
        if not date_range or not date_range.min_date or not date_range.max_date:
            logger.warning(f"âš ï¸ [Dashboard Rankings Region] ë‚ ì§œ ë²”ìœ„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ (ì§€ì—­ í•„í„°: {region_name}) - ë¹ˆ ë°ì´í„° ë°˜í™˜")
            return {
                "success": True,
                "data": {}
            }
        
        # ë°ì´í„°ê°€ ìˆëŠ” ê¸°ê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚ ì§œ ë²”ìœ„ ì„¤ì •
        max_date = date_range.max_date
        min_date = date_range.min_date
        
        # ë°ì´í„° ê¸°ê°„ ê³„ì‚° (ì¼ ë‹¨ìœ„)
        data_span_days = (max_date - min_date).days
        
        # ìµœê·¼ ê¸°ê°„: ìµœëŒ€ ë‚ ì§œë¡œë¶€í„° trend_months ê°œì›” ì „
        recent_start = max_date - timedelta(days=trend_months * 30)
        # ì´ì „ ê¸°ê°„: recent_startë¡œë¶€í„° trend_months ê°œì›” ì „
        previous_start = recent_start - timedelta(days=trend_months * 30)
        
        # trending ê¸°ê°„: ìµœëŒ€ ë‚ ì§œë¡œë¶€í„° trending_daysì¼ ì „
        trending_start = max_date - timedelta(days=trending_days)
        
        # ë‚ ì§œ ë²”ìœ„ê°€ ë°ì´í„° ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ ì¡°ì •
        if data_span_days < trend_months * 30 * 2:
            logger.warning(f"âš ï¸ [Dashboard Rankings Region] ë°ì´í„° ê¸°ê°„ì´ ë¶€ì¡±í•¨ ({data_span_days}ì¼). ë‚ ì§œ ë²”ìœ„ ì¡°ì •")
            if data_span_days >= trend_months * 30:
                recent_start = max_date - timedelta(days=trend_months * 30)
                previous_start = min_date
            else:
                mid_date = min_date + timedelta(days=data_span_days // 2)
                recent_start = mid_date
                previous_start = min_date
        
        if previous_start < min_date:
            previous_start = min_date
        if recent_start < min_date:
            recent_start = min_date + timedelta(days=trend_months * 30)
            previous_start = min_date
        if trending_start < min_date:
            trending_start = min_date
        
        logger.info(f"ğŸ“… [Dashboard Rankings Region] ë‚ ì§œ ë²”ìœ„ - min_date: {min_date}, max_date: {max_date}, data_span_days: {data_span_days}, previous_start: {previous_start}, recent_start: {recent_start}, trending_start: {trending_start}, recent_end: {max_date}")
        
        # ì§€ì—­ í•„í„° ì¡°ê±´ êµ¬ì„± (ì´ë¯¸ ìœ„ì—ì„œ êµ¬ì„±í–ˆì§€ë§Œ, ì¿¼ë¦¬ì—ì„œ ì¬ì‚¬ìš©)
        # region_filter_conditionsëŠ” ì´ë¯¸ ìœ„ì—ì„œ êµ¬ì„±ë¨
        
        # ì§€ì •í•œ ì‹œë„ ë‚´ ìš”ì¦˜ ê´€ì‹¬ ë§ì€ ì•„íŒŒíŠ¸ (ìµœê·¼ trending_daysì¼ê°„ ê±°ë˜ëŸ‰ ê¸°ì¤€ TOP 10)
        # ì§€ì—­ë³„ë¡œ ë‚˜ëˆ„ì§€ ì•Šê³  ì§€ì •í•œ ì‹œë„ ë‚´ ì „ì²´ ì•„íŒŒíŠ¸ë¥¼ ëŒ€ìƒìœ¼ë¡œ ë­í‚¹
        trending_where_conditions = [
            base_filter,
            date_field.isnot(None),
            date_field >= trending_start,
            date_field <= max_date,
            (Apartment.is_deleted == False) | (Apartment.is_deleted.is_(None)),
            (State.is_deleted == False) | (State.is_deleted.is_(None)),
            trans_table.exclusive_area.isnot(None),
            trans_table.exclusive_area > 0
        ]
        if region_filter_conditions:
            trending_where_conditions.extend(region_filter_conditions)
        
        # ì•„íŒŒíŠ¸ë³„ë¡œë§Œ ê·¸ë£¹í™” (ì§€ì—­ ì •ë³´ëŠ” ì„ íƒì— í¬í•¨í•˜ë˜ ê·¸ë£¹í™”ì—ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        # ê°™ì€ ì•„íŒŒíŠ¸ê°€ ì—¬ëŸ¬ ì§€ì—­ì— ê±¸ì³ ìˆì–´ë„ í•˜ë‚˜ë¡œ ì§‘ê³„
        trending_stmt = (
            select(
                Apartment.apt_id,
                Apartment.apt_name,
                func.max(State.city_name).label('city_name'),  # ì§€ì—­ ì •ë³´ëŠ” í•˜ë‚˜ë§Œ ì„ íƒ
                func.max(State.region_name).label('region_name'),
                func.count(trans_table.trans_id).label('transaction_count'),
                func.avg(price_field / trans_table.exclusive_area * 3.3).label('avg_price_per_pyeong')
            )
            .join(Apartment, trans_table.apt_id == Apartment.apt_id)
            .join(State, Apartment.region_id == State.region_id)
            .where(and_(*trending_where_conditions))
            .group_by(Apartment.apt_id, Apartment.apt_name)  # ì•„íŒŒíŠ¸ë³„ë¡œë§Œ ê·¸ë£¹í™”
            .having(func.count(trans_table.trans_id) >= 2)
            .order_by(desc(func.count(trans_table.trans_id)))
            .limit(10)
        )
        
        # ì§€ì •í•œ ì‹œë„ ë‚´ ì´ì „ ê¸°ê°„ í‰ê·  ê°€ê²© (ì§€ì—­ë³„ë¡œ ë‚˜ëˆ„ì§€ ì•Šê³  ì „ì²´ ì•„íŒŒíŠ¸ ëŒ€ìƒ)
        previous_where_conditions = [
            base_filter,
            date_field.isnot(None),
            date_field >= previous_start,
            date_field < recent_start,
            (Apartment.is_deleted == False) | (Apartment.is_deleted.is_(None)),
            (State.is_deleted == False) | (State.is_deleted.is_(None)),
            trans_table.exclusive_area.isnot(None),
            trans_table.exclusive_area > 0
        ]
        if region_filter_conditions:
            previous_where_conditions.extend(region_filter_conditions)
        
        previous_prices_stmt = (
            select(
                Apartment.apt_id,
                Apartment.apt_name,
                func.max(State.city_name).label('city_name'),  # ì§€ì—­ ì •ë³´ëŠ” í•˜ë‚˜ë§Œ ì„ íƒ
                func.max(State.region_name).label('region_name'),
                func.avg(price_field / trans_table.exclusive_area * 3.3).label('avg_price_per_pyeong')
            )
            .join(Apartment, trans_table.apt_id == Apartment.apt_id)
            .join(State, Apartment.region_id == State.region_id)
            .where(and_(*previous_where_conditions))
            .group_by(Apartment.apt_id, Apartment.apt_name)  # ì•„íŒŒíŠ¸ë³„ë¡œë§Œ ê·¸ë£¹í™”
            .having(func.count(trans_table.trans_id) >= 2)
        )
        
        # ì§€ì •í•œ ì‹œë„ ë‚´ ìµœê·¼ ê¸°ê°„ í‰ê·  ê°€ê²© (ì§€ì—­ë³„ë¡œ ë‚˜ëˆ„ì§€ ì•Šê³  ì „ì²´ ì•„íŒŒíŠ¸ ëŒ€ìƒ)
        recent_where_conditions = [
            base_filter,
            date_field.isnot(None),
            date_field >= recent_start,
            date_field <= max_date,
            (Apartment.is_deleted == False) | (Apartment.is_deleted.is_(None)),
            (State.is_deleted == False) | (State.is_deleted.is_(None)),
            trans_table.exclusive_area.isnot(None),
            trans_table.exclusive_area > 0
        ]
        if region_filter_conditions:
            recent_where_conditions.extend(region_filter_conditions)
        
        recent_prices_stmt = (
            select(
                Apartment.apt_id,
                Apartment.apt_name,
                func.max(State.city_name).label('city_name'),  # ì§€ì—­ ì •ë³´ëŠ” í•˜ë‚˜ë§Œ ì„ íƒ
                func.max(State.region_name).label('region_name'),
                func.avg(price_field / trans_table.exclusive_area * 3.3).label('avg_price_per_pyeong')
            )
            .join(Apartment, trans_table.apt_id == Apartment.apt_id)
            .join(State, Apartment.region_id == State.region_id)
            .where(and_(*recent_where_conditions))
            .group_by(Apartment.apt_id, Apartment.apt_name)  # ì•„íŒŒíŠ¸ë³„ë¡œë§Œ ê·¸ë£¹í™”
            .having(func.count(trans_table.trans_id) >= 2)
        )
        
        # ì¿¼ë¦¬ ë³‘ë ¬ ì‹¤í–‰
        logger.info("ğŸš€ [Dashboard Rankings Region] ì§€ì—­ë³„ ë­í‚¹ ì¿¼ë¦¬ ì‹¤í–‰ ì‹œì‘")
        trending_result, previous_prices_result, recent_prices_result = await asyncio.gather(
            db.execute(trending_stmt),
            db.execute(previous_prices_stmt),
            db.execute(recent_prices_stmt)
        )
        
        logger.info(f"âœ… [Dashboard Rankings Region] ì§€ì—­ë³„ ë­í‚¹ ì¿¼ë¦¬ ì‹¤í–‰ ì™„ë£Œ")
        
        # ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        trending_rows = trending_result.fetchall()
        previous_prices_rows = previous_prices_result.fetchall()
        recent_prices_rows = recent_prices_result.fetchall()
        
        logger.info(f"ğŸ“Š [Dashboard Rankings Region] ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ - trending: {len(trending_rows)}ê°œ, previous: {len(previous_prices_rows)}ê°œ ì•„íŒŒíŠ¸, recent: {len(recent_prices_rows)}ê°œ ì•„íŒŒíŠ¸")
        
        # ì§€ì—­ í•„í„°ê°€ ì ìš©ëœ ê²½ìš°, ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ê²½ê³  ë¡œê·¸ ì¶œë ¥
        if region_name and len(trending_rows) == 0 and len(previous_prices_rows) == 0 and len(recent_prices_rows) == 0:
            logger.warning(f"âš ï¸ [Dashboard Rankings Region] ì§€ì—­ í•„í„° '{region_name}'ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (city_name: {region_filter_city_name})")
            # ì‹œë„ê°€ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if region_filter_city_name:
                check_region_stmt = select(State).where(
                    and_(
                        State.city_name == region_filter_city_name,
                        (State.is_deleted == False) | (State.is_deleted.is_(None))
                    )
                ).limit(5)
                check_region_result = await db.execute(check_region_stmt)
                existing_regions = check_region_result.scalars().all()
                if existing_regions:
                    logger.info(f"ğŸ” [Dashboard Rankings Region] í•´ë‹¹ ì‹œë„ëŠ” ì¡´ì¬í•˜ì§€ë§Œ ê±°ë˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‹œë„: {region_filter_city_name}")
                else:
                    logger.warning(f"âš ï¸ [Dashboard Rankings Region] í•´ë‹¹ ì‹œë„ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (city_name: {region_filter_city_name})")
        
        # ìš”ì¦˜ ê´€ì‹¬ ë§ì€ ì•„íŒŒíŠ¸ ì²˜ë¦¬ (ì§€ì •í•œ ì‹œë„ ë‚´ ì „ì²´ ì•„íŒŒíŠ¸ ëŒ€ìƒ, ì§€ì—­ë³„ë¡œ ë‚˜ëˆ„ì§€ ì•ŠìŒ)
        # ì´ë¯¸ ì¿¼ë¦¬ì—ì„œ .limit(10)ì´ ì ìš©ë˜ì–´ ìˆì§€ë§Œ, ì•ˆì „ì„ ìœ„í•´ ë‹¤ì‹œ ì œí•œ
        trending_apartments = []
        for row in trending_rows[:10]:  # ìµœëŒ€ 10ê°œë§Œ ì²˜ë¦¬
            trending_apartments.append({
                "apt_id": row.apt_id,
                "apt_name": row.apt_name or "-",
                "region": f"{row.city_name} {row.region_name}" if row.city_name and row.region_name else "-",
                "transaction_count": row.transaction_count or 0,
                "avg_price_per_pyeong": round(float(row.avg_price_per_pyeong or 0), 1)
            })
        
        # ì´ì „ ê¸°ê°„ ê°€ê²© ì²˜ë¦¬ (ì§€ì •í•œ ì‹œë„ ë‚´ ì „ì²´ ì•„íŒŒíŠ¸ ëŒ€ìƒ)
        previous_prices: Dict[int, Dict[str, Any]] = {}
        for row in previous_prices_rows:
            previous_prices[row.apt_id] = {
                "apt_name": row.apt_name or "-",
                "region": f"{row.city_name} {row.region_name}" if row.city_name and row.region_name else "-",
                "avg_price_per_pyeong": float(row.avg_price_per_pyeong or 0)
            }
        
        # ìµœê·¼ ê¸°ê°„ ê°€ê²© ì²˜ë¦¬ ë° ìƒìŠ¹ë¥ /í•˜ë½ë¥  ê³„ì‚° (ì§€ì •í•œ ì‹œë„ ë‚´ ì „ì²´ ì•„íŒŒíŠ¸ ëŒ€ìƒ)
        rising_apartments = []
        falling_apartments = []
        
        for row in recent_prices_rows:
            apt_id = row.apt_id
            recent_avg = float(row.avg_price_per_pyeong or 0)
            
            if apt_id not in previous_prices:
                continue
            
            previous_avg = previous_prices[apt_id]["avg_price_per_pyeong"]
            
            if previous_avg == 0 or recent_avg == 0:
                continue
            
            change_rate = ((recent_avg - previous_avg) / previous_avg) * 100
            
            apt_data = {
                "apt_id": apt_id,
                "apt_name": row.apt_name or previous_prices[apt_id]["apt_name"],
                "region": f"{row.city_name} {row.region_name}" if row.city_name and row.region_name else previous_prices[apt_id]["region"],
                "change_rate": round(change_rate, 2),
                "recent_avg": round(recent_avg, 1),
                "previous_avg": round(previous_avg, 1)
            }
            
            if change_rate > 0:
                rising_apartments.append(apt_data)
            elif change_rate < 0:
                falling_apartments.append(apt_data)
        
        # ì •ë ¬ ë° TOP 5 ì„ íƒ
        rising_apartments.sort(key=lambda x: x["change_rate"], reverse=True)
        falling_apartments.sort(key=lambda x: x["change_rate"])
        
        rising_apartments = rising_apartments[:5]
        falling_apartments = falling_apartments[:5]
        
        logger.info(f"ğŸ“Š [Dashboard Rankings Region] ìµœì¢… ê²°ê³¼ - trending: {len(trending_apartments)}, rising: {len(rising_apartments)}, falling: {len(falling_apartments)}")
        
        response_data = {
            "success": True,
            "data": {
                "trending": trending_apartments,  # ìš”ì¦˜ ê´€ì‹¬ ë§ì€ ì•„íŒŒíŠ¸ TOP 10
                "rising": rising_apartments,  # ìƒìŠ¹ë¥  TOP 5
                "falling": falling_apartments  # í•˜ë½ë¥  TOP 5
            }
        }
        
        logger.info(f"âœ… [Dashboard Rankings Region] ì‘ë‹µ ë°ì´í„° ìƒì„± ì™„ë£Œ")
        
        # ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ìºì‹œì— ì €ì¥
        has_data = (len(trending_apartments) > 0 or 
                    len(rising_apartments) > 0 or 
                    len(falling_apartments) > 0)
        
        if has_data:
            logger.info(f"ğŸ’¾ [Dashboard Rankings Region] ë°ì´í„°ê°€ ìˆìœ¼ë¯€ë¡œ ìºì‹œì— ì €ì¥")
            await set_to_cache(cache_key, response_data, ttl=1800)
        else:
            logger.warning(f"âš ï¸ [Dashboard Rankings Region] ë°ì´í„°ê°€ ì—†ìœ¼ë¯€ë¡œ ìºì‹œì— ì €ì¥í•˜ì§€ ì•ŠìŒ")
        
        return response_data
        
    except Exception as e:
        logger.error(f"âŒ [Dashboard Rankings Region] ì§€ì—­ë³„ ëŒ€ì‹œë³´ë“œ ë­í‚¹ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        logger.error(f"âŒ [Dashboard Rankings Region] ì—ëŸ¬ ìƒì„¸ ì •ë³´:", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )
