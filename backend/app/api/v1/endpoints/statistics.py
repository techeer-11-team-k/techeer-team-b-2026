"""
í†µê³„ ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸

ë‹´ë‹¹ ê¸°ëŠ¥:
- RVOL(ìƒëŒ€ ê±°ë˜ëŸ‰) ê³„ì‚° ë° ì¡°íšŒ
- 4ë¶„ë©´ ë¶„ë¥˜ (ë§¤ë§¤/ì „ì›”ì„¸ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨ ê¸°ë°˜)

ì„±ëŠ¥ ìµœì í™”:
- ê¸°ê°„ ì œí•œ: ìµœëŒ€ 2~3ê°œì›”
- ì›”ë³„ ì§‘ê³„ë¡œ ê°„ì†Œí™”
- ê¸´ ìºì‹œ TTL (6ì‹œê°„)
"""
import logging
import sys
import asyncio
from datetime import date, datetime, timedelta
from typing import Optional, List, Dict, Any
from fastapi import APIRouter, Depends, HTTPException, Query, status
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func, and_, or_, case, desc, text, extract
from sqlalchemy.orm import selectinload

from app.api.v1.deps import get_db
from app.models.sale import Sale
from app.models.rent import Rent
from app.models.apartment import Apartment
from app.models.state import State
from app.models.house_score import HouseScore
from app.models.population_movement import PopulationMovement
from app.schemas.statistics import (
    RVOLResponse,
    RVOLDataPoint,
    QuadrantResponse,
    QuadrantDataPoint,
    StatisticsSummaryResponse,
    HPIResponse,
    HPIDataPoint,
    HPIHeatmapResponse,
    HPIHeatmapDataPoint,
    PopulationMovementResponse,
    PopulationMovementDataPoint,
    PopulationMovementSankeyResponse,
    PopulationMovementSankeyDataPoint,
    CorrelationAnalysisResponse,
    TransactionVolumeResponse,
    MarketPhaseResponse,
    MarketPhaseDataPoint,
    HPIRegionTypeResponse,
    HPIRegionTypeDataPoint,
    PopulationMovementRegionTypeResponse,
    PopulationMovementRegionTypeDataPoint
)
from app.utils.cache import get_from_cache, set_to_cache, build_cache_key

# ë¡œê±° ì„¤ì • (Docker ë¡œê·¸ì— ì¶œë ¥ë˜ë„ë¡)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
if not logger.handlers:
    handler = logging.StreamHandler(sys.stdout)
    handler.setLevel(logging.INFO)
    formatter = logging.Formatter(
        '%(asctime)s | %(levelname)s | %(name)s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.propagate = True  # ë£¨íŠ¸ ë¡œê±°ë¡œë„ ì „íŒŒ

router = APIRouter()

# ìºì‹œ TTL: 6ì‹œê°„ (í†µê³„ ë°ì´í„°ëŠ” ìì£¼ ë³€í•˜ì§€ ì•ŠìŒ)
STATISTICS_CACHE_TTL = 21600


# ============================================================
# í—¬í¼ í•¨ìˆ˜
# ============================================================

def normalize_city_name(city_name: str) -> str:
    """
    ì‹œë„ëª…ì„ í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”
    
    Args:
        city_name: ì‹œë„ëª… (ì˜ˆ: "ì„œìš¸íŠ¹ë³„ì‹œ", "ë¶€ì‚°ê´‘ì—­ì‹œ")
    
    Returns:
        ì •ê·œí™”ëœ ì§€ì—­ëª… (ì˜ˆ: "ì„œìš¸", "ë¶€ì‚°")
    """
    mapping = {
        "ì„œìš¸íŠ¹ë³„ì‹œ": "ì„œìš¸",
        "ë¶€ì‚°ê´‘ì—­ì‹œ": "ë¶€ì‚°",
        "ëŒ€êµ¬ê´‘ì—­ì‹œ": "ëŒ€êµ¬",
        "ì¸ì²œê´‘ì—­ì‹œ": "ì¸ì²œ",
        "ê´‘ì£¼ê´‘ì—­ì‹œ": "ê´‘ì£¼",
        "ëŒ€ì „ê´‘ì—­ì‹œ": "ëŒ€ì „",
        "ìš¸ì‚°ê´‘ì—­ì‹œ": "ìš¸ì‚°",
        "ê²½ê¸°ë„": "ê²½ê¸°",
    }
    return mapping.get(city_name, city_name)


def get_region_type_filter(region_type: str):
    """
    ì§€ì—­ ìœ í˜•ì— ë”°ë¥¸ city_name í•„í„° ì¡°ê±´ ë°˜í™˜
    
    Args:
        region_type: ì§€ì—­ ìœ í˜• ("ì „êµ­", "ìˆ˜ë„ê¶Œ", "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ")
    
    Returns:
        SQLAlchemy í•„í„° ì¡°ê±´ (Noneì´ë©´ í•„í„° ì—†ìŒ)
    """
    if region_type == "ì „êµ­":
        return None
    elif region_type == "ìˆ˜ë„ê¶Œ":
        return State.city_name.in_(['ì„œìš¸íŠ¹ë³„ì‹œ', 'ê²½ê¸°ë„', 'ì¸ì²œê´‘ì—­ì‹œ'])
    elif region_type == "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ":
        return State.city_name.in_(['ë¶€ì‚°ê´‘ì—­ì‹œ', 'ëŒ€êµ¬ê´‘ì—­ì‹œ', 'ê´‘ì£¼ê´‘ì—­ì‹œ', 'ëŒ€ì „ê´‘ì—­ì‹œ', 'ìš¸ì‚°ê´‘ì—­ì‹œ'])
    else:
        raise ValueError(f"ìœ íš¨í•˜ì§€ ì•Šì€ region_type: {region_type}")


def calculate_market_phase(price_change_rate: float, volume_change_rate: float) -> tuple[str, str, str]:
    """
    ê°€ê²© ë³€í™”ìœ¨ê³¼ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹œì¥ êµ­ë©´ ë¶„ë¥˜
    
    Args:
        price_change_rate: ê°€ê²© ë³€í™”ìœ¨ (%)
        volume_change_rate: ê±°ë˜ëŸ‰ ë³€í™”ìœ¨ (%)
    
    Returns:
        (phase, trend, change) íŠœí”Œ
        - phase: "ìƒìŠ¹ê¸°", "íšŒë³µê¸°", "ì¹¨ì²´ê¸°", "í›„í‡´ê¸°"
        - trend: "up", "down"
        - change: ë³€í™”ìœ¨ ë¬¸ìì—´ (ì˜ˆ: "+1.5%")
    """
    if price_change_rate > 0 and volume_change_rate > 0:
        phase = "ìƒìŠ¹ê¸°"
        trend = "up"
    elif price_change_rate > 0 and volume_change_rate < 0:
        phase = "íšŒë³µê¸°"
        trend = "up"
    elif price_change_rate < 0 and volume_change_rate < 0:
        phase = "ì¹¨ì²´ê¸°"
        trend = "down"
    elif price_change_rate < 0 and volume_change_rate > 0:
        phase = "í›„í‡´ê¸°"
        trend = "down"
    else:
        # ë³€í™”ìœ¨ì´ 0ì¸ ê²½ìš°ëŠ” ì¤‘ë¦½ìœ¼ë¡œ ì²˜ë¦¬
        if price_change_rate == 0 and volume_change_rate == 0:
            phase = "ì¤‘ë¦½"
            trend = "up"
        elif price_change_rate == 0:
            phase = "íšŒë³µê¸°" if volume_change_rate > 0 else "ì¹¨ì²´ê¸°"
            trend = "up" if volume_change_rate > 0 else "down"
        else:
            phase = "ìƒìŠ¹ê¸°" if price_change_rate > 0 else "í›„í‡´ê¸°"
            trend = "up" if price_change_rate > 0 else "down"
    
    # ë³€í™”ìœ¨ ë¬¸ìì—´ ìƒì„± (ê°€ê²© ë³€í™”ìœ¨ ê¸°ì¤€)
    change_sign = "+" if price_change_rate >= 0 else ""
    change_str = f"{change_sign}{price_change_rate:.1f}%"
    
    return (phase, trend, change_str)


def calculate_quadrant(sale_change_rate: float, rent_change_rate: float) -> tuple[int, str]:
    """
    4ë¶„ë©´ ë¶„ë¥˜ ê³„ì‚°
    
    Args:
        sale_change_rate: ë§¤ë§¤ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨ (%)
        rent_change_rate: ì „ì›”ì„¸ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨ (%)
    
    Returns:
        (quadrant_number, quadrant_label) íŠœí”Œ
    """
    if sale_change_rate > 0 and rent_change_rate < 0:
        return (1, "ë§¤ìˆ˜ ì „í™˜")
    elif sale_change_rate < 0 and rent_change_rate > 0:
        return (2, "ì„ëŒ€ ì„ í˜¸/ê´€ë§")
    elif sale_change_rate < 0 and rent_change_rate < 0:
        return (3, "ì‹œì¥ ìœ„ì¶•")
    elif sale_change_rate > 0 and rent_change_rate > 0:
        return (4, "í™œì„±í™”")
    else:
        # ë³€í™”ìœ¨ì´ 0ì¸ ê²½ìš°ëŠ” ì¤‘ë¦½ìœ¼ë¡œ ì²˜ë¦¬
        if sale_change_rate == 0 and rent_change_rate == 0:
            return (0, "ì¤‘ë¦½")
        elif sale_change_rate == 0:
            return (2 if rent_change_rate > 0 else 3, "ì„ëŒ€ ì„ í˜¸/ê´€ë§" if rent_change_rate > 0 else "ì‹œì¥ ìœ„ì¶•")
        else:
            return (1 if sale_change_rate > 0 else 3, "ë§¤ìˆ˜ ì „í™˜" if sale_change_rate > 0 else "ì‹œì¥ ìœ„ì¶•")


@router.get(
    "/rvol",
    response_model=RVOLResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="RVOL(ìƒëŒ€ ê±°ë˜ëŸ‰) ì¡°íšŒ",
    description="""
    RVOL(Relative Volume)ì„ ê³„ì‚°í•˜ì—¬ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ### RVOL ê³„ì‚° ë°©ë²•
    - í˜„ì¬ ê±°ë˜ëŸ‰ì„ ê³¼ê±° ì¼ì • ê¸°ê°„ì˜ í‰ê·  ê±°ë˜ëŸ‰ìœ¼ë¡œ ë‚˜ëˆˆ ê°’
    - ì˜ˆ: ìµœê·¼ 2ê°œì›” ê±°ë˜ëŸ‰ Ã· ì§ì „ 2ê°œì›” í‰ê·  ê±°ë˜ëŸ‰
    
    ### í•´ì„
    - **RVOL > 1**: í‰ì†Œë³´ë‹¤ ê±°ë˜ê°€ í™œë°œí•¨ (í‰ê·  ì´ìƒ)
    - **RVOL = 1**: í‰ì†Œì™€ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì˜ ê±°ë˜ëŸ‰
    - **RVOL < 1**: í‰ì†Œë³´ë‹¤ ê±°ë˜ê°€ í•œì‚°í•¨ (í‰ê·  ì´í•˜)
    
    ### Query Parameters
    - `transaction_type`: ê±°ë˜ ìœ í˜• (sale: ë§¤ë§¤, rent: ì „ì›”ì„¸, ê¸°ë³¸ê°’: sale)
    - `current_period_months`: í˜„ì¬ ê¸°ê°„ (ê°œì›”, ê¸°ë³¸ê°’: 6, ìµœëŒ€: 6)
    - `average_period_months`: í‰ê·  ê³„ì‚° ê¸°ê°„ (ê°œì›”, ê¸°ë³¸ê°’: 6, ìµœëŒ€: 6)
    """
)
async def get_rvol(
    transaction_type: str = Query("sale", description="ê±°ë˜ ìœ í˜•: sale(ë§¤ë§¤), rent(ì „ì›”ì„¸)"),
    current_period_months: int = Query(6, ge=1, le=12, description="í˜„ì¬ ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 12)"),
    average_period_months: int = Query(6, ge=1, le=12, description="í‰ê·  ê³„ì‚° ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 12)"),
    db: AsyncSession = Depends(get_db)
):
    """
    RVOL(ìƒëŒ€ ê±°ë˜ëŸ‰) ì¡°íšŒ - ì„±ëŠ¥ ìµœì í™” ë²„ì „
    
    ì›”ë³„ ì§‘ê³„ë¡œ ê°„ì†Œí™”í•˜ì—¬ ë¹ ë¥¸ ì‘ë‹µ ì œê³µ
    """
    cache_key = build_cache_key(
        "statistics", "rvol_v2", transaction_type, 
        str(current_period_months), str(average_period_months)
    )
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        logger.info(f"âœ… [Statistics RVOL] ìºì‹œì—ì„œ ë°˜í™˜")
        return cached_data
    
    try:
        logger.info(
            f"ğŸ” [Statistics RVOL] RVOL ë°ì´í„° ì¡°íšŒ ì‹œì‘ - "
            f"transaction_type: {transaction_type}, "
            f"current_period_months: {current_period_months}, "
            f"average_period_months: {average_period_months}"
        )
        
        # ê±°ë˜ ìœ í˜•ì— ë”°ë¥¸ í…Œì´ë¸” ë° í•„ë“œ ì„ íƒ
        if transaction_type == "sale":
            trans_table = Sale
            date_field = Sale.contract_date
            base_filter = and_(
                Sale.is_canceled == False,
                (Sale.is_deleted == False) | (Sale.is_deleted.is_(None)),
                Sale.contract_date.isnot(None),
                or_(Sale.remarks != "ë”ë¯¸", Sale.remarks.is_(None))
            )
        else:  # rent
            trans_table = Rent
            date_field = Rent.deal_date
            base_filter = and_(
                (Rent.is_deleted == False) | (Rent.is_deleted.is_(None)),
                Rent.deal_date.isnot(None),
                or_(Rent.remarks != "ë”ë¯¸", Rent.remarks.is_(None))
            )
        
        # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ê¸°ê°„ ì„¤ì • (min/max ì¿¼ë¦¬ ì œê±°)
        today = date.today()
        # í˜„ì¬ ë‹¬ì˜ ì²« ë‚  (í˜„ì¬ ë‹¬ ì œì™¸)
        current_month_start = date(today.year, today.month, 1)
        
        # í˜„ì¬ ê¸°ê°„: ìµœê·¼ current_period_months ê°œì›” (í˜„ì¬ ë‹¬ ì œì™¸)
        current_start = current_month_start - timedelta(days=current_period_months * 30)
        current_end = current_month_start  # í˜„ì¬ ë‹¬ì˜ ì²« ë‚  ì „ê¹Œì§€
        
        # í‰ê·  ê³„ì‚° ê¸°ê°„: current_start ì´ì „ average_period_months ê°œì›”
        average_start = current_start - timedelta(days=average_period_months * 30)
        average_end = current_start
        
        logger.info(
            f"ğŸ“… [Statistics RVOL] ë‚ ì§œ ë²”ìœ„ - "
            f"current_start: {current_start}, current_end: {current_end}, "
            f"average_start: {average_start}, average_end: {average_end}"
        )
        
        # ì›”ë³„ ì§‘ê³„ë¡œ ê°„ì†Œí™” (ì¼ë³„ ëŒ€ì‹  ì›”ë³„)
        # í‰ê·  ê¸°ê°„ ì›”ë³„ ê±°ë˜ëŸ‰
        average_volume_stmt = (
            select(
                extract('year', date_field).label('year'),
                extract('month', date_field).label('month'),
                func.count(trans_table.trans_id).label('count')
            )
            .where(
                and_(
                    base_filter,
                    date_field >= average_start,
                    date_field < average_end
                )
            )
            .group_by(extract('year', date_field), extract('month', date_field))
        )
        
        # í˜„ì¬ ê¸°ê°„ ì›”ë³„ ê±°ë˜ëŸ‰
        current_volume_stmt = (
            select(
                extract('year', date_field).label('year'),
                extract('month', date_field).label('month'),
                func.count(trans_table.trans_id).label('count')
            )
            .where(
                and_(
                    base_filter,
                    date_field >= current_start,
                    date_field < current_end  # í˜„ì¬ ë‹¬ ì œì™¸ (ë¯¸ë§Œìœ¼ë¡œ ë³€ê²½)
                )
            )
            .group_by(extract('year', date_field), extract('month', date_field))
        )
        
        # ë³‘ë ¬ ì‹¤í–‰
        average_result, current_result = await asyncio.gather(
            db.execute(average_volume_stmt),
            db.execute(current_volume_stmt)
        )
        
        average_rows = average_result.fetchall()
        current_rows = current_result.fetchall()
        
        # í‰ê·  ê±°ë˜ëŸ‰ ê³„ì‚°
        if average_rows:
            total_average = sum(row.count for row in average_rows)
            average_monthly_volume = total_average / len(average_rows)
        else:
            average_monthly_volume = 1  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        
        logger.info(
            f"ğŸ“Š [Statistics RVOL] í‰ê·  ê±°ë˜ëŸ‰ ê³„ì‚° - "
            f"average_monthly_volume: {average_monthly_volume}"
        )
        
        # RVOL ë°ì´í„° ìƒì„± (ì›”ë³„) - í˜„ì¬ ë‹¬ ì œì™¸
        rvol_data = []
        current_year = today.year
        current_month = today.month
        
        for row in current_rows:
            year = int(row.year)
            month = int(row.month)
            
            # í˜„ì¬ ë‹¬ ì œì™¸
            if year == current_year and month == current_month:
                continue
                
            count = row.count or 0
            
            # RVOL ê³„ì‚°
            rvol = count / average_monthly_volume if average_monthly_volume > 0 else 0
            
            rvol_data.append(
                RVOLDataPoint(
                    date=f"{year}-{month:02d}-01",
                    current_volume=count,
                    average_volume=round(average_monthly_volume, 2),
                    rvol=round(rvol, 2)
                )
            )
        
        # ë‚ ì§œìˆœ ì •ë ¬
        rvol_data.sort(key=lambda x: x.date)
        
        period_description = f"ìµœê·¼ {current_period_months}ê°œì›” vs ì§ì „ {average_period_months}ê°œì›”"
        
        response_data = RVOLResponse(
            success=True,
            data=rvol_data,
            period=period_description
        )
        
        # ìºì‹œì— ì €ì¥ (TTL: 6ì‹œê°„)
        if len(rvol_data) > 0:
            await set_to_cache(cache_key, response_data.dict(), ttl=STATISTICS_CACHE_TTL)
        
        logger.info(f"âœ… [Statistics RVOL] RVOL ë°ì´í„° ìƒì„± ì™„ë£Œ - ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(rvol_data)}")
        
        return response_data
        
    except Exception as e:
        logger.error(f"âŒ [Statistics RVOL] RVOL ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"RVOL ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/quadrant",
    response_model=QuadrantResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="4ë¶„ë©´ ë¶„ë¥˜ ì¡°íšŒ",
    description="""
    ë§¤ë§¤ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨ê³¼ ì „ì›”ì„¸ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨ì„ ê¸°ë°˜ìœ¼ë¡œ 4ë¶„ë©´ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    ### 4ë¶„ë©´ ë¶„ë¥˜
    - **xì¶•**: ë§¤ë§¤ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨
    - **yì¶•**: ì „ì›”ì„¸ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨
    
    ### í•´ì„
    1. **ë§¤ë§¤â†‘ / ì „ì›”ì„¸â†“**: ë§¤ìˆ˜ ì „í™˜ (ì‚¬ëŠ” ìª½ìœ¼ë¡œ ì´ë™)
    2. **ë§¤ë§¤â†“ / ì „ì›”ì„¸â†‘**: ì„ëŒ€ ì„ í˜¸/ê´€ë§ (ë¹Œë¦¬ëŠ” ìª½ìœ¼ë¡œ ì´ë™)
    3. **ë§¤ë§¤â†“ / ì „ì›”ì„¸â†“**: ì‹œì¥ ìœ„ì¶• (ì „ì²´ ìœ ë™ì„± ê²½ìƒ‰)
    4. **ë§¤ë§¤â†‘ / ì „ì›”ì„¸â†‘**: í™œì„±í™” (ìˆ˜ìš” ìì²´ê°€ ê°•í•¨, ì´ì‚¬/ê±°ë˜ ì¦ê°€)
    
    ### Query Parameters
    - `period_months`: ë¹„êµ ê¸°ê°„ (ê°œì›”, ê¸°ë³¸ê°’: 2, ìµœëŒ€: 6)
    """
)
async def get_quadrant(
    period_months: int = Query(2, ge=1, le=12, description="ë¹„êµ ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 12)"),
    db: AsyncSession = Depends(get_db)
):
    """
    4ë¶„ë©´ ë¶„ë¥˜ ì¡°íšŒ - ì„±ëŠ¥ ìµœì í™” ë²„ì „
    
    ì›”ë³„ ì§‘ê³„ë¡œ ê°„ì†Œí™”í•˜ì—¬ ë¹ ë¥¸ ì‘ë‹µ ì œê³µ
    """
    cache_key = build_cache_key("statistics", "quadrant_v2", str(period_months))
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        logger.info(f"âœ… [Statistics Quadrant] ìºì‹œì—ì„œ ë°˜í™˜")
        return cached_data
    
    try:
        logger.info(
            f"ğŸ” [Statistics Quadrant] 4ë¶„ë©´ ë¶„ë¥˜ ë°ì´í„° ì¡°íšŒ ì‹œì‘ - "
            f"period_months: {period_months}"
        )
        
        # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ê¸°ê°„ ì„¤ì •
        today = date.today()
        # í˜„ì¬ ë‹¬ì˜ ì²« ë‚  (í˜„ì¬ ë‹¬ ì œì™¸)
        current_month_start = date(today.year, today.month, 1)
        
        # ìµœê·¼ ê¸°ê°„ê³¼ ì´ì „ ê¸°ê°„ ì„¤ì • (í˜„ì¬ ë‹¬ ì œì™¸)
        recent_start = current_month_start - timedelta(days=period_months * 30)
        recent_end = current_month_start  # í˜„ì¬ ë‹¬ì˜ ì²« ë‚  ì „ê¹Œì§€
        
        previous_start = recent_start - timedelta(days=period_months * 30)
        previous_end = recent_start
        
        logger.info(
            f"ğŸ“… [Statistics Quadrant] ë‚ ì§œ ë²”ìœ„ - "
            f"previous_start: {previous_start}, previous_end: {previous_end}, "
            f"recent_start: {recent_start}, recent_end: {recent_end}"
        )
        
        # ì›”ë³„ ì§‘ê³„ (to_char ëŒ€ì‹  extract ì‚¬ìš© - ì¸ë±ìŠ¤ í™œìš© ê°€ëŠ¥)
        # ë§¤ë§¤ ê±°ë˜ëŸ‰: ì´ì „ ê¸°ê°„
        sale_previous_stmt = (
            select(
                extract('year', Sale.contract_date).label('year'),
                extract('month', Sale.contract_date).label('month'),
                func.count(Sale.trans_id).label('count')
            )
            .where(
                and_(
                    Sale.is_canceled == False,
                    (Sale.is_deleted == False) | (Sale.is_deleted.is_(None)),
                    Sale.contract_date.isnot(None),
                    Sale.contract_date >= previous_start,
                    Sale.contract_date < previous_end,
                    or_(Sale.remarks != "ë”ë¯¸", Sale.remarks.is_(None))
                )
            )
            .group_by(extract('year', Sale.contract_date), extract('month', Sale.contract_date))
        )
        
        # ë§¤ë§¤ ê±°ë˜ëŸ‰: ìµœê·¼ ê¸°ê°„
        sale_recent_stmt = (
            select(
                extract('year', Sale.contract_date).label('year'),
                extract('month', Sale.contract_date).label('month'),
                func.count(Sale.trans_id).label('count')
            )
            .where(
                and_(
                    Sale.is_canceled == False,
                    (Sale.is_deleted == False) | (Sale.is_deleted.is_(None)),
                    Sale.contract_date.isnot(None),
                    Sale.contract_date >= recent_start,
                    Sale.contract_date < recent_end,  # í˜„ì¬ ë‹¬ ì œì™¸ (ë¯¸ë§Œìœ¼ë¡œ ë³€ê²½)
                    or_(Sale.remarks != "ë”ë¯¸", Sale.remarks.is_(None))
                )
            )
            .group_by(extract('year', Sale.contract_date), extract('month', Sale.contract_date))
        )
        
        # ì „ì›”ì„¸ ê±°ë˜ëŸ‰: ì´ì „ ê¸°ê°„
        rent_previous_stmt = (
            select(
                extract('year', Rent.deal_date).label('year'),
                extract('month', Rent.deal_date).label('month'),
                func.count(Rent.trans_id).label('count')
            )
            .where(
                and_(
                    (Rent.is_deleted == False) | (Rent.is_deleted.is_(None)),
                    Rent.deal_date.isnot(None),
                    Rent.deal_date >= previous_start,
                    Rent.deal_date < previous_end,
                    or_(Rent.remarks != "ë”ë¯¸", Rent.remarks.is_(None))
                )
            )
            .group_by(extract('year', Rent.deal_date), extract('month', Rent.deal_date))
        )
        
        # ì „ì›”ì„¸ ê±°ë˜ëŸ‰: ìµœê·¼ ê¸°ê°„
        rent_recent_stmt = (
            select(
                extract('year', Rent.deal_date).label('year'),
                extract('month', Rent.deal_date).label('month'),
                func.count(Rent.trans_id).label('count')
            )
            .where(
                and_(
                    (Rent.is_deleted == False) | (Rent.is_deleted.is_(None)),
                    Rent.deal_date.isnot(None),
                    Rent.deal_date >= recent_start,
                    Rent.deal_date < recent_end,  # í˜„ì¬ ë‹¬ ì œì™¸ (ë¯¸ë§Œìœ¼ë¡œ ë³€ê²½)
                    or_(Rent.remarks != "ë”ë¯¸", Rent.remarks.is_(None))
                )
            )
            .group_by(extract('year', Rent.deal_date), extract('month', Rent.deal_date))
        )
        
        # ì¿¼ë¦¬ ë³‘ë ¬ ì‹¤í–‰ (ì„±ëŠ¥ ìµœì í™”)
        sale_previous_result, sale_recent_result, rent_previous_result, rent_recent_result = await asyncio.gather(
            db.execute(sale_previous_stmt),
            db.execute(sale_recent_stmt),
            db.execute(rent_previous_stmt),
            db.execute(rent_recent_stmt)
        )
        
        sale_previous_rows = sale_previous_result.fetchall()
        sale_recent_rows = sale_recent_result.fetchall()
        rent_previous_rows = rent_previous_result.fetchall()
        rent_recent_rows = rent_recent_result.fetchall()
        
        # ì´ì „ ê¸°ê°„ í‰ê·  ê³„ì‚°
        sale_previous_total = sum(row.count for row in sale_previous_rows) if sale_previous_rows else 0
        rent_previous_total = sum(row.count for row in rent_previous_rows) if rent_previous_rows else 0
        
        sale_previous_avg = sale_previous_total / len(sale_previous_rows) if sale_previous_rows else 1
        rent_previous_avg = rent_previous_total / len(rent_previous_rows) if rent_previous_rows else 1
        
        # ìµœê·¼ ê¸°ê°„ ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        sale_recent_dict = {f"{int(row.year)}-{int(row.month):02d}": row.count for row in sale_recent_rows}
        rent_recent_dict = {f"{int(row.year)}-{int(row.month):02d}": row.count for row in rent_recent_rows}
        
        # ëª¨ë“  ê¸°ê°„ ìˆ˜ì§‘ (í˜„ì¬ ë‹¬ ì œì™¸)
        all_periods = set(sale_recent_dict.keys()) | set(rent_recent_dict.keys())
        current_year = today.year
        current_month = today.month
        current_period_key = f"{current_year}-{current_month:02d}"
        
        # í˜„ì¬ ë‹¬ ì œì™¸
        all_periods.discard(current_period_key)
        
        quadrant_data = []
        quadrant_counts = {1: 0, 2: 0, 3: 0, 4: 0}
        
        for period in sorted(all_periods):
            sale_recent_count = sale_recent_dict.get(period, 0)
            rent_recent_count = rent_recent_dict.get(period, 0)
            
            # ë³€í™”ìœ¨ ê³„ì‚°
            sale_change_rate = ((sale_recent_count - sale_previous_avg) / sale_previous_avg * 100) if sale_previous_avg > 0 else 0
            rent_change_rate = ((rent_recent_count - rent_previous_avg) / rent_previous_avg * 100) if rent_previous_avg > 0 else 0
            
            # 4ë¶„ë©´ ë¶„ë¥˜
            quadrant_num, quadrant_label = calculate_quadrant(sale_change_rate, rent_change_rate)
            
            if quadrant_num > 0:
                quadrant_counts[quadrant_num] = quadrant_counts.get(quadrant_num, 0) + 1
            
            quadrant_data.append(
                QuadrantDataPoint(
                    date=period,
                    sale_volume_change_rate=round(sale_change_rate, 2),
                    rent_volume_change_rate=round(rent_change_rate, 2),
                    quadrant=quadrant_num,
                    quadrant_label=quadrant_label
                )
            )
        
        summary = {
            "total_periods": len(quadrant_data),
            "quadrant_distribution": quadrant_counts,
            "sale_previous_avg": round(sale_previous_avg, 2),
            "rent_previous_avg": round(rent_previous_avg, 2)
        }
        
        response_data = QuadrantResponse(
            success=True,
            data=quadrant_data,
            summary=summary
        )
        
        # ìºì‹œì— ì €ì¥ (TTL: 6ì‹œê°„)
        if len(quadrant_data) > 0:
            await set_to_cache(cache_key, response_data.dict(), ttl=STATISTICS_CACHE_TTL)
        
        logger.info(f"âœ… [Statistics Quadrant] 4ë¶„ë©´ ë¶„ë¥˜ ë°ì´í„° ìƒì„± ì™„ë£Œ - ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(quadrant_data)}")
        
        return response_data
        
    except Exception as e:
        logger.error(f"âŒ [Statistics Quadrant] 4ë¶„ë©´ ë¶„ë¥˜ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"4ë¶„ë©´ ë¶„ë¥˜ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/hpi",
    response_model=HPIResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="ì£¼íƒê°€ê²©ì§€ìˆ˜(HPI) ì¡°íšŒ",
    description="""
    ì£¼íƒê°€ê²©ì§€ìˆ˜(Housing Price Index, HPI)ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ### ì£¼íƒê°€ê²©ì§€ìˆ˜ë€?
    íŠ¹ì • ì‹œì ì˜ ì£¼íƒ ê°€ê²©ì„ ê¸°ì¤€(100)ìœ¼ë¡œ ì¡ê³ , ì´í›„ ê°€ê²©ì´ ì–¼ë§ˆë‚˜ ë³€í–ˆëŠ”ì§€ë¥¼ ìˆ˜ì¹˜í™”í•œ í†µê³„ ì§€í‘œì…ë‹ˆë‹¤.
    
    ### ì§€ìˆ˜ í•´ì„
    - **ì§€ìˆ˜ > 100**: ê¸°ì¤€ ì‹œì ë³´ë‹¤ ì§‘ê°’ì´ ì˜¬ëìŒ
    - **ì§€ìˆ˜ = 100**: ê¸°ì¤€ ì‹œì ê³¼ ë™ì¼
    - **ì§€ìˆ˜ < 100**: ê¸°ì¤€ ì‹œì ë³´ë‹¤ ì§‘ê°’ì´ ë‚´ë ¸ìŒ
    
    ### Query Parameters
    - `region_id`: ì§€ì—­ ID (ì„ íƒ, ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì „ì²´ ì§€ì—­ í‰ê· )
    - `index_type`: ì§€ìˆ˜ ìœ í˜• (APT: ì•„íŒŒíŠ¸, HOUSE: ë‹¨ë…ì£¼íƒ, ALL: ì „ì²´, ê¸°ë³¸ê°’: APT)
    - `months`: ì¡°íšŒ ê¸°ê°„ (ê°œì›”, ê¸°ë³¸ê°’: 24, ìµœëŒ€: 60)
    """
)
async def get_hpi(
    region_id: Optional[int] = Query(None, description="ì§€ì—­ ID (ì„ íƒ)"),
    index_type: str = Query("APT", description="ì§€ìˆ˜ ìœ í˜•: APT(ì•„íŒŒíŠ¸), HOUSE(ë‹¨ë…ì£¼íƒ), ALL(ì „ì²´)"),
    months: int = Query(24, ge=1, le=60, description="ì¡°íšŒ ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 60)"),
    db: AsyncSession = Depends(get_db)
):
    """
    ì£¼íƒê°€ê²©ì§€ìˆ˜(HPI) ì¡°íšŒ
    
    ì§€ì—­ë³„ ì£¼íƒê°€ê²©ì§€ìˆ˜ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    # ìœ íš¨í•œ index_type ê²€ì¦
    valid_index_types = ["APT", "HOUSE", "ALL"]
    if index_type not in valid_index_types:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ index_typeì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê°’: {', '.join(valid_index_types)}"
        )
    
    cache_key = build_cache_key(
        "statistics", "hpi", 
        str(region_id) if region_id else "all",
        index_type,
        str(months)
    )
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        logger.info(f"âœ… [Statistics HPI] ìºì‹œì—ì„œ ë°˜í™˜")
        return cached_data
    
    try:
        logger.info(
            f"ğŸ” [Statistics HPI] HPI ë°ì´í„° ì¡°íšŒ ì‹œì‘ - "
            f"region_id: {region_id}, index_type: {index_type}, months: {months}"
        )
        
        # ê¸°ì¤€ ë‚ ì§œ ê³„ì‚° (í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ìµœê·¼ monthsê°œì›”)
        today = date.today()
        # base_ymì€ YYYYMM í˜•ì‹ì´ë¯€ë¡œ, í˜„ì¬ ë…„ì›”ì„ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
        current_year = today.year
        current_month = today.month
        
        # ìµœì†Œ base_ym ê³„ì‚° (monthsê°œì›” ì „)
        # ì›” ë‹¨ìœ„ë¡œ ê³„ì‚°
        total_months = current_year * 12 + current_month - 1
        start_total_months = total_months - months + 1  # í˜„ì¬ ë‹¬ í¬í•¨
        start_year = start_total_months // 12
        start_month = (start_total_months % 12) + 1
        
        start_base_ym = f"{start_year:04d}{start_month:02d}"
        end_base_ym = f"{current_year:04d}{current_month:02d}"
        
        logger.info(
            f"ğŸ“… [Statistics HPI] ë‚ ì§œ ë²”ìœ„ - "
            f"start_base_ym: {start_base_ym}, end_base_ym: {end_base_ym}"
        )
        
        # ì¿¼ë¦¬ êµ¬ì„±
        # region_idê°€ ì§€ì •ëœ ê²½ìš°: íŠ¹ì • ì§€ì—­ë§Œ ì¡°íšŒ
        if region_id is not None:
            query = (
                select(
                    HouseScore.base_ym,
                    HouseScore.index_value,
                    HouseScore.index_change_rate,
                    HouseScore.index_type,
                    State.city_name.label('region_name')  # ì‹œë„ëª… ì‚¬ìš©
                )
                .join(State, HouseScore.region_id == State.region_id)
                .where(
                    and_(
                        HouseScore.region_id == region_id,
                        HouseScore.is_deleted == False,
                        HouseScore.index_type == index_type,
                        HouseScore.base_ym >= start_base_ym,
                        HouseScore.base_ym <= end_base_ym
                    )
                )
                .order_by(HouseScore.base_ym)
            )
        else:
            # region_idê°€ ì—†ëŠ” ê²½ìš°: ì‹œë„(city_name) ë ˆë²¨ë¡œ ê·¸ë£¹í™” (ì¸êµ¬ ì´ë™ ë°ì´í„°ì™€ ë™ì¼í•œ ë ˆë²¨)
            query = (
                select(
                    HouseScore.base_ym,
                    func.avg(HouseScore.index_value).label('index_value'),
                    func.avg(HouseScore.index_change_rate).label('index_change_rate'),
                    func.max(HouseScore.index_type).label('index_type'),
                    State.city_name.label('region_name')  # ì‹œë„ëª…ìœ¼ë¡œ ê·¸ë£¹í™”
                )
                .join(State, HouseScore.region_id == State.region_id)
                .where(
                    and_(
                        HouseScore.is_deleted == False,
                        State.is_deleted == False,
                        HouseScore.index_type == index_type,
                        HouseScore.base_ym >= start_base_ym,
                        HouseScore.base_ym <= end_base_ym
                    )
                )
                .group_by(HouseScore.base_ym, State.city_name)
                .order_by(HouseScore.base_ym, State.city_name)
            )
        
        result = await db.execute(query)
        rows = result.fetchall()
        
        logger.info(
            f"ğŸ“Š [Statistics HPI] ì¿¼ë¦¬ ê²°ê³¼ - "
            f"ì´ {len(rows)}ê±´ ì¡°íšŒë¨"
        )
        
        # ì‹œë„ë³„ ë°ì´í„° ê°œìˆ˜ í™•ì¸
        if rows:
            region_counts = {}
            for row in rows:
                region_name = row.region_name if hasattr(row, 'region_name') and row.region_name else "Unknown"
                region_counts[region_name] = region_counts.get(region_name, 0) + 1
            
            logger.info(
                f"ğŸ“‹ [Statistics HPI] ì‹œë„ë³„ ë°ì´í„° ê°œìˆ˜ - "
                f"{', '.join([f'{k}: {v}ê±´' for k, v in sorted(region_counts.items())])}"
            )
        
        # ë°ì´í„° í¬ì¸íŠ¸ ìƒì„±
        hpi_data = []
        for row in rows:
            base_ym = row.base_ym
            # YYYYMM -> YYYY-MM í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            year = base_ym[:4]
            month = base_ym[4:6]
            date_str = f"{year}-{month}"
            
            index_value = float(row.index_value) if row.index_value is not None else 0.0
            index_change_rate = float(row.index_change_rate) if row.index_change_rate is not None else None
            
            # region_name ì²˜ë¦¬: ì‹œë„ëª…(city_name) ì‚¬ìš©
            region_name = row.region_name if hasattr(row, 'region_name') and row.region_name else None
            
            hpi_data.append(
                HPIDataPoint(
                    date=date_str,
                    index_value=round(index_value, 2),
                    index_change_rate=round(index_change_rate, 2) if index_change_rate is not None else None,
                    region_name=region_name,
                    index_type=index_type
                )
            )
        
        # ë‚ ì§œìˆœ ì •ë ¬ (ì´ë¯¸ ì •ë ¬ë˜ì–´ ìˆì§€ë§Œ í™•ì‹¤íˆ)
        hpi_data.sort(key=lambda x: x.date)
        
        # ì§€ì—­ë³„/ë‚ ì§œë³„ ë°ì´í„° ê°œìˆ˜ í™•ì¸
        if hpi_data:
            date_counts = {}
            region_date_counts = {}
            for item in hpi_data:
                date_counts[item.date] = date_counts.get(item.date, 0) + 1
                if item.region_name:
                    key = f"{item.region_name}-{item.date}"
                    region_date_counts[key] = region_date_counts.get(key, 0) + 1
            
            logger.info(
                f"ğŸ“ˆ [Statistics HPI] ë°ì´í„° í¬ì¸íŠ¸ ìƒì„¸ - "
                f"ì´ {len(hpi_data)}ê±´, "
                f"ë‚ ì§œë³„ ê°œìˆ˜: {dict(sorted(date_counts.items())[:5])}... (ìµœì‹  5ê°œë§Œ í‘œì‹œ), "
                f"ì‹œë„ ìˆ˜: {len(set(item.region_name for item in hpi_data if item.region_name))}ê°œ"
            )
            
            # ê° ì‹œë„ë³„ ìµœì‹  ë°ì´í„° ìƒ˜í”Œ ë¡œê¹…
            latest_by_region = {}
            for item in reversed(hpi_data):  # ìµœì‹ ë¶€í„°
                if item.region_name and item.region_name not in latest_by_region:
                    latest_by_region[item.region_name] = item
            
            if latest_by_region:
                sample_regions = list(latest_by_region.items())[:5]  # ìµœëŒ€ 5ê°œë§Œ
                logger.info(
                    f"ğŸ“ [Statistics HPI] ì‹œë„ë³„ ìµœì‹  ë°ì´í„° ìƒ˜í”Œ - "
                    f"{', '.join([f'{r}: {d.date} {d.index_value}' for r, d in sample_regions])}"
                )
        
        region_desc = f"ì§€ì—­ ID {region_id}" if region_id else "ì „ì²´ ì§€ì—­ í‰ê· "
        period_desc = f"{months}ê°œì›” ({hpi_data[0].date if hpi_data else 'N/A'} ~ {hpi_data[-1].date if hpi_data else 'N/A'})"
        
        response_data = HPIResponse(
            success=True,
            data=hpi_data,
            region_id=region_id,
            index_type=index_type,
            period=f"{region_desc}, {index_type}, {period_desc}"
        )
        
        # ìºì‹œì— ì €ì¥ (TTL: 6ì‹œê°„)
        if len(hpi_data) > 0:
            await set_to_cache(cache_key, response_data.dict(), ttl=STATISTICS_CACHE_TTL)
        
        logger.info(f"âœ… [Statistics HPI] HPI ë°ì´í„° ìƒì„± ì™„ë£Œ - ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(hpi_data)}")
        
        return response_data
        
    except Exception as e:
        logger.error(f"âŒ [Statistics HPI] HPI ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"HPI ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/hpi/heatmap",
    response_model=HPIHeatmapResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="ì£¼íƒê°€ê²©ì§€ìˆ˜(HPI) íˆíŠ¸ë§µ ì¡°íšŒ",
    description="""
    ê´‘ì—­ì‹œ/íŠ¹ë³„ì‹œ/ë„ë³„ ì£¼íƒê°€ê²©ì§€ìˆ˜ë¥¼ íˆíŠ¸ë§µ í˜•ì‹ìœ¼ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ê° ë„/ì‹œì˜ ìµœì‹  HPI ê°’ì„ ë°˜í™˜í•˜ì—¬ ì§€ì—­ë³„ ê°€ê²© ì¶”ì´ë¥¼ í•œëˆˆì— ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    ### Query Parameters
    - `index_type`: ì§€ìˆ˜ ìœ í˜• (APT: ì•„íŒŒíŠ¸, HOUSE: ë‹¨ë…ì£¼íƒ, ALL: ì „ì²´, ê¸°ë³¸ê°’: APT)
    """
)
async def get_hpi_heatmap(
    index_type: str = Query("APT", description="ì§€ìˆ˜ ìœ í˜•: APT(ì•„íŒŒíŠ¸), HOUSE(ë‹¨ë…ì£¼íƒ), ALL(ì „ì²´)"),
    db: AsyncSession = Depends(get_db)
):
    """
    ì£¼íƒê°€ê²©ì§€ìˆ˜(HPI) íˆíŠ¸ë§µ ì¡°íšŒ
    
    ë„/ì‹œë³„ ìµœì‹  HPI ê°’ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    # ìœ íš¨í•œ index_type ê²€ì¦
    valid_index_types = ["APT", "HOUSE", "ALL"]
    if index_type not in valid_index_types:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ index_typeì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê°’: {', '.join(valid_index_types)}"
        )
    
    cache_key = build_cache_key("statistics", "hpi_heatmap", index_type)
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        logger.info(f"âœ… [Statistics HPI Heatmap] ìºì‹œì—ì„œ ë°˜í™˜")
        return cached_data
    
    try:
        logger.info(
            f"ğŸ” [Statistics HPI Heatmap] HPI íˆíŠ¸ë§µ ë°ì´í„° ì¡°íšŒ ì‹œì‘ - "
            f"index_type: {index_type}"
        )
        
        # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ìµœì‹  base_ym ì°¾ê¸°
        today = date.today()
        current_year = today.year
        current_month = today.month
        current_base_ym = f"{current_year:04d}{current_month:02d}"
        
        # ìµœì‹  base_ymë¶€í„° ì—­ìˆœìœ¼ë¡œ ì°¾ê¸° (ìµœëŒ€ 12ê°œì›” ì „ê¹Œì§€)
        found_base_ym = None
        for i in range(12):
            check_year = current_year
            check_month = current_month - i
            if check_month <= 0:
                check_year -= 1
                check_month += 12
            check_base_ym = f"{check_year:04d}{check_month:02d}"
            
            # í•´ë‹¹ base_ymì— ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
            check_query = (
                select(func.count(HouseScore.index_id))
                .where(
                    and_(
                        HouseScore.is_deleted == False,
                        HouseScore.index_type == index_type,
                        HouseScore.base_ym == check_base_ym
                    )
                )
            )
            check_result = await db.execute(check_query)
            count = check_result.scalar() or 0
            
            if count > 0:
                found_base_ym = check_base_ym
                break
        
        if not found_base_ym:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="HPI ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        logger.info(f"ğŸ“… [Statistics HPI Heatmap] ì‚¬ìš©í•  base_ym: {found_base_ym}")
        
        # ë„/ì‹œë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í‰ê·  HPI ê³„ì‚°
        query = (
            select(
                State.city_name,
                func.avg(HouseScore.index_value).label('index_value'),
                func.avg(HouseScore.index_change_rate).label('index_change_rate'),
                func.count(HouseScore.index_id).label('region_count')
            )
            .join(State, HouseScore.region_id == State.region_id)
            .where(
                and_(
                    HouseScore.is_deleted == False,
                    State.is_deleted == False,
                    HouseScore.index_type == index_type,
                    HouseScore.base_ym == found_base_ym
                )
            )
            .group_by(State.city_name)
            .order_by(State.city_name)
        )
        
        result = await db.execute(query)
        rows = result.fetchall()
        
        # ë°ì´í„° í¬ì¸íŠ¸ ìƒì„±
        heatmap_data = []
        for row in rows:
            city_name = row.city_name
            index_value = float(row.index_value) if row.index_value is not None else 0.0
            index_change_rate = float(row.index_change_rate) if row.index_change_rate is not None else None
            region_count = int(row.region_count) if row.region_count else 0
            
            heatmap_data.append(
                HPIHeatmapDataPoint(
                    city_name=city_name,
                    index_value=round(index_value, 2),
                    index_change_rate=round(index_change_rate, 2) if index_change_rate is not None else None,
                    base_ym=found_base_ym,
                    region_count=region_count
                )
            )
        
        # ë„/ì‹œëª… ìˆœìœ¼ë¡œ ì •ë ¬
        heatmap_data.sort(key=lambda x: x.city_name)
        
        response_data = HPIHeatmapResponse(
            success=True,
            data=heatmap_data,
            index_type=index_type,
            base_ym=found_base_ym
        )
        
        # ìºì‹œì— ì €ì¥ (TTL: 6ì‹œê°„)
        if len(heatmap_data) > 0:
            await set_to_cache(cache_key, response_data.dict(), ttl=STATISTICS_CACHE_TTL)
        
        logger.info(f"âœ… [Statistics HPI Heatmap] HPI íˆíŠ¸ë§µ ë°ì´í„° ìƒì„± ì™„ë£Œ - ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(heatmap_data)}")
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [Statistics HPI Heatmap] HPI íˆíŠ¸ë§µ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"HPI íˆíŠ¸ë§µ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/summary",
    response_model=StatisticsSummaryResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="í†µê³„ ìš”ì•½ ì¡°íšŒ",
    description="""
    RVOLê³¼ 4ë¶„ë©´ ë¶„ë¥˜ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¡°íšŒí•©ë‹ˆë‹¤.
    """
)
async def get_statistics_summary(
    transaction_type: str = Query("sale", description="ê±°ë˜ ìœ í˜•: sale(ë§¤ë§¤), rent(ì „ì›”ì„¸)"),
    current_period_months: int = Query(6, ge=1, le=12, description="í˜„ì¬ ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 12)"),
    average_period_months: int = Query(6, ge=1, le=12, description="í‰ê·  ê³„ì‚° ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 12)"),
    quadrant_period_months: int = Query(2, ge=1, le=12, description="4ë¶„ë©´ ë¹„êµ ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 12)"),
    db: AsyncSession = Depends(get_db)
):
    """
    í†µê³„ ìš”ì•½ ì¡°íšŒ
    
    RVOLê³¼ 4ë¶„ë©´ ë¶„ë¥˜ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    # RVOLê³¼ 4ë¶„ë©´ ë¶„ë¥˜ë¥¼ ë³‘ë ¬ë¡œ ì¡°íšŒ
    rvol_task = get_rvol(transaction_type, current_period_months, average_period_months, db)
    quadrant_task = get_quadrant(quadrant_period_months, db)
    
    rvol_response, quadrant_response = await asyncio.gather(rvol_task, quadrant_task)
    
    return StatisticsSummaryResponse(
        success=True,
        rvol=rvol_response,
        quadrant=quadrant_response
    )


@router.get(
    "/population-movements",
    response_model=PopulationMovementResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="ì¸êµ¬ ì´ë™ ë°ì´í„° ì¡°íšŒ",
    description="""
    ì§€ì—­ë³„ ì¸êµ¬ ì´ë™ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ### Query Parameters
    - `region_id`: ì§€ì—­ ID (ì„ íƒ, ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì „ì²´)
    - `start_ym`: ì‹œì‘ ë…„ì›” (YYYYMM, ê¸°ë³¸ê°’: ìµœê·¼ 12ê°œì›”)
    - `end_ym`: ì¢…ë£Œ ë…„ì›” (YYYYMM, ê¸°ë³¸ê°’: í˜„ì¬)
    """
)
async def get_population_movements(
    region_id: Optional[int] = Query(None, description="ì§€ì—­ ID (ì„ íƒ)"),
    start_ym: Optional[str] = Query(None, description="ì‹œì‘ ë…„ì›” (YYYYMM)"),
    end_ym: Optional[str] = Query(None, description="ì¢…ë£Œ ë…„ì›” (YYYYMM)"),
    db: AsyncSession = Depends(get_db)
):
    """
    ì¸êµ¬ ì´ë™ ë°ì´í„° ì¡°íšŒ
    """
    try:
        # ê¸°ë³¸ ê¸°ê°„ ì„¤ì • (ìµœê·¼ 12ê°œì›”)
        if not end_ym:
            end_date = datetime.now()
            end_ym = end_date.strftime("%Y%m")
        
        if not start_ym:
            start_date = datetime.now() - timedelta(days=365)
            start_ym = start_date.strftime("%Y%m")
        
        # ì¿¼ë¦¬ êµ¬ì„±: ì‹œë„ ë ˆë²¨ ë°ì´í„°ë§Œ ì¡°íšŒ (city_name ì‚¬ìš©)
        query = select(
            PopulationMovement,
            State.city_name  # ì‹œë„ëª… ì‚¬ìš© (ì˜ˆ: ì„œìš¸íŠ¹ë³„ì‹œ, ë¶€ì‚°ê´‘ì—­ì‹œ)
        ).join(
            State, PopulationMovement.region_id == State.region_id
        ).where(
            and_(
                PopulationMovement.base_ym >= start_ym,
                PopulationMovement.base_ym <= end_ym,
                PopulationMovement.is_deleted == False
            )
        )
        
        if region_id:
            query = query.where(PopulationMovement.region_id == region_id)
        
        query = query.order_by(PopulationMovement.base_ym.desc())
        
        result = await db.execute(query)
        rows = result.all()
        
        logger.info(
            f"ğŸ“Š [Statistics Population Movement] ì¸êµ¬ ì´ë™ ë°ì´í„° ì¡°íšŒ - "
            f"ì´ {len(rows)}ê±´ ì¡°íšŒë¨"
        )
        
        # ì§€ì—­ë³„ ë°ì´í„° ê°œìˆ˜ í™•ì¸
        if rows:
            region_counts = {}
            region_net_totals = {}  # ì§€ì—­ë³„ ìˆœì´ë™ í•©ê³„
            for movement, city_name in rows:
                region_name = city_name or "Unknown"
                region_counts[region_name] = region_counts.get(region_name, 0) + 1
                # ìˆœì´ë™ í•©ê³„ ê³„ì‚°
                if region_name not in region_net_totals:
                    region_net_totals[region_name] = 0
                region_net_totals[region_name] += movement.net_migration or 0
            
            logger.info(
                f"ğŸ“‹ [Statistics Population Movement] ì‹œë„ë³„ ë°ì´í„° ê°œìˆ˜ - "
                f"{', '.join([f'{k}: {v}ê±´' for k, v in sorted(region_counts.items())])}"
            )
            
            logger.info(
                f"ğŸ“Š [Statistics Population Movement] ì‹œë„ë³„ ìˆœì´ë™ í•©ê³„ - "
                f"{', '.join([f'{k}: {v}ëª…' for k, v in sorted(region_net_totals.items())])}"
            )
        
        data_points = []
        for movement, city_name in rows:
            # YYYYMM -> YYYY-MM ë³€í™˜
            year = movement.base_ym[:4]
            month = movement.base_ym[4:]
            date_str = f"{year}-{month}"
            
            data_points.append(PopulationMovementDataPoint(
                date=date_str,
                region_id=movement.region_id,
                region_name=city_name,  # ì‹œë„ëª… ë°˜í™˜
                in_migration=movement.in_migration,
                out_migration=movement.out_migration,
                net_migration=movement.net_migration
            ))
        
        return PopulationMovementResponse(
            success=True,
            data=data_points,
            period=f"{start_ym} ~ {end_ym}"
        )
        
    except Exception as e:
        logger.error(f"âŒ ì¸êµ¬ ì´ë™ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì¸êµ¬ ì´ë™ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        )


# ============================================================
# ì£¼íƒ ìˆ˜ìš” í˜ì´ì§€ìš© ìƒˆë¡œìš´ API
# ============================================================

@router.get(
    "/transaction-volume",
    response_model=TransactionVolumeResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="ì§€ì—­ë³„ ì›”ë³„/ë…„ë„ë³„ ê±°ë˜ëŸ‰ ì¡°íšŒ",
    description="""
    ì§€ì—­ ìœ í˜•ë³„ë¡œ ì›”ë³„ ë˜ëŠ” ë…„ë„ë³„ ê±°ë˜ëŸ‰ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ### ì§€ì—­ ìœ í˜•
    - **ì „êµ­**: ëª¨ë“  ì§€ì—­
    - **ìˆ˜ë„ê¶Œ**: ì„œìš¸íŠ¹ë³„ì‹œ + ê²½ê¸°ë„ + ì¸ì²œê´‘ì—­ì‹œ
    - **ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ**: ë¶€ì‚°ê´‘ì—­ì‹œ, ëŒ€êµ¬ê´‘ì—­ì‹œ, ê´‘ì£¼ê´‘ì—­ì‹œ, ëŒ€ì „ê´‘ì—­ì‹œ, ìš¸ì‚°ê´‘ì—­ì‹œ
    
    ### Query Parameters
    - `region_type`: ì§€ì—­ ìœ í˜• (required)
    - `period_type`: ê¸°ê°„ ìœ í˜• (required, "monthly" | "yearly")
    - `year_range`: ë…„ë„ ë²”ìœ„ (optional, monthlyì¼ ë•Œë§Œ, 2 | 3 | 5)
    - `start_year`: ì‹œì‘ ì—°ë„ (optional, yearlyì¼ ë•Œë§Œ)
    - `end_year`: ì¢…ë£Œ ì—°ë„ (optional, yearlyì¼ ë•Œë§Œ)
    - `transaction_type`: ê±°ë˜ ìœ í˜• (optional, "sale" | "rent", ê¸°ë³¸ê°’: "sale")
    """
)
async def get_transaction_volume(
    region_type: str = Query(..., description="ì§€ì—­ ìœ í˜•: ì „êµ­, ìˆ˜ë„ê¶Œ, ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"),
    period_type: str = Query(..., description="ê¸°ê°„ ìœ í˜•: monthly(ì›”ë³„), yearly(ë…„ë„ë³„)"),
    year_range: Optional[int] = Query(None, ge=2, le=5, description="ë…„ë„ ë²”ìœ„ (ì›”ë³„ì¼ ë•Œë§Œ, 2 | 3 | 5)"),
    start_year: Optional[int] = Query(None, ge=2000, le=2100, description="ì‹œì‘ ì—°ë„ (ë…„ë„ë³„ì¼ ë•Œë§Œ)"),
    end_year: Optional[int] = Query(None, ge=2000, le=2100, description="ì¢…ë£Œ ì—°ë„ (ë…„ë„ë³„ì¼ ë•Œë§Œ)"),
    transaction_type: str = Query("sale", description="ê±°ë˜ ìœ í˜•: sale(ë§¤ë§¤), rent(ì „ì›”ì„¸)"),
    db: AsyncSession = Depends(get_db)
):
    """
    ì§€ì—­ë³„ ì›”ë³„/ë…„ë„ë³„ ê±°ë˜ëŸ‰ ì¡°íšŒ
    """
    # ìœ íš¨ì„± ê²€ì¦
    valid_region_types = ["ì „êµ­", "ìˆ˜ë„ê¶Œ", "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"]
    if region_type not in valid_region_types:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ region_typeì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê°’: {', '.join(valid_region_types)}"
        )
    
    valid_period_types = ["monthly", "yearly"]
    if period_type not in valid_period_types:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ period_typeì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê°’: {', '.join(valid_period_types)}"
        )
    
    if period_type == "monthly" and year_range is None:
        year_range = 3  # ê¸°ë³¸ê°’
    if period_type == "yearly" and (start_year is None or end_year is None):
        # ê¸°ë³¸ê°’: ìµœê·¼ 5ë…„
        current_year = date.today().year
        end_year = current_year
        start_year = current_year - 5
    
    cache_key = build_cache_key(
        "statistics", "transaction-volume", region_type, period_type,
        str(year_range) if year_range else "",
        str(start_year) if start_year else "",
        str(end_year) if end_year else "",
        transaction_type
    )
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        logger.info(f"âœ… [Statistics Transaction Volume] ìºì‹œì—ì„œ ë°˜í™˜")
        return cached_data
    
    try:
        logger.info(
            f"ğŸ” [Statistics Transaction Volume] ê±°ë˜ëŸ‰ ë°ì´í„° ì¡°íšŒ ì‹œì‘ - "
            f"region_type: {region_type}, period_type: {period_type}, "
            f"year_range: {year_range}, transaction_type: {transaction_type}"
        )
        
        # ê±°ë˜ ìœ í˜•ì— ë”°ë¥¸ í…Œì´ë¸” ë° í•„ë“œ ì„ íƒ
        if transaction_type == "sale":
            trans_table = Sale
            date_field = Sale.contract_date
            base_filter = and_(
                Sale.is_canceled == False,
                (Sale.is_deleted == False) | (Sale.is_deleted.is_(None)),
                Sale.contract_date.isnot(None),
                or_(Sale.remarks != "ë”ë¯¸", Sale.remarks.is_(None))
            )
        else:  # rent
            trans_table = Rent
            date_field = Rent.deal_date
            base_filter = and_(
                (Rent.is_deleted == False) | (Rent.is_deleted.is_(None)),
                Rent.deal_date.isnot(None),
                or_(Rent.remarks != "ë”ë¯¸", Rent.remarks.is_(None))
            )
        
        # ì§€ì—­ í•„í„° ì¡°ê±´
        region_filter = get_region_type_filter(region_type)
        
        # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
        today = date.today()
        current_year = today.year
        current_month = today.month
        
        if period_type == "monthly":
            # ì›”ë³„: ìµœê·¼ year_rangeë…„ ë°ì´í„°
            start_date = date(current_year - year_range, 1, 1)
            end_date = date(current_year, current_month, 1)  # í˜„ì¬ ë‹¬ ì œì™¸
        else:  # yearly
            # ë…„ë„ë³„: start_year ~ end_year
            start_date = date(start_year, 1, 1)
            end_date = date(end_year, 12, 31)
        
        # ì¿¼ë¦¬ êµ¬ì„±
        query = (
            select(
                extract('year', date_field).label('year'),
                extract('month', date_field).label('month'),
                func.count(trans_table.trans_id).label('count')
            )
            .join(Apartment, trans_table.apt_id == Apartment.apt_id)
            .join(State, Apartment.region_id == State.region_id)
            .where(
                and_(
                    base_filter,
                    date_field >= start_date,
                    date_field <= end_date,
                    (Apartment.is_deleted == False) | (Apartment.is_deleted.is_(None)),
                    (State.is_deleted == False) | (State.is_deleted.is_(None))
                )
            )
        )
        
        # ì§€ì—­ í•„í„° ì ìš©
        if region_filter is not None:
            query = query.where(region_filter)
        
        # ê·¸ë£¹í™”
        if period_type == "monthly":
            query = query.group_by(
                extract('year', date_field),
                extract('month', date_field)
            ).order_by(
                extract('year', date_field),
                extract('month', date_field)
            )
        else:  # yearly
            query = query.group_by(
                extract('year', date_field)
            ).order_by(
                extract('year', date_field)
            )
        
        result = await db.execute(query)
        rows = result.fetchall()
        
        # ì‘ë‹µ ë°ì´í„° ìƒì„±
        if period_type == "monthly":
            # ì›”ë³„: ë™ì  í‚¤ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            monthly_data_map: Dict[str, Dict[str, Any]] = {}
            years_set = set()
            
            for row in rows:
                year = int(row.year)
                month = int(row.month)
                count = row.count or 0
                
                years_set.add(year)
                month_key = f"{month}ì›”"
                
                if month_key not in monthly_data_map:
                    monthly_data_map[month_key] = {"period": month_key}
                
                monthly_data_map[month_key][year] = count
            
            data = list(monthly_data_map.values())
            years = sorted(years_set)
            
            response_data = TransactionVolumeResponse(
                success=True,
                data=data,
                years=years,
                region_type=region_type,
                period_type=period_type,
                year_range=year_range,
                start_year=None,
                end_year=None
            )
        else:  # yearly
            # ë…„ë„ë³„: {period, value} í˜•ì‹
            data = []
            for row in rows:
                year = int(row.year)
                count = row.count or 0
                data.append({
                    "period": str(year),
                    "value": count
                })
            
            response_data = TransactionVolumeResponse(
                success=True,
                data=data,
                years=None,
                region_type=region_type,
                period_type=period_type,
                year_range=None,
                start_year=start_year,
                end_year=end_year
            )
        
        # ìºì‹œì— ì €ì¥
        if len(data) > 0:
            await set_to_cache(cache_key, response_data.dict(), ttl=STATISTICS_CACHE_TTL)
        
        logger.info(f"âœ… [Statistics Transaction Volume] ê±°ë˜ëŸ‰ ë°ì´í„° ìƒì„± ì™„ë£Œ - ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(data)}")
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [Statistics Transaction Volume] ê±°ë˜ëŸ‰ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ê±°ë˜ëŸ‰ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/market-phase",
    response_model=MarketPhaseResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="ì§€ì—­ë³„ ì‹œì¥ êµ­ë©´ ë¶„ì„",
    description="""
    ê°€ê²© ë³€í™”ìœ¨ê³¼ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨ì„ ê¸°ë°˜ìœ¼ë¡œ ì§€ì—­ë³„ ì‹œì¥ êµ­ë©´ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    
    ### ì‹œì¥ êµ­ë©´ ë¶„ë¥˜
    - **ìƒìŠ¹ê¸°**: ê°€ê²©â†‘ + ê±°ë˜ëŸ‰â†‘
    - **íšŒë³µê¸°**: ê°€ê²©â†‘ + ê±°ë˜ëŸ‰â†“
    - **ì¹¨ì²´ê¸°**: ê°€ê²©â†“ + ê±°ë˜ëŸ‰â†“
    - **í›„í‡´ê¸°**: ê°€ê²©â†“ + ê±°ë˜ëŸ‰â†‘
    
    ### Query Parameters
    - `region_type`: ì§€ì—­ ìœ í˜• (required)
    - `region_id`: íŠ¹ì • ì§€ì—­ ID (optional)
    - `period_months`: ë¹„êµ ê¸°ê°„ (optional, ê¸°ë³¸ê°’: 2)
    - `transaction_type`: ê±°ë˜ ìœ í˜• (optional, ê¸°ë³¸ê°’: "sale")
    """
)
async def get_market_phase(
    region_type: str = Query(..., description="ì§€ì—­ ìœ í˜•: ì „êµ­, ìˆ˜ë„ê¶Œ, ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"),
    region_id: Optional[int] = Query(None, description="íŠ¹ì • ì§€ì—­ ID (ì§€ì • ì‹œ í•´ë‹¹ ì§€ì—­ë§Œ ì¡°íšŒ)"),
    period_months: int = Query(2, ge=1, le=12, description="ë¹„êµ ê¸°ê°„ (ê°œì›”)"),
    transaction_type: str = Query("sale", description="ê±°ë˜ ìœ í˜•: sale(ë§¤ë§¤), rent(ì „ì›”ì„¸)"),
    db: AsyncSession = Depends(get_db)
):
    """
    ì§€ì—­ë³„ ì‹œì¥ êµ­ë©´ ë¶„ì„
    """
    # ìœ íš¨ì„± ê²€ì¦
    valid_region_types = ["ì „êµ­", "ìˆ˜ë„ê¶Œ", "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"]
    if region_type not in valid_region_types:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ region_typeì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê°’: {', '.join(valid_region_types)}"
        )
    
    cache_key = build_cache_key(
        "statistics", "market-phase", region_type,
        str(region_id) if region_id else "all",
        str(period_months),
        transaction_type
    )
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        logger.info(f"âœ… [Statistics Market Phase] ìºì‹œì—ì„œ ë°˜í™˜")
        return cached_data
    
    try:
        logger.info(
            f"ğŸ” [Statistics Market Phase] ì‹œì¥ êµ­ë©´ ë¶„ì„ ì‹œì‘ - "
            f"region_type: {region_type}, region_id: {region_id}, "
            f"period_months: {period_months}, transaction_type: {transaction_type}"
        )
        
        # ê±°ë˜ ìœ í˜•ì— ë”°ë¥¸ í…Œì´ë¸” ë° í•„ë“œ ì„ íƒ
        if transaction_type == "sale":
            trans_table = Sale
            price_field = Sale.trans_price
            date_field = Sale.contract_date
            area_field = Sale.exclusive_area
            base_filter = and_(
                Sale.is_canceled == False,
                (Sale.is_deleted == False) | (Sale.is_deleted.is_(None)),
                Sale.contract_date.isnot(None),
                Sale.trans_price.isnot(None),
                Sale.exclusive_area.isnot(None),
                Sale.exclusive_area > 0,
                or_(Sale.remarks != "ë”ë¯¸", Sale.remarks.is_(None))
            )
        else:  # rent
            trans_table = Rent
            price_field = Rent.deposit_price
            date_field = Rent.deal_date
            area_field = Rent.exclusive_area
            base_filter = and_(
                or_(Rent.monthly_rent == 0, Rent.monthly_rent.is_(None)),  # ì „ì„¸ë§Œ
                (Rent.is_deleted == False) | (Rent.is_deleted.is_(None)),
                Rent.deal_date.isnot(None),
                Rent.deposit_price.isnot(None),
                Rent.exclusive_area.isnot(None),
                Rent.exclusive_area > 0,
                or_(Rent.remarks != "ë”ë¯¸", Rent.remarks.is_(None))
            )
        
        # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
        today = date.today()
        current_month_start = date(today.year, today.month, 1)
        
        recent_start = current_month_start - timedelta(days=period_months * 30)
        recent_end = current_month_start  # í˜„ì¬ ë‹¬ ì œì™¸
        
        previous_start = recent_start - timedelta(days=period_months * 30)
        previous_end = recent_start
        
        # ì§€ì—­ í•„í„° ì¡°ê±´
        region_filter = get_region_type_filter(region_type)
        
        # ì‹œêµ°êµ¬ ë ˆë²¨ë¡œ ê·¸ë£¹í™” (region_id, region_name ì‚¬ìš©)
        # ìµœê·¼ ê¸°ê°„ í‰ê·  ê°€ê²© ë° ê±°ë˜ëŸ‰
        recent_stmt = (
            select(
                State.region_id,
                State.city_name,
                State.region_name,
                func.avg(price_field / area_field * 3.3).label('avg_price_per_pyeong'),
                func.count(trans_table.trans_id).label('volume')
            )
            .join(Apartment, trans_table.apt_id == Apartment.apt_id)
            .join(State, Apartment.region_id == State.region_id)
            .where(
                and_(
                    base_filter,
                    date_field >= recent_start,
                    date_field < recent_end,
                    (Apartment.is_deleted == False) | (Apartment.is_deleted.is_(None)),
                    (State.is_deleted == False) | (State.is_deleted.is_(None))
                )
            )
            .group_by(State.region_id, State.city_name, State.region_name)
            .having(func.count(trans_table.trans_id) >= 5)  # ìµœì†Œ 5ê±´ ì´ìƒ
        )
        
        # ì´ì „ ê¸°ê°„ í‰ê·  ê°€ê²© ë° ê±°ë˜ëŸ‰
        previous_stmt = (
            select(
                State.region_id,
                State.city_name,
                State.region_name,
                func.avg(price_field / area_field * 3.3).label('avg_price_per_pyeong'),
                func.count(trans_table.trans_id).label('volume')
            )
            .join(Apartment, trans_table.apt_id == Apartment.apt_id)
            .join(State, Apartment.region_id == State.region_id)
            .where(
                and_(
                    base_filter,
                    date_field >= previous_start,
                    date_field < previous_end,
                    (Apartment.is_deleted == False) | (Apartment.is_deleted.is_(None)),
                    (State.is_deleted == False) | (State.is_deleted.is_(None))
                )
            )
            .group_by(State.region_id, State.city_name, State.region_name)
            .having(func.count(trans_table.trans_id) >= 5)  # ìµœì†Œ 5ê±´ ì´ìƒ
        )
        
        # ì§€ì—­ í•„í„° ì ìš©
        if region_filter is not None:
            recent_stmt = recent_stmt.where(region_filter)
            previous_stmt = previous_stmt.where(region_filter)
        
        # íŠ¹ì • ì§€ì—­ í•„í„° ì ìš©
        if region_id is not None:
            recent_stmt = recent_stmt.where(State.region_id == region_id)
            previous_stmt = previous_stmt.where(State.region_id == region_id)
        
        # ë³‘ë ¬ ì‹¤í–‰
        recent_result, previous_result = await asyncio.gather(
            db.execute(recent_stmt),
            db.execute(previous_stmt)
        )
        
        recent_rows = recent_result.fetchall()
        previous_rows = previous_result.fetchall()
        
        # ì´ì „ ê¸°ê°„ ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        previous_data_map: Dict[int, Dict[str, Any]] = {}
        for row in previous_rows:
            previous_data_map[row.region_id] = {
                "avg_price": float(row.avg_price_per_pyeong or 0),
                "volume": row.volume or 0
            }
        
        # ì‹œì¥ êµ­ë©´ ë¶„ì„ ë°ì´í„° ìƒì„±
        market_phases = []
        for row in recent_rows:
            region_id_val = row.region_id
            city_name = row.city_name
            region_name = row.region_name
            
            # ì´ì „ ê¸°ê°„ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
            if region_id_val not in previous_data_map:
                continue
            
            recent_price = float(row.avg_price_per_pyeong or 0)
            recent_volume = row.volume or 0
            previous_price = previous_data_map[region_id_val]["avg_price"]
            previous_volume = previous_data_map[region_id_val]["volume"]
            
            # ë³€í™”ìœ¨ ê³„ì‚°
            if previous_price > 0:
                price_change_rate = ((recent_price - previous_price) / previous_price) * 100
            else:
                price_change_rate = 0.0
            
            if previous_volume > 0:
                volume_change_rate = ((recent_volume - previous_volume) / previous_volume) * 100
            else:
                volume_change_rate = 0.0
            
            # ì‹œì¥ êµ­ë©´ ë¶„ë¥˜
            phase, trend, change_str = calculate_market_phase(price_change_rate, volume_change_rate)
            
            # ì§€ì—­ëª… ìƒì„± (ì‹œë„ + ì‹œêµ°êµ¬)
            full_region_name = f"{city_name} {region_name}" if region_name else city_name
            
            market_phases.append(MarketPhaseDataPoint(
                region_id=region_id_val,
                region_name=full_region_name,
                city_name=city_name,
                phase=phase,
                trend=trend,
                change=change_str,
                price_change_rate=round(price_change_rate, 2),
                volume_change_rate=round(volume_change_rate, 2),
                recent_price=round(recent_price, 1),
                previous_price=round(previous_price, 1),
                recent_volume=recent_volume,
                previous_volume=previous_volume
            ))
        
        # ì •ë ¬ (ì‹œë„ â†’ ì‹œêµ°êµ¬)
        market_phases.sort(key=lambda x: (x.city_name, x.region_name))
        
        response_data = MarketPhaseResponse(
            success=True,
            data=market_phases,
            region_type=region_type,
            period_months=period_months
        )
        
        # ìºì‹œì— ì €ì¥
        if len(market_phases) > 0:
            await set_to_cache(cache_key, response_data.dict(), ttl=STATISTICS_CACHE_TTL)
        
        logger.info(f"âœ… [Statistics Market Phase] ì‹œì¥ êµ­ë©´ ë¶„ì„ ì™„ë£Œ - ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(market_phases)}")
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [Statistics Market Phase] ì‹œì¥ êµ­ë©´ ë¶„ì„ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì‹œì¥ êµ­ë©´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/hpi/by-region-type",
    response_model=HPIRegionTypeResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="ì§€ì—­ ìœ í˜•ë³„ ì£¼íƒ ê°€ê²© ì§€ìˆ˜ ì¡°íšŒ",
    description="""
    ì§€ì—­ ìœ í˜•ë³„ë¡œ ì£¼íƒ ê°€ê²© ì§€ìˆ˜ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ### Query Parameters
    - `region_type`: ì§€ì—­ ìœ í˜• (required)
    - `index_type`: ì§€ìˆ˜ ìœ í˜• (optional, ê¸°ë³¸ê°’: "APT")
    - `base_ym`: ê¸°ì¤€ ë…„ì›” (optional, ê¸°ë³¸ê°’: ìµœì‹ )
    """
)
async def get_hpi_by_region_type(
    region_type: str = Query(..., description="ì§€ì—­ ìœ í˜•: ì „êµ­, ìˆ˜ë„ê¶Œ, ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"),
    index_type: str = Query("APT", description="ì§€ìˆ˜ ìœ í˜•: APT(ì•„íŒŒíŠ¸), HOUSE(ë‹¨ë…ì£¼íƒ), ALL(ì „ì²´)"),
    base_ym: Optional[str] = Query(None, description="ê¸°ì¤€ ë…„ì›” (YYYYMM, ê¸°ë³¸ê°’: ìµœì‹ )"),
    db: AsyncSession = Depends(get_db)
):
    """
    ì§€ì—­ ìœ í˜•ë³„ ì£¼íƒ ê°€ê²© ì§€ìˆ˜ ì¡°íšŒ
    """
    # ìœ íš¨ì„± ê²€ì¦
    valid_region_types = ["ì „êµ­", "ìˆ˜ë„ê¶Œ", "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"]
    if region_type not in valid_region_types:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ region_typeì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê°’: {', '.join(valid_region_types)}"
        )
    
    valid_index_types = ["APT", "HOUSE", "ALL"]
    if index_type not in valid_index_types:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ index_typeì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê°’: {', '.join(valid_index_types)}"
        )
    
    cache_key = build_cache_key(
        "statistics", "hpi-by-region-type", region_type, index_type,
        base_ym if base_ym else "latest"
    )
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        logger.info(f"âœ… [Statistics HPI Region Type] ìºì‹œì—ì„œ ë°˜í™˜")
        return cached_data
    
    try:
        logger.info(
            f"ğŸ” [Statistics HPI Region Type] HPI ë°ì´í„° ì¡°íšŒ ì‹œì‘ - "
            f"region_type: {region_type}, index_type: {index_type}, base_ym: {base_ym}"
        )
        
        # base_ymì´ ì—†ìœ¼ë©´ ìµœì‹  ë°ì´í„° ì°¾ê¸°
        if not base_ym:
            today = date.today()
            current_year = today.year
            current_month = today.month
            
            # ìµœì‹  base_ym ì°¾ê¸° (ìµœëŒ€ 12ê°œì›” ì „ê¹Œì§€)
            found_base_ym = None
            for i in range(12):
                check_year = current_year
                check_month = current_month - i
                if check_month <= 0:
                    check_year -= 1
                    check_month += 12
                check_base_ym = f"{check_year:04d}{check_month:02d}"
                
                # í•´ë‹¹ base_ymì— ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
                check_query = (
                    select(func.count(HouseScore.index_id))
                    .where(
                        and_(
                            HouseScore.is_deleted == False,
                            HouseScore.index_type == index_type,
                            HouseScore.base_ym == check_base_ym
                        )
                    )
                )
                check_result = await db.execute(check_query)
                count = check_result.scalar() or 0
                
                if count > 0:
                    found_base_ym = check_base_ym
                    break
            
            if not found_base_ym:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail="HPI ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            
            base_ym = found_base_ym
        
        # ì§€ì—­ í•„í„° ì¡°ê±´
        region_filter = get_region_type_filter(region_type)
        
        # ìˆ˜ë„ê¶Œì˜ ê²½ìš° ì‹œ/êµ° ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”, ê·¸ ì™¸ëŠ” ì‹œë„ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”
        if region_type == "ìˆ˜ë„ê¶Œ":
            # ìˆ˜ë„ê¶Œ: ì‹œ/êµ° ë‹¨ìœ„ë¡œ ê·¸ë£¹í™” (ì„œìš¸íŠ¹ë³„ì‹œëŠ” "ì„œìš¸", ì¸ì²œê´‘ì—­ì‹œëŠ” "ì¸ì²œ", ê²½ê¸°ë„ëŠ” ì‹œ/êµ°ëª…)
            query = (
                select(
                    State.city_name,
                    State.region_name,
                    func.avg(HouseScore.index_value).label('index_value'),
                    func.avg(HouseScore.index_change_rate).label('index_change_rate'),
                    func.count(HouseScore.index_id).label('region_count')
                )
                .join(State, HouseScore.region_id == State.region_id)
                .where(
                    and_(
                        HouseScore.is_deleted == False,
                        State.is_deleted == False,
                        HouseScore.index_type == index_type,
                        HouseScore.base_ym == base_ym
                    )
                )
                .group_by(State.city_name, State.region_name)
            )
            
            # ì§€ì—­ í•„í„° ì ìš©
            if region_filter is not None:
                query = query.where(region_filter)
            
            query = query.order_by(State.city_name, State.region_name)
            
            result = await db.execute(query)
            rows = result.fetchall()
            
            # ì‘ë‹µ ë°ì´í„° ìƒì„±: ì‹œ/êµ° ë‹¨ìœ„
            hpi_data = []
            for row in rows:
                city_name = row.city_name
                region_name = row.region_name
                
                # ì„œìš¸íŠ¹ë³„ì‹œì™€ ì¸ì²œê´‘ì—­ì‹œëŠ” ì‹œë„ëª…ë§Œ ì‚¬ìš©, ê²½ê¸°ë„ëŠ” ì‹œ/êµ°ëª… ì‚¬ìš©
                if city_name == "ì„œìš¸íŠ¹ë³„ì‹œ":
                    display_name = "ì„œìš¸"
                elif city_name == "ì¸ì²œê´‘ì—­ì‹œ":
                    display_name = "ì¸ì²œ"
                else:
                    # ê²½ê¸°ë„: ì‹œ/êµ°ëª…ì—ì„œ "ì‹œ", "êµ°", "êµ¬" ì œê±°
                    display_name = region_name.replace("ì‹œ", "").replace("êµ°", "").replace("êµ¬", "") if region_name else city_name
                
                index_value = float(row.index_value or 0)
                index_change_rate = float(row.index_change_rate) if row.index_change_rate is not None else None
                
                hpi_data.append(HPIRegionTypeDataPoint(
                    id=None,
                    name=display_name,
                    value=round(index_value, 2),
                    index_change_rate=round(index_change_rate, 2) if index_change_rate is not None else None
                ))
        else:
            # ì „êµ­, ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ: ì‹œë„ ë ˆë²¨ë¡œ ê·¸ë£¹í™”
            query = (
                select(
                    State.city_name,
                    func.avg(HouseScore.index_value).label('index_value'),
                    func.avg(HouseScore.index_change_rate).label('index_change_rate'),
                    func.count(HouseScore.index_id).label('region_count')
                )
                .join(State, HouseScore.region_id == State.region_id)
                .where(
                    and_(
                        HouseScore.is_deleted == False,
                        State.is_deleted == False,
                        HouseScore.index_type == index_type,
                        HouseScore.base_ym == base_ym
                    )
                )
                .group_by(State.city_name)
            )
            
            # ì§€ì—­ í•„í„° ì ìš©
            if region_filter is not None:
                query = query.where(region_filter)
            
            query = query.order_by(State.city_name)
            
            result = await db.execute(query)
            rows = result.fetchall()
            
            # ì‘ë‹µ ë°ì´í„° ìƒì„±: ì‹œë„ ë‹¨ìœ„
            hpi_data = []
            for row in rows:
                city_name = row.city_name
                normalized_name = normalize_city_name(city_name)
                index_value = float(row.index_value or 0)
                index_change_rate = float(row.index_change_rate) if row.index_change_rate is not None else None
                
                hpi_data.append(HPIRegionTypeDataPoint(
                    id=None,
                    name=normalized_name,
                    value=round(index_value, 2),
                    index_change_rate=round(index_change_rate, 2) if index_change_rate is not None else None
                ))
        
        response_data = HPIRegionTypeResponse(
            success=True,
            data=hpi_data,
            region_type=region_type,
            index_type=index_type,
            base_ym=base_ym
        )
        
        # ìºì‹œì— ì €ì¥
        if len(hpi_data) > 0:
            await set_to_cache(cache_key, response_data.dict(), ttl=STATISTICS_CACHE_TTL)
        
        logger.info(f"âœ… [Statistics HPI Region Type] HPI ë°ì´í„° ìƒì„± ì™„ë£Œ - ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(hpi_data)}")
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [Statistics HPI Region Type] HPI ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"HPI ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/population-movements/by-region-type",
    response_model=PopulationMovementRegionTypeResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="ì§€ì—­ ìœ í˜•ë³„ ì¸êµ¬ ìˆœì´ë™ ì¡°íšŒ",
    description="""
    ì§€ì—­ ìœ í˜•ë³„ë¡œ ì¸êµ¬ ìˆœì´ë™ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ### Query Parameters
    - `region_type`: ì§€ì—­ ìœ í˜• (required)
    - `start_ym`: ì‹œì‘ ë…„ì›” (optional, ê¸°ë³¸ê°’: ìµœê·¼ 3ê°œì›” ì „)
    - `end_ym`: ì¢…ë£Œ ë…„ì›” (optional, ê¸°ë³¸ê°’: ìµœì‹ )
    - `aggregate`: ì§‘ê³„ ë°©ì‹ (optional, "sum" | "avg", ê¸°ë³¸ê°’: "sum")
    """
)
async def get_population_movements_by_region_type(
    region_type: str = Query(..., description="ì§€ì—­ ìœ í˜•: ì „êµ­, ìˆ˜ë„ê¶Œ, ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"),
    start_ym: Optional[str] = Query(None, description="ì‹œì‘ ë…„ì›” (YYYYMM)"),
    end_ym: Optional[str] = Query(None, description="ì¢…ë£Œ ë…„ì›” (YYYYMM)"),
    aggregate: str = Query("sum", description="ì§‘ê³„ ë°©ì‹: sum(í•©ê³„), avg(í‰ê· )"),
    db: AsyncSession = Depends(get_db)
):
    """
    ì§€ì—­ ìœ í˜•ë³„ ì¸êµ¬ ìˆœì´ë™ ì¡°íšŒ
    """
    # ìœ íš¨ì„± ê²€ì¦
    valid_region_types = ["ì „êµ­", "ìˆ˜ë„ê¶Œ", "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"]
    if region_type not in valid_region_types:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ region_typeì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê°’: {', '.join(valid_region_types)}"
        )
    
    valid_aggregates = ["sum", "avg"]
    if aggregate not in valid_aggregates:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ aggregateì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê°’: {', '.join(valid_aggregates)}"
        )
    
    # ê¸°ë³¸ ê¸°ê°„ ì„¤ì •
    if not end_ym:
        end_date = datetime.now()
        end_ym = end_date.strftime("%Y%m")
    
    if not start_ym:
        # ìµœê·¼ 3ê°œì›” ì „
        start_date = datetime.now() - timedelta(days=90)
        start_ym = start_date.strftime("%Y%m")
    
    # ê¸°ê°„ ê°œì›” ìˆ˜ ê³„ì‚°
    start_year = int(start_ym[:4])
    start_month = int(start_ym[4:])
    end_year = int(end_ym[:4])
    end_month = int(end_ym[4:])
    period_months = (end_year - start_year) * 12 + (end_month - start_month) + 1
    
    cache_key = build_cache_key(
        "statistics", "population-movements-by-region-type", region_type,
        start_ym, end_ym, aggregate
    )
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        logger.info(f"âœ… [Statistics Population Movement Region Type] ìºì‹œì—ì„œ ë°˜í™˜")
        return cached_data
    
    try:
        logger.info(
            f"ğŸ” [Statistics Population Movement Region Type] ì¸êµ¬ ìˆœì´ë™ ë°ì´í„° ì¡°íšŒ ì‹œì‘ - "
            f"region_type: {region_type}, start_ym: {start_ym}, end_ym: {end_ym}, aggregate: {aggregate}"
        )
        
        # ì§€ì—­ í•„í„° ì¡°ê±´
        region_filter = get_region_type_filter(region_type)
        
        # ì§‘ê³„ í•¨ìˆ˜ ì„ íƒ
        if aggregate == "sum":
            net_migration_func = func.sum(PopulationMovement.net_migration)
            in_migration_func = func.sum(PopulationMovement.in_migration)
            out_migration_func = func.sum(PopulationMovement.out_migration)
        else:  # avg
            net_migration_func = func.avg(PopulationMovement.net_migration)
            in_migration_func = func.avg(PopulationMovement.in_migration)
            out_migration_func = func.avg(PopulationMovement.out_migration)
        
        # ì¿¼ë¦¬ êµ¬ì„±: ì‹œë„ ë ˆë²¨ë¡œ ê·¸ë£¹í™”
        query = (
            select(
                State.city_name,
                net_migration_func.label('net_migration'),
                in_migration_func.label('in_migration'),
                out_migration_func.label('out_migration')
            )
            .join(State, PopulationMovement.region_id == State.region_id)
            .where(
                and_(
                    PopulationMovement.base_ym >= start_ym,
                    PopulationMovement.base_ym <= end_ym,
                    PopulationMovement.is_deleted == False,
                    State.is_deleted == False
                )
            )
            .group_by(State.city_name)
        )
        
        # ì§€ì—­ í•„í„° ì ìš©
        if region_filter is not None:
            query = query.where(region_filter)
        
        query = query.order_by(desc(net_migration_func))  # ìˆœì´ë™ í° ìˆœì„œëŒ€ë¡œ
        
        result = await db.execute(query)
        rows = result.fetchall()
        
        # ì‘ë‹µ ë°ì´í„° ìƒì„±
        migration_data = []
        for row in rows:
            city_name = row.city_name
            normalized_name = normalize_city_name(city_name)
            net_migration = int(row.net_migration or 0) if aggregate == "sum" else float(row.net_migration or 0)
            in_migration = int(row.in_migration or 0) if aggregate == "sum" else float(row.in_migration or 0)
            out_migration = int(row.out_migration or 0) if aggregate == "sum" else float(row.out_migration or 0)
            
            # ì •ìˆ˜ë¡œ ë³€í™˜ (í‰ê· ì¸ ê²½ìš° ë°˜ì˜¬ë¦¼)
            if aggregate == "avg":
                net_migration = int(round(net_migration))
                in_migration = int(round(in_migration))
                out_migration = int(round(out_migration))
            
            label = "ìˆœìœ ì…" if net_migration > 0 else "ìˆœìœ ì¶œ"
            
            migration_data.append(PopulationMovementRegionTypeDataPoint(
                name=normalized_name,
                value=net_migration,
                label=label,
                in_migration=in_migration,
                out_migration=out_migration,
                net_migration=net_migration
            ))
        
        response_data = PopulationMovementRegionTypeResponse(
            success=True,
            data=migration_data,
            region_type=region_type,
            start_ym=start_ym,
            end_ym=end_ym,
            period_months=period_months
        )
        
        # ìºì‹œì— ì €ì¥
        if len(migration_data) > 0:
            await set_to_cache(cache_key, response_data.dict(), ttl=STATISTICS_CACHE_TTL)
        
        logger.info(f"âœ… [Statistics Population Movement Region Type] ì¸êµ¬ ìˆœì´ë™ ë°ì´í„° ìƒì„± ì™„ë£Œ - ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(migration_data)}")
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [Statistics Population Movement Region Type] ì¸êµ¬ ìˆœì´ë™ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì¸êµ¬ ìˆœì´ë™ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )
