"""
í†µê³„ ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸

ë‹´ë‹¹ ê¸°ëŠ¥:
- RVOL(ìƒëŒ€ ê±°ë˜ëŸ‰) ê³„ì‚° ë° ì¡°íšŒ
- 4ë¶„ë©´ ë¶„ë¥˜ (ë§¤ë§¤/ì „ì›”ì„¸ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨ ê¸°ë°˜)

ì„±ëŠ¥ ìµœì í™”:
- ê¸°ê°„ ì œí•œ: ìµœëŒ€ 2~3ê°œì›”
- ì›”ë³„ ì§‘ê³„ë¡œ ê°„ì†Œí™”
- ê¸´ ìºì‹œ TTL (6ì‹œê°„)
"""
import logging
import sys
import asyncio
from collections import defaultdict
from datetime import date, datetime, timedelta
from typing import Optional, List, Dict, Any, Union
from fastapi import APIRouter, Depends, HTTPException, Query, status
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func, and_, or_, case, desc, text, extract
from sqlalchemy.orm import selectinload

from app.api.v1.deps import get_db
from app.models.sale import Sale
from app.models.rent import Rent
from app.models.apartment import Apartment
from app.models.state import State
from app.models.house_score import HouseScore
from app.models.population_movement import PopulationMovement
from app.schemas.statistics import (
    RVOLResponse,
    RVOLDataPoint,
    QuadrantResponse,
    QuadrantDataPoint,
    StatisticsSummaryResponse,
    HPIResponse,
    HPIDataPoint,
    HPIHeatmapResponse,
    HPIHeatmapDataPoint,
    PopulationMovementResponse,
    PopulationMovementDataPoint,
    PopulationMovementSankeyResponse,
    PopulationMovementSankeyDataPoint,
    CorrelationAnalysisResponse,
    HPIRegionTypeResponse,
    HPIRegionTypeDataPoint,
    TransactionVolumeResponse,
    TransactionVolumeDataPoint,
    MarketPhaseResponse,
    MarketPhaseListResponse,
    MarketPhaseDataPoint,
    MarketPhaseCalculationMethod,
    MarketPhaseThresholds
)
from app.utils.cache import get_from_cache, set_to_cache, build_cache_key, delete_cache_pattern

# ë¡œê±° ì„¤ì • (Docker ë¡œê·¸ì— ì¶œë ¥ë˜ë„ë¡)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
if not logger.handlers:
    handler = logging.StreamHandler(sys.stdout)
    handler.setLevel(logging.INFO)
    formatter = logging.Formatter(
        '%(asctime)s | %(levelname)s | %(name)s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.propagate = True  # ë£¨íŠ¸ ë¡œê±°ë¡œë„ ì „íŒŒ

router = APIRouter()

# ìºì‹œ TTL: 6ì‹œê°„ (í†µê³„ ë°ì´í„°ëŠ” ìì£¼ ë³€í•˜ì§€ ì•ŠìŒ)
STATISTICS_CACHE_TTL = 21600


# ============================================================
# í—¬í¼ í•¨ìˆ˜
# ============================================================

def normalize_city_name(city_name: str) -> str:
    """
    ì‹œë„ëª…ì„ í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”
    
    Args:
        city_name: ì‹œë„ëª… (ì˜ˆ: "ì„œìš¸íŠ¹ë³„ì‹œ", "ë¶€ì‚°ê´‘ì—­ì‹œ")
    
    Returns:
        ì •ê·œí™”ëœ ì§€ì—­ëª… (ì˜ˆ: "ì„œìš¸", "ë¶€ì‚°")
    """
    mapping = {
        "ì„œìš¸íŠ¹ë³„ì‹œ": "ì„œìš¸",
        "ë¶€ì‚°ê´‘ì—­ì‹œ": "ë¶€ì‚°",
        "ëŒ€êµ¬ê´‘ì—­ì‹œ": "ëŒ€êµ¬",
        "ì¸ì²œê´‘ì—­ì‹œ": "ì¸ì²œ",
        "ê´‘ì£¼ê´‘ì—­ì‹œ": "ê´‘ì£¼",
        "ëŒ€ì „ê´‘ì—­ì‹œ": "ëŒ€ì „",
        "ìš¸ì‚°ê´‘ì—­ì‹œ": "ìš¸ì‚°",
        "ê²½ê¸°ë„": "ê²½ê¸°",
    }
    return mapping.get(city_name, city_name)


def normalize_metropolitan_region_name(city_name: str, region_name: str) -> str:
    """
    ìˆ˜ë„ê¶Œì˜ êµ¬ ë‹¨ìœ„ ì§€ì—­ëª…ì„ ì‹œ/êµ° ë‹¨ìœ„ë¡œ ì •ê·œí™”
    
    Args:
        city_name: ì‹œë„ëª… (ì˜ˆ: "ì„œìš¸íŠ¹ë³„ì‹œ", "ê²½ê¸°ë„", "ì¸ì²œê´‘ì—­ì‹œ")
        region_name: ì‹œêµ°êµ¬ëª… (ì˜ˆ: "ê°•ë‚¨êµ¬", "ìˆ˜ì›ì‹œ", "ê¶Œì„ êµ¬")
    
    Returns:
        ì •ê·œí™”ëœ ì‹œ/êµ°ëª… (ì˜ˆ: "ì„œìš¸", "ìˆ˜ì›", "ì¸ì²œ")
    """
    if not region_name:
        if city_name == "ì„œìš¸íŠ¹ë³„ì‹œ":
            return "ì„œìš¸"
        elif city_name == "ì¸ì²œê´‘ì—­ì‹œ":
            return "ì¸ì²œ"
        else:
            return city_name
    
    # êµ¬ ë‹¨ìœ„ë¥¼ ì‹œ/êµ° ë‹¨ìœ„ë¡œ ë§¤í•‘
    gu_to_city_map = {
        # ìˆ˜ì›ì‹œ êµ¬ë“¤
        "ê¶Œì„ êµ¬": "ìˆ˜ì›",
        "ì˜í†µêµ¬": "ìˆ˜ì›",
        "ì¥ì•ˆêµ¬": "ìˆ˜ì›",
        "íŒ”ë‹¬êµ¬": "ìˆ˜ì›",
        "ê¶Œì„ ": "ìˆ˜ì›",
        "ì˜í†µ": "ìˆ˜ì›",
        "ì¥ì•ˆ": "ìˆ˜ì›",
        "íŒ”ë‹¬": "ìˆ˜ì›",
        # ìš©ì¸ì‹œ êµ¬ë“¤ (ìˆ˜ì§€ëŠ” ìš©ì¸, ê¸°í¥ì€ ì‹œí¥ìœ¼ë¡œ ë§¤í•‘)
        "ìˆ˜ì§€êµ¬": "ìš©ì¸",
        "ì²˜ì¸êµ¬": "ìš©ì¸",
        "ìˆ˜ì§€": "ìš©ì¸",
        "ì²˜ì¸": "ìš©ì¸",
        # ê¸°í¥ â†’ ì‹œí¥ìœ¼ë¡œ ë§¤í•‘
        "ê¸°í¥êµ¬": "ì‹œí¥",
        "ê¸°í¥": "ì‹œí¥",
        # ì•ˆì‚°ì‹œ êµ¬ë“¤
        "ë‹¨ì›êµ¬": "ì•ˆì‚°",
        "ìƒë¡êµ¬": "ì•ˆì‚°",
        "ë‹¨ì›": "ì•ˆì‚°",
        "ìƒë¡": "ì•ˆì‚°",
        # ê³ ì–‘ì‹œ êµ¬ë“¤
        "ë•ì–‘êµ¬": "ê³ ì–‘",
        "ì¼ì‚°ë™êµ¬": "ê³ ì–‘",
        "ì¼ì‚°ì„œêµ¬": "ê³ ì–‘",
        "ë•ì–‘": "ê³ ì–‘",
        "ì¼ì‚°ë™": "ê³ ì–‘",
        "ì¼ì‚°ì„œ": "ê³ ì–‘",
        # ì•ˆì–‘ì‹œ êµ¬ë“¤
        "ë™ì•ˆêµ¬": "ì•ˆì–‘",
        "ë§Œì•ˆêµ¬": "ì•ˆì–‘",
        "ë™ì•ˆ": "ì•ˆì–‘",
        "ë§Œì•ˆ": "ì•ˆì–‘",
        # ì„±ë‚¨ì‹œ êµ¬ë“¤
        "ë¶„ë‹¹êµ¬": "ì„±ë‚¨",
        "ìˆ˜ì •êµ¬": "ì„±ë‚¨",
        "ì¤‘ì›êµ¬": "ì„±ë‚¨",
        "ë¶„ë‹¹": "ì„±ë‚¨",
        "ìˆ˜ì •": "ì„±ë‚¨",
        "ì¤‘ì›": "ì„±ë‚¨",
        # ë¶€ì²œì‹œ êµ¬ë“¤
        "ì†Œì‚¬êµ¬": "ë¶€ì²œ",
        "ì˜¤ì •êµ¬": "ë¶€ì²œ",
        "ì›ë¯¸êµ¬": "ë¶€ì²œ",
        "ì†Œì‚¬": "ë¶€ì²œ",
        "ì˜¤ì •": "ë¶€ì²œ",
        "ì›ë¯¸": "ë¶€ì²œ",
        # ë¶ˆì™„ì „í•œ ì´ë¦„ ë§¤í•‘
        "ë¦¬": "êµ¬ë¦¬",
        "í¬": "êµ°í¬",
    }
    
    # ì›ë³¸ region_nameì—ì„œ ì§ì ‘ ë§¤í•‘ í™•ì¸ (ë¶ˆì™„ì „í•œ ì´ë¦„ ì²˜ë¦¬)
    if region_name in gu_to_city_map:
        return gu_to_city_map[region_name]
    
    # "ë¶€ì²œ ì†Œì‚¬", "ë¶€ì²œ ì˜¤ì •", "ë¶€ì²œ ì›ë¯¸" ê°™ì€ í˜•ì‹ ì²˜ë¦¬
    if "ë¶€ì²œ" in region_name:
        # "ë¶€ì²œì‹œ ì†Œì‚¬êµ¬" ë˜ëŠ” "ë¶€ì²œ ì†Œì‚¬" í˜•ì‹ ì²˜ë¦¬
        parts = region_name.replace("ì‹œ", "").replace("êµ¬", "").split()
        if len(parts) > 1:
            gu_name = parts[1].strip()
            if gu_name in gu_to_city_map:
                return gu_to_city_map[gu_name]
        return "ë¶€ì²œ"
    
    # êµ¬ ë‹¨ìœ„ ë§¤í•‘ í™•ì¸
    normalized_region = region_name.replace("ì‹œ", "").replace("êµ°", "").replace("êµ¬", "").strip()
    
    # ì •ê·œí™”ëœ ì´ë¦„ìœ¼ë¡œ ë§¤í•‘ í™•ì¸
    if normalized_region in gu_to_city_map:
        return gu_to_city_map[normalized_region]
    
    # ì„œìš¸íŠ¹ë³„ì‹œì™€ ì¸ì²œê´‘ì—­ì‹œëŠ” ì‹œë„ëª…ë§Œ ì‚¬ìš©
    if city_name == "ì„œìš¸íŠ¹ë³„ì‹œ":
        return "ì„œìš¸"
    elif city_name == "ì¸ì²œê´‘ì—­ì‹œ":
        return "ì¸ì²œ"
    
    # ê²½ê¸°ë„: ì‹œ/êµ°ëª…ì—ì„œ "ì‹œ", "êµ°", "êµ¬" ì œê±°
    # ì´ë¯¸ ì •ê·œí™”ëœ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if normalized_region:
        return normalized_region
    
    return city_name


def normalize_metropolitan_region_name_without_fallback(city_name: str, region_name: str) -> str:
    """
    ìˆ˜ë„ê¶Œì˜ êµ¬ ë‹¨ìœ„ ì§€ì—­ëª…ì„ ì‹œ/êµ° ë‹¨ìœ„ë¡œ ì •ê·œí™” (ì˜ˆì™¸ì²˜ë¦¬ ì œì™¸ ë²„ì „)
    "ë¦¬", "í¬", "ê¸°í¥"ì€ ë§¤í•‘í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ë°˜í™˜
    
    Args:
        city_name: ì‹œë„ëª… (ì˜ˆ: "ì„œìš¸íŠ¹ë³„ì‹œ", "ê²½ê¸°ë„", "ì¸ì²œê´‘ì—­ì‹œ")
        region_name: ì‹œêµ°êµ¬ëª… (ì˜ˆ: "ê°•ë‚¨êµ¬", "ìˆ˜ì›ì‹œ", "ê¶Œì„ êµ¬")
    
    Returns:
        ì •ê·œí™”ëœ ì‹œ/êµ°ëª… (ì˜ˆ: "ì„œìš¸", "ìˆ˜ì›", "ì¸ì²œ") ë˜ëŠ” ì›ë³¸ ("ë¦¬", "í¬", "ê¸°í¥")
    """
    if not region_name:
        if city_name == "ì„œìš¸íŠ¹ë³„ì‹œ":
            return "ì„œìš¸"
        elif city_name == "ì¸ì²œê´‘ì—­ì‹œ":
            return "ì¸ì²œ"
        else:
            return city_name
    
    # êµ¬ ë‹¨ìœ„ë¥¼ ì‹œ/êµ° ë‹¨ìœ„ë¡œ ë§¤í•‘ (ë¦¬, í¬, ê¸°í¥ ì œì™¸)
    gu_to_city_map = {
        # ìˆ˜ì›ì‹œ êµ¬ë“¤
        "ê¶Œì„ êµ¬": "ìˆ˜ì›",
        "ì˜í†µêµ¬": "ìˆ˜ì›",
        "ì¥ì•ˆêµ¬": "ìˆ˜ì›",
        "íŒ”ë‹¬êµ¬": "ìˆ˜ì›",
        "ê¶Œì„ ": "ìˆ˜ì›",
        "ì˜í†µ": "ìˆ˜ì›",
        "ì¥ì•ˆ": "ìˆ˜ì›",
        "íŒ”ë‹¬": "ìˆ˜ì›",
        # ìš©ì¸ì‹œ êµ¬ë“¤ (ìˆ˜ì§€, ì²˜ì¸ë§Œ ìš©ì¸ìœ¼ë¡œ, ê¸°í¥ì€ ë§¤í•‘í•˜ì§€ ì•ŠìŒ)
        "ìˆ˜ì§€êµ¬": "ìš©ì¸",
        "ì²˜ì¸êµ¬": "ìš©ì¸",
        "ìˆ˜ì§€": "ìš©ì¸",
        "ì²˜ì¸": "ìš©ì¸",
        # ì•ˆì‚°ì‹œ êµ¬ë“¤
        "ë‹¨ì›êµ¬": "ì•ˆì‚°",
        "ìƒë¡êµ¬": "ì•ˆì‚°",
        "ë‹¨ì›": "ì•ˆì‚°",
        "ìƒë¡": "ì•ˆì‚°",
        # ê³ ì–‘ì‹œ êµ¬ë“¤
        "ë•ì–‘êµ¬": "ê³ ì–‘",
        "ì¼ì‚°ë™êµ¬": "ê³ ì–‘",
        "ì¼ì‚°ì„œêµ¬": "ê³ ì–‘",
        "ë•ì–‘": "ê³ ì–‘",
        "ì¼ì‚°ë™": "ê³ ì–‘",
        "ì¼ì‚°ì„œ": "ê³ ì–‘",
        # ì•ˆì–‘ì‹œ êµ¬ë“¤
        "ë™ì•ˆêµ¬": "ì•ˆì–‘",
        "ë§Œì•ˆêµ¬": "ì•ˆì–‘",
        "ë™ì•ˆ": "ì•ˆì–‘",
        "ë§Œì•ˆ": "ì•ˆì–‘",
        # ì„±ë‚¨ì‹œ êµ¬ë“¤
        "ë¶„ë‹¹êµ¬": "ì„±ë‚¨",
        "ìˆ˜ì •êµ¬": "ì„±ë‚¨",
        "ì¤‘ì›êµ¬": "ì„±ë‚¨",
        "ë¶„ë‹¹": "ì„±ë‚¨",
        "ìˆ˜ì •": "ì„±ë‚¨",
        "ì¤‘ì›": "ì„±ë‚¨",
        # ë¶€ì²œì‹œ êµ¬ë“¤
        "ì†Œì‚¬êµ¬": "ë¶€ì²œ",
        "ì˜¤ì •êµ¬": "ë¶€ì²œ",
        "ì›ë¯¸êµ¬": "ë¶€ì²œ",
        "ì†Œì‚¬": "ë¶€ì²œ",
        "ì˜¤ì •": "ë¶€ì²œ",
        "ì›ë¯¸": "ë¶€ì²œ",
    }
    
    # ì›ë³¸ region_name í™•ì¸
    normalized_region = region_name.replace("ì‹œ", "").replace("êµ°", "").replace("êµ¬", "").strip()
    
    # "ë¦¬", "í¬", "ê¸°í¥"ì€ ì˜ˆì™¸ì²˜ë¦¬ìš©ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if normalized_region == "ë¦¬" or normalized_region == "í¬" or normalized_region == "ê¸°í¥":
        return normalized_region
    
    # ì›ë³¸ region_nameì—ì„œ ì§ì ‘ ë§¤í•‘ í™•ì¸
    if region_name in gu_to_city_map:
        return gu_to_city_map[region_name]
    
    # "ë¶€ì²œ ì†Œì‚¬", "ë¶€ì²œ ì˜¤ì •", "ë¶€ì²œ ì›ë¯¸" ê°™ì€ í˜•ì‹ ì²˜ë¦¬
    if "ë¶€ì²œ" in region_name:
        parts = region_name.replace("ì‹œ", "").replace("êµ¬", "").split()
        if len(parts) > 1:
            gu_name = parts[1].strip()
            if gu_name in gu_to_city_map:
                return gu_to_city_map[gu_name]
        return "ë¶€ì²œ"
    
    # ì •ê·œí™”ëœ ì´ë¦„ìœ¼ë¡œ ë§¤í•‘ í™•ì¸
    if normalized_region in gu_to_city_map:
        return gu_to_city_map[normalized_region]
    
    # ì„œìš¸íŠ¹ë³„ì‹œì™€ ì¸ì²œê´‘ì—­ì‹œëŠ” ì‹œë„ëª…ë§Œ ì‚¬ìš©
    if city_name == "ì„œìš¸íŠ¹ë³„ì‹œ":
        return "ì„œìš¸"
    elif city_name == "ì¸ì²œê´‘ì—­ì‹œ":
        return "ì¸ì²œ"
    
    # ê²½ê¸°ë„: ì‹œ/êµ°ëª…ì—ì„œ "ì‹œ", "êµ°", "êµ¬" ì œê±°
    if normalized_region:
        return normalized_region
    
    return city_name


def get_region_type_filter(region_type: str):
    """
    ì§€ì—­ ìœ í˜•ì— ë”°ë¥¸ city_name í•„í„° ì¡°ê±´ ë°˜í™˜
    
    Args:
        region_type: ì§€ì—­ ìœ í˜• ("ì „êµ­", "ìˆ˜ë„ê¶Œ", "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ")
    
    Returns:
        SQLAlchemy í•„í„° ì¡°ê±´ (Noneì´ë©´ í•„í„° ì—†ìŒ)
    """
    if region_type == "ì „êµ­":
        return None
    elif region_type == "ìˆ˜ë„ê¶Œ":
        return State.city_name.in_(['ì„œìš¸íŠ¹ë³„ì‹œ', 'ê²½ê¸°ë„', 'ì¸ì²œê´‘ì—­ì‹œ'])
    elif region_type == "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ":
        return State.city_name.in_(['ë¶€ì‚°ê´‘ì—­ì‹œ', 'ëŒ€êµ¬ê´‘ì—­ì‹œ', 'ê´‘ì£¼ê´‘ì—­ì‹œ', 'ëŒ€ì „ê´‘ì—­ì‹œ', 'ìš¸ì‚°ê´‘ì—­ì‹œ'])
    else:
        raise ValueError(f"ìœ íš¨í•˜ì§€ ì•Šì€ region_type: {region_type}")


def calculate_quadrant(sale_change_rate: float, rent_change_rate: float) -> tuple[int, str]:
    """
    4ë¶„ë©´ ë¶„ë¥˜ ê³„ì‚°
    
    Args:
        sale_change_rate: ë§¤ë§¤ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨ (%)
        rent_change_rate: ì „ì›”ì„¸ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨ (%)
    
    Returns:
        (quadrant_number, quadrant_label) íŠœí”Œ
    """
    if sale_change_rate > 0 and rent_change_rate < 0:
        return (1, "ë§¤ìˆ˜ ì „í™˜")
    elif sale_change_rate < 0 and rent_change_rate > 0:
        return (2, "ì„ëŒ€ ì„ í˜¸/ê´€ë§")
    elif sale_change_rate < 0 and rent_change_rate < 0:
        return (3, "ì‹œì¥ ìœ„ì¶•")
    elif sale_change_rate > 0 and rent_change_rate > 0:
        return (4, "í™œì„±í™”")
    else:
        # ë³€í™”ìœ¨ì´ 0ì¸ ê²½ìš°ëŠ” ì¤‘ë¦½ìœ¼ë¡œ ì²˜ë¦¬
        if sale_change_rate == 0 and rent_change_rate == 0:
            return (0, "ì¤‘ë¦½")
        elif sale_change_rate == 0:
            return (2 if rent_change_rate > 0 else 3, "ì„ëŒ€ ì„ í˜¸/ê´€ë§" if rent_change_rate > 0 else "ì‹œì¥ ìœ„ì¶•")
        else:
            return (1 if sale_change_rate > 0 else 3, "ë§¤ìˆ˜ ì „í™˜" if sale_change_rate > 0 else "ì‹œì¥ ìœ„ì¶•")


# ============================================================
# ì‹œì¥ êµ­ë©´ ì§€í‘œ í—¬í¼ í•¨ìˆ˜
# ============================================================

def get_region_filters(region_type: str, city_name: Optional[str] = None) -> list:
    """
    ì§€ì—­ ìœ í˜•ì— ë”°ë¥¸ í•„í„° ì¡°ê±´ ë°˜í™˜
    
    Args:
        region_type: ì§€ì—­ ìœ í˜• ("ì „êµ­", "ìˆ˜ë„ê¶Œ", "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ")
        city_name: íŠ¹ì • ì‹œë„ëª… (ì§€ë°©5ëŒ€ê´‘ì—­ì‹œì¼ ë•Œ íŠ¹ì • ì§€ì—­ í•„í„°ë§)
    
    Returns:
        SQLAlchemy í•„í„° ì¡°ê±´ ë¦¬ìŠ¤íŠ¸
    """
    if region_type == "ì „êµ­":
        filters = []
        logger.debug(f"ì§€ì—­ í•„í„°: ì „êµ­ (í•„í„° ì—†ìŒ)")
        return filters
    elif region_type == "ìˆ˜ë„ê¶Œ":
        filters = [State.city_name.in_(['ì„œìš¸íŠ¹ë³„ì‹œ', 'ê²½ê¸°ë„', 'ì¸ì²œê´‘ì—­ì‹œ'])]
        logger.debug(f"ì§€ì—­ í•„í„°: ìˆ˜ë„ê¶Œ - ì„œìš¸íŠ¹ë³„ì‹œ, ê²½ê¸°ë„, ì¸ì²œê´‘ì—­ì‹œ")
        return filters
    elif region_type == "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ":
        if city_name:
            filters = [State.city_name == city_name]
            logger.debug(f"ì§€ì—­ í•„í„°: ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ - {city_name}")
            return filters
        filters = [State.city_name.in_(['ë¶€ì‚°ê´‘ì—­ì‹œ', 'ëŒ€êµ¬ê´‘ì—­ì‹œ', 'ê´‘ì£¼ê´‘ì—­ì‹œ', 'ëŒ€ì „ê´‘ì—­ì‹œ', 'ìš¸ì‚°ê´‘ì—­ì‹œ'])]
        logger.debug(f"ì§€ì—­ í•„í„°: ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ - ë¶€ì‚°ê´‘ì—­ì‹œ, ëŒ€êµ¬ê´‘ì—­ì‹œ, ê´‘ì£¼ê´‘ì—­ì‹œ, ëŒ€ì „ê´‘ì—­ì‹œ, ìš¸ì‚°ê´‘ì—­ì‹œ")
        return filters
    else:
        logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” region_type: {region_type}")
        return []


async def get_thresholds(
    db: AsyncSession,
    region_type: str,
    region_name: Optional[str] = None,
    volume_threshold: Optional[float] = None,
    price_threshold: Optional[float] = None
) -> tuple[float, float]:
    """
    ì„ê³„ê°’ ì¡°íšŒ (API íŒŒë¼ë¯¸í„° ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
    
    ìš°ì„ ìˆœìœ„:
    1. API íŒŒë¼ë¯¸í„°
    2. ì§€ì—­ë³„ ì„¤ì •ê°’ í…Œì´ë¸” (í–¥í›„ êµ¬í˜„)
    3. ê¸°ë³¸ê°’
    
    Args:
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        region_type: ì§€ì—­ ìœ í˜• ("ì „êµ­", "ìˆ˜ë„ê¶Œ", "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ")
        region_name: ì§€ì—­ëª… (ì§€ë°©5ëŒ€ê´‘ì—­ì‹œì¼ ë•Œ)
        volume_threshold: API íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬ëœ ê±°ë˜ëŸ‰ ì„ê³„ê°’
        price_threshold: API íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬ëœ ê°€ê²© ì„ê³„ê°’
    
    Returns:
        (volume_threshold, price_threshold) íŠœí”Œ
    """
    # 1. API íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
    if volume_threshold is not None and price_threshold is not None:
        return volume_threshold, price_threshold
    
    # 2. ì§€ì—­ë³„ ì„¤ì •ê°’ í…Œì´ë¸”ì—ì„œ ì¡°íšŒ (í–¥í›„ êµ¬í˜„)
    # TODO: market_phase_thresholds í…Œì´ë¸” ì¡°íšŒ
    # if db:
    #     threshold_record = await db.query(MarketPhaseThreshold).filter(
    #         MarketPhaseThreshold.region_type == region_type,
    #         MarketPhaseThreshold.region_name == region_name if region_name else None
    #     ).first()
    #     
    #     if threshold_record:
    #         return (
    #             volume_threshold or threshold_record.volume_threshold,
    #             price_threshold or threshold_record.price_threshold
    #         )
    
    # 3. ì§€ì—­ë³„ ê¸°ë³¸ê°’ ì‚¬ìš©
    # API íŒŒë¼ë¯¸í„°ê°€ ì—†ìœ¼ë©´ ì§€ì—­ë³„ ê¸°ë³¸ê°’ ì ìš©
    if region_type == "ì „êµ­":
        default_vol_threshold = 2.0
        default_price_threshold = 0.5
    elif region_type == "ìˆ˜ë„ê¶Œ":
        default_vol_threshold = 2.5
        default_price_threshold = 0.6
    elif region_type == "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ":
        default_vol_threshold = 1.7
        default_price_threshold = 0.4
    else:
        default_vol_threshold = 2.0
        default_price_threshold = 0.5
    
    final_vol_threshold = volume_threshold if volume_threshold is not None else default_vol_threshold
    final_price_threshold = price_threshold if price_threshold is not None else default_price_threshold
    
    logger.info(
        f"[Thresholds] Threshold lookup - "
        f"region_type: {region_type}, region_name: {region_name}, "
        f"API params: vol={volume_threshold}, price={price_threshold}, "
        f"Final values: vol={final_vol_threshold}, price={final_price_threshold}"
    )
    
    return final_vol_threshold, final_price_threshold


def calculate_market_phase(
    volume_change_rate: Optional[float],
    price_change_rate: Optional[float],
    current_month_volume: int,
    min_transaction_count: int = 5,
    volume_threshold: float = 2.0,
    price_threshold: float = 0.5
) -> dict:
    """
    ë²Œì§‘ ìˆœí™˜ ëª¨í˜•ì— ë”°ë¥¸ ì‹œì¥ êµ­ë©´ íŒë³„
    
    6ê°œ êµ­ë©´:
    1. íšŒë³µ (Recovery): ê±°ë˜ëŸ‰ ì¦ê°€ â†‘ / ê°€ê²© í•˜ë½ í˜¹ì€ ë³´í•© â†’
    2. ìƒìŠ¹ (Expansion): ê±°ë˜ëŸ‰ ì¦ê°€ â†‘ / ê°€ê²© ìƒìŠ¹ â†‘
    3. ë‘”í™” (Slowdown): ê±°ë˜ëŸ‰ ê°ì†Œ â†“ / ê°€ê²© ìƒìŠ¹ â†‘
    4. í›„í‡´ (Recession): ê±°ë˜ëŸ‰ ê°ì†Œ â†“ / ê°€ê²© í•˜ë½ â†“
    5. ì¹¨ì²´ (Depression): ê±°ë˜ëŸ‰ ê¸‰ê° â†“ / ê°€ê²© í•˜ë½ì„¸ ì§€ì† â†“
    6. ì²œì°© (Trough): ê±°ë˜ëŸ‰ ë¯¸ì„¸ ì¦ê°€ â†‘ / ê°€ê²© í•˜ë½ â†“
    
    Args:
        volume_change_rate: ê±°ë˜ëŸ‰ ë³€ë™ë¥  (%)
        price_change_rate: ê°€ê²© ë³€ë™ë¥  (%)
        current_month_volume: í˜„ì¬ ì›” ê±°ë˜ëŸ‰
        min_transaction_count: ìµœì†Œ ê±°ë˜ ê±´ìˆ˜ (ê¸°ë³¸ê°’: 5)
        volume_threshold: ê±°ë˜ëŸ‰ ë³€ë™ ì„ê³„ê°’ (%)
        price_threshold: ê°€ê²© ë³€ë™ ì„ê³„ê°’ (%)
    
    Returns:
        {
            "phase": int | None,
            "phase_label": str,
            "description": str,
            "current_month_volume": int,
            "min_required_volume": int
        } ë”•ì…”ë„ˆë¦¬
    """
    # ì˜ˆì™¸ ì²˜ë¦¬: ê±°ë˜ëŸ‰ì´ ë„ˆë¬´ ì ì€ ê²½ìš°
    if current_month_volume < min_transaction_count:
        return {
            "phase": None,
            "phase_label": "ë°ì´í„° ë¶€ì¡±",
            "description": f"ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ íŒë³„ ë¶ˆê°€ (í˜„ì¬ ì›” ê±°ë˜ëŸ‰: {current_month_volume}ê±´, ìµœì†Œ ìš”êµ¬ëŸ‰: {min_transaction_count}ê±´)",
            "current_month_volume": current_month_volume,
            "min_required_volume": min_transaction_count
        }
    
    # ë°ì´í„° ë¶€ì¡± ì²´í¬
    if volume_change_rate is None or price_change_rate is None:
        return {
            "phase": None,
            "phase_label": "ë°ì´í„° ë¶€ì¡±",
            "description": "ê°€ê²© ë˜ëŠ” ê±°ë˜ëŸ‰ ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ íŒë³„ ë¶ˆê°€",
            "current_month_volume": current_month_volume,
            "min_required_volume": min_transaction_count
        }
    
    # ì„ê³„ê°’ ê¸°ë°˜ íŒë³„
    volume_up = volume_change_rate > volume_threshold
    volume_down = volume_change_rate < -volume_threshold
    price_up = price_change_rate > price_threshold
    price_down = price_change_rate < -price_threshold
    price_stable = -price_threshold <= price_change_rate <= price_threshold
    
    # 1. íšŒë³µ (Recovery): ê±°ë˜ëŸ‰ ì¦ê°€ â†‘ / ê°€ê²© í•˜ë½ í˜¹ì€ ë³´í•© â†’
    if volume_up and (price_down or price_stable):
        return {
            "phase": 1,
            "phase_label": "íšŒë³µ",
            "description": "ê±°ë˜ëŸ‰ ì¦ê°€ì™€ ê°€ê²© í•˜ë½/ë³´í•©ì´ ë™ë°˜ë˜ëŠ” ë°”ë‹¥ ë‹¤ì§€ê¸° ë‹¨ê³„ì…ë‹ˆë‹¤.",
            "current_month_volume": current_month_volume,
            "min_required_volume": min_transaction_count
        }
    
    # 2. ìƒìŠ¹ (Expansion): ê±°ë˜ëŸ‰ ì¦ê°€ â†‘ / ê°€ê²© ìƒìŠ¹ â†‘
    if volume_up and price_up:
        return {
            "phase": 2,
            "phase_label": "ìƒìŠ¹",
            "description": "ê±°ë˜ëŸ‰ ì¦ê°€ì™€ ê°€ê²© ìƒìŠ¹ì´ ë™ë°˜ë˜ëŠ” í™œí™©ê¸°ì…ë‹ˆë‹¤.",
            "current_month_volume": current_month_volume,
            "min_required_volume": min_transaction_count
        }
    
    # 3. ë‘”í™” (Slowdown): ê±°ë˜ëŸ‰ ê°ì†Œ â†“ / ê°€ê²© ìƒìŠ¹ â†‘
    if volume_down and price_up:
        return {
            "phase": 3,
            "phase_label": "ë‘”í™”",
            "description": "ê±°ë˜ëŸ‰ ê°ì†Œì™€ ê°€ê²© ìƒìŠ¹ì´ ë™ë°˜ë˜ëŠ” ì—ë„ˆì§€ ê³ ê°ˆ ë‹¨ê³„ì…ë‹ˆë‹¤.",
            "current_month_volume": current_month_volume,
            "min_required_volume": min_transaction_count
        }
    
    # 4. í›„í‡´ (Recession): ê±°ë˜ëŸ‰ ê°ì†Œ â†“ / ê°€ê²© í•˜ë½ â†“
    if volume_down and price_down:
        return {
            "phase": 4,
            "phase_label": "í›„í‡´",
            "description": "ê±°ë˜ëŸ‰ ê°ì†Œì™€ ê°€ê²© í•˜ë½ì´ ë™ë°˜ë˜ëŠ” ë³¸ê²© í•˜ë½ ë‹¨ê³„ì…ë‹ˆë‹¤.",
            "current_month_volume": current_month_volume,
            "min_required_volume": min_transaction_count
        }
    
    # 5. ì¹¨ì²´ (Depression): ê±°ë˜ëŸ‰ ê¸‰ê° â†“ / ê°€ê²© í•˜ë½ì„¸ ì§€ì† â†“
    if volume_change_rate < -5.0 and price_change_rate < -1.0:
        return {
            "phase": 5,
            "phase_label": "ì¹¨ì²´",
            "description": "ê±°ë˜ëŸ‰ ê¸‰ê°ê³¼ ê°€ê²© í•˜ë½ì„¸ ì§€ì†ì´ ë™ë°˜ë˜ëŠ” ì¹¨ì²´ê¸°ì…ë‹ˆë‹¤.",
            "current_month_volume": current_month_volume,
            "min_required_volume": min_transaction_count
        }
    
    # 6. ì²œì°© (Trough): ê±°ë˜ëŸ‰ ë¯¸ì„¸ ì¦ê°€ â†‘ / ê°€ê²© í•˜ë½ â†“
    if 0 < volume_change_rate <= volume_threshold and price_down:
        return {
            "phase": 6,
            "phase_label": "ì²œì°©",
            "description": "ê±°ë˜ëŸ‰ ë¯¸ì„¸ ì¦ê°€ì™€ ê°€ê²© í•˜ë½ì´ ë™ë°˜ë˜ëŠ” ë°˜ë“± ì¤€ë¹„ ë‹¨ê³„ì…ë‹ˆë‹¤.",
            "current_month_volume": current_month_volume,
            "min_required_volume": min_transaction_count
        }
    
    # ê¸°ë³¸ê°’: ì¤‘ë¦½
    return {
        "phase": 0,
        "phase_label": "ì¤‘ë¦½",
        "description": "ì‹œì¥ì´ ì¤‘ë¦½ ìƒíƒœì…ë‹ˆë‹¤.",
        "current_month_volume": current_month_volume,
        "min_required_volume": min_transaction_count
    }


async def calculate_volume_change_rate_average(
    db: AsyncSession,
    region_type: str,
    city_name: Optional[str] = None,
    average_period_months: int = 6
) -> tuple[Optional[float], int]:
    """
    ê³¼ê±° í‰ê·  ëŒ€ë¹„ ê±°ë˜ëŸ‰ ë³€ë™ë¥  ê³„ì‚°
    
    Args:
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        region_type: ì§€ì—­ ìœ í˜•
        city_name: íŠ¹ì • ì‹œë„ëª… (ì§€ë°©5ëŒ€ê´‘ì—­ì‹œì¼ ë•Œ)
        average_period_months: í‰ê·  ê³„ì‚° ê¸°ê°„ (ê°œì›”)
    
    Returns:
        (volume_change_rate, current_month_volume) íŠœí”Œ
    """
    # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ê¸°ê°„ ê³„ì‚°
    # ê°€ì´ë“œ ë¬¸ì„œì— ë”°ë¥´ë©´ "ì´ì „ ë‹¬" ë°ì´í„°ë¥¼ ì¡°íšŒ (ì™„ì „íˆ ì§‘ê³„ëœ ë°ì´í„°)
    now = datetime.now()
    current_month_start = now.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
    
    # ì´ì „ ë‹¬ ê³„ì‚° (ë” ì•ˆì „í•œ ë°©ë²•)
    if current_month_start.month == 1:
        previous_month_start = current_month_start.replace(year=current_month_start.year - 1, month=12)
    else:
        previous_month_start = current_month_start.replace(month=current_month_start.month - 1)
    
    # ì§€ì—­ í•„í„°
    region_filters = get_region_filters(region_type, city_name)
    
    # í˜„ì¬ ì›” ê±°ë˜ëŸ‰ (ì´ì „ ë‹¬ ì™„ì „íˆ ì§‘ê³„ëœ ë°ì´í„°)
    # ê°€ì´ë“œ ë¬¸ì„œ: contract_date >= DATE_TRUNC('month', CURRENT_DATE) - INTERVAL '1 month'
    #            AND contract_date < DATE_TRUNC('month', CURRENT_DATE)
    current_volume_query = select(func.count(Sale.trans_id)).select_from(
        Sale.__table__.join(
            Apartment.__table__,
            Sale.apt_id == Apartment.apt_id
        ).join(
            State.__table__,
            Apartment.region_id == State.region_id
        )
    ).where(
        and_(
            Sale.is_canceled == False,
            or_(Sale.is_deleted == False, Sale.is_deleted.is_(None)),
            Sale.contract_date.isnot(None),
            # TODO: ì‹¤ì œ ë°ì´í„° ì‚¬ìš© ì‹œ ì•„ë˜ ì£¼ì„ í•´ì œ
            # or_(Sale.remarks != 'ë”ë¯¸', Sale.remarks.is_(None)),
            Sale.contract_date >= previous_month_start,
            Sale.contract_date < current_month_start,
            *region_filters
        )
    )
    
    current_volume_result = await db.execute(current_volume_query)
    current_month_volume = current_volume_result.scalar() or 0
    
    # ë””ë²„ê¹…: ì¿¼ë¦¬ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
    if current_month_volume == 0:
        logger.warning(
            f"ê±°ë˜ëŸ‰ ë³€ë™ë¥  ê³„ì‚°: í˜„ì¬ ì›” ê±°ë˜ëŸ‰ 0 - "
            f"region_type: {region_type}, city_name: {city_name}, "
            f"ì¡°íšŒ ê¸°ê°„: {previous_month_start.date()} ~ {current_month_start.date()}, "
            f"í•„í„° ì¡°ê±´: {region_filters}"
        )
        
        # ë””ë²„ê¹…: í•„í„° ì—†ì´ ì „ì²´ ê±°ë˜ëŸ‰ í™•ì¸
        debug_query = select(func.count(Sale.trans_id)).select_from(
            Sale.__table__.join(
                Apartment.__table__,
                Sale.apt_id == Apartment.apt_id
            ).join(
                State.__table__,
                Apartment.region_id == State.region_id
            )
        ).where(
            and_(
                Sale.is_canceled == False,
                or_(Sale.is_deleted == False, Sale.is_deleted.is_(None)),
                Sale.contract_date.isnot(None),
                Sale.contract_date >= previous_month_start,
                Sale.contract_date < current_month_start,
                *region_filters
            )
        )
        debug_result = await db.execute(debug_query)
        debug_count = debug_result.scalar() or 0
        logger.info(f"ë””ë²„ê¹…: í•„í„° ì ìš© ê±°ë˜ëŸ‰ = {debug_count}")
        
        # ë””ë²„ê¹…: í•´ë‹¹ ì§€ì—­ì˜ ì „ì²´ ê±°ë˜ëŸ‰ í™•ì¸ (í•„í„° ì—†ì´)
        if city_name:
            city_only_query = select(func.count(Sale.trans_id)).select_from(
                Sale.__table__.join(
                    Apartment.__table__,
                    Sale.apt_id == Apartment.apt_id
                ).join(
                    State.__table__,
                    Apartment.region_id == State.region_id
                )
            ).where(
                and_(
                    Sale.is_canceled == False,
                    or_(Sale.is_deleted == False, Sale.is_deleted.is_(None)),
                    Sale.contract_date.isnot(None),
                    Sale.contract_date >= previous_month_start,
                    Sale.contract_date < current_month_start,
                    State.city_name == city_name
                )
            )
            city_result = await db.execute(city_only_query)
            city_count = city_result.scalar() or 0
            logger.info(
                f"ğŸ” ë””ë²„ê¹…: {city_name} ì§€ì—­ ì „ì²´ ê±°ë˜ëŸ‰ (í•„í„° ì—†ì´) = {city_count}, "
                f"ì¡°íšŒ ê¸°ê°„: {previous_month_start.date()} ~ {current_month_start.date()}"
            )
            
            # ë””ë²„ê¹…: í•´ë‹¹ ì§€ì—­ì˜ ì•„íŒŒíŠ¸ ìˆ˜ í™•ì¸
            apt_count_query = select(func.count(Apartment.apt_id)).select_from(
                Apartment.__table__.join(
                    State.__table__,
                    Apartment.region_id == State.region_id
                )
            ).where(
                State.city_name == city_name
            )
            apt_result = await db.execute(apt_count_query)
            apt_count = apt_result.scalar() or 0
            logger.info(f"ğŸ” ë””ë²„ê¹…: {city_name} ì§€ì—­ ì•„íŒŒíŠ¸ ìˆ˜ = {apt_count}")
            
            # ë””ë²„ê¹…: í•´ë‹¹ ì§€ì—­ì˜ ì „ì²´ ê±°ë˜ ìˆ˜ í™•ì¸ (ê¸°ê°„ ì œí•œ ì—†ì´)
            all_time_query = select(func.count(Sale.trans_id)).select_from(
                Sale.__table__.join(
                    Apartment.__table__,
                    Sale.apt_id == Apartment.apt_id
                ).join(
                    State.__table__,
                    Apartment.region_id == State.region_id
                )
            ).where(
                and_(
                    Sale.is_canceled == False,
                    or_(Sale.is_deleted == False, Sale.is_deleted.is_(None)),
                    Sale.contract_date.isnot(None),
                    State.city_name == city_name
                )
            )
            all_time_result = await db.execute(all_time_query)
            all_time_count = all_time_result.scalar() or 0
            logger.info(f"ğŸ” ë””ë²„ê¹…: {city_name} ì§€ì—­ ì „ì²´ ê¸°ê°„ ê±°ë˜ëŸ‰ = {all_time_count}")
        
        return None, 0
    
    # ê³¼ê±° í‰ê·  ê±°ë˜ëŸ‰ ê³„ì‚° (Nê°œì›” í‰ê· )
    # ì›”ë³„ ê±°ë˜ëŸ‰ì„ êµ¬í•œ í›„ í‰ê·  ê³„ì‚°
    avg_start_date = previous_month_start - timedelta(days=30 * average_period_months)
    
    # ì›”ë³„ ê±°ë˜ëŸ‰ ì¡°íšŒ
    monthly_volumes_query = select(
        extract('year', Sale.contract_date).label('year'),
        extract('month', Sale.contract_date).label('month'),
        func.count(Sale.trans_id).label('volume')
    ).select_from(
        Sale.__table__.join(
            Apartment.__table__,
            Sale.apt_id == Apartment.apt_id
        ).join(
            State.__table__,
            Apartment.region_id == State.region_id
        )
    ).where(
        and_(
            Sale.is_canceled == False,
            or_(Sale.is_deleted == False, Sale.is_deleted.is_(None)),
            Sale.contract_date.isnot(None),
            # TODO: ì‹¤ì œ ë°ì´í„° ì‚¬ìš© ì‹œ ì•„ë˜ ì£¼ì„ í•´ì œ
            # or_(Sale.remarks != 'ë”ë¯¸', Sale.remarks.is_(None)),
            Sale.contract_date >= avg_start_date,
            Sale.contract_date < previous_month_start,
            *region_filters
        )
    ).group_by(
        extract('year', Sale.contract_date),
        extract('month', Sale.contract_date)
    )
    
    monthly_volumes_result = await db.execute(monthly_volumes_query)
    monthly_volumes = [row.volume for row in monthly_volumes_result.fetchall()]
    
    if not monthly_volumes:
        logger.warning(
            f"ê±°ë˜ëŸ‰ ë³€ë™ë¥  ê³„ì‚°: ê³¼ê±° í‰ê·  ë°ì´í„° ì—†ìŒ - "
            f"region_type: {region_type}, city_name: {city_name}, "
            f"ê¸°ê°„: {average_period_months}ê°œì›”"
        )
        return None, current_month_volume
    
    avg_volume = sum(monthly_volumes) / len(monthly_volumes)
    
    if avg_volume == 0:
        logger.warning(
            f"ê±°ë˜ëŸ‰ ë³€ë™ë¥  ê³„ì‚°: ê³¼ê±° í‰ê·  ê±°ë˜ëŸ‰ 0 - "
            f"region_type: {region_type}, city_name: {city_name}"
        )
        return None, current_month_volume
    
    volume_change_rate = ((current_month_volume - avg_volume) / avg_volume) * 100
    return volume_change_rate, current_month_volume


async def calculate_volume_change_rate_mom(
    db: AsyncSession,
    region_type: str,
    city_name: Optional[str] = None
) -> tuple[Optional[float], int]:
    """
    ì „ì›” ëŒ€ë¹„ ê±°ë˜ëŸ‰ ë³€ë™ë¥  ê³„ì‚°
    
    Args:
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        region_type: ì§€ì—­ ìœ í˜•
        city_name: íŠ¹ì • ì‹œë„ëª… (ì§€ë°©5ëŒ€ê´‘ì—­ì‹œì¼ ë•Œ)
    
    Returns:
        (volume_change_rate, current_month_volume) íŠœí”Œ
    """
    # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ê¸°ê°„ ê³„ì‚°
    now = datetime.now()
    current_month_start = now.replace(day=1)
    previous_month_start = (current_month_start - timedelta(days=1)).replace(day=1)
    two_months_ago_start = (previous_month_start - timedelta(days=1)).replace(day=1)
    
    # ì§€ì—­ í•„í„°
    region_filters = get_region_filters(region_type, city_name)
    logger.debug(
        f"ê±°ë˜ëŸ‰ ë³€ë™ë¥  ê³„ì‚° (mom) - "
        f"region_type: {region_type}, city_name: {city_name}, "
        f"í•„í„° ê°œìˆ˜: {len(region_filters)}"
    )
    
    # ìµœê·¼ 2ê°œì›” ê±°ë˜ëŸ‰ ì¡°íšŒ
    monthly_volumes_query = select(
        extract('year', Sale.contract_date).label('year'),
        extract('month', Sale.contract_date).label('month'),
        func.count(Sale.trans_id).label('volume')
    ).select_from(
        Sale.__table__.join(
            Apartment.__table__,
            Sale.apt_id == Apartment.apt_id
        ).join(
            State.__table__,
            Apartment.region_id == State.region_id
        )
    ).where(
        and_(
            Sale.is_canceled == False,
            or_(Sale.is_deleted == False, Sale.is_deleted.is_(None)),
            Sale.contract_date.isnot(None),
            # TODO: ì‹¤ì œ ë°ì´í„° ì‚¬ìš© ì‹œ ì•„ë˜ ì£¼ì„ í•´ì œ
            # or_(Sale.remarks != 'ë”ë¯¸', Sale.remarks.is_(None)),
            Sale.contract_date >= two_months_ago_start,
            Sale.contract_date < current_month_start,
            *region_filters
        )
    ).group_by(
        extract('year', Sale.contract_date),
        extract('month', Sale.contract_date)
    ).order_by(
        desc(extract('year', Sale.contract_date)),
        desc(extract('month', Sale.contract_date))
    ).limit(2)
    
    monthly_volumes_result = await db.execute(monthly_volumes_query)
    monthly_data = monthly_volumes_result.fetchall()
    
    if len(monthly_data) < 2:
        logger.warning(
            f"ê±°ë˜ëŸ‰ ë³€ë™ë¥  ê³„ì‚° (ì „ì›” ëŒ€ë¹„): ë°ì´í„° ë¶€ì¡± - "
            f"í•„ìš”: 2ê°œì›”, ì‹¤ì œ: {len(monthly_data)}ê°œì›”, "
            f"region_type: {region_type}, city_name: {city_name}"
        )
        return None, 0
    
    current_volume = monthly_data[0].volume
    previous_volume = monthly_data[1].volume
    
    if previous_volume == 0:
        logger.warning(
            f"ê±°ë˜ëŸ‰ ë³€ë™ë¥  ê³„ì‚° (ì „ì›” ëŒ€ë¹„): ì „ì›” ê±°ë˜ëŸ‰ 0 - "
            f"region_type: {region_type}, city_name: {city_name}"
        )
        return None, current_volume
    
    volume_change_rate = ((current_volume - previous_volume) / previous_volume) * 100
    return volume_change_rate, current_volume


async def calculate_price_change_rate_moving_average(
    db: AsyncSession,
    region_type: str,
    city_name: Optional[str] = None
) -> Optional[float]:
    """
    ìµœê·¼ 3ê°œì›” ì´ë™í‰ê·  ë³€ë™ë¥  ê³„ì‚°
    
    ìµœê·¼ 3ê°œì›” í‰ê·  vs ì´ì „ 3ê°œì›” í‰ê·  ë¹„êµ
    
    Args:
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        region_type: ì§€ì—­ ìœ í˜•
        city_name: íŠ¹ì • ì‹œë„ëª… (ì§€ë°©5ëŒ€ê´‘ì—­ì‹œì¼ ë•Œ)
    
    Returns:
        ê°€ê²© ë³€ë™ë¥  (%) ë˜ëŠ” None
    """
    # ìµœê·¼ 6ê°œì›” HPI ë°ì´í„° ì¡°íšŒ í•„ìš”
    # base_ymì€ YYYYMM í˜•ì‹ ë¬¸ìì—´ (CHAR(6))
    now = datetime.now()
    current_year_month = now.strftime('%Y%m')  # ë¬¸ìì—´ë¡œ ìœ ì§€
    
    # 6ê°œì›” ì „ base_ym ê³„ì‚°
    six_months_ago = now - timedelta(days=180)
    start_base_ym = six_months_ago.strftime('%Y%m')  # ë¬¸ìì—´ë¡œ ìœ ì§€
    
    # ì§€ì—­ í•„í„°
    region_filters = get_region_filters(region_type, city_name)
    logger.debug(
        f"ê°€ê²© ë³€ë™ë¥  ê³„ì‚° - "
        f"region_type: {region_type}, city_name: {city_name}, "
        f"í•„í„° ê°œìˆ˜: {len(region_filters)}"
    )
    
    # HPI ë°ì´í„° ì¡°íšŒ (ìµœê·¼ 6ê°œì›”)
    # base_ymì€ ë¬¸ìì—´ì´ë¯€ë¡œ ë¬¸ìì—´ ë¹„êµ ì‚¬ìš©
    hpi_query = select(
        HouseScore.base_ym,
        HouseScore.index_value,
        State.city_name,
        State.region_id
    ).join(
        State, HouseScore.region_id == State.region_id
    ).where(
        and_(
            HouseScore.is_deleted == False,
            State.is_deleted == False,
            HouseScore.index_type == 'APT',
            HouseScore.base_ym >= start_base_ym,
            HouseScore.base_ym <= current_year_month,
            *region_filters
        )
    ).order_by(
        desc(HouseScore.base_ym)
    )
    
    hpi_result = await db.execute(hpi_query)
    hpi_data = hpi_result.fetchall()
    
    if len(hpi_data) < 6:
        # ìµœì†Œ 6ê°œì›” ë°ì´í„° í•„ìš”
        logger.warning(
            f"ê°€ê²© ë³€ë™ë¥  ê³„ì‚°: ë°ì´í„° ë¶€ì¡± - "
            f"í•„ìš”: 6ê°œì›”, ì‹¤ì œ: {len(hpi_data)}ê°œì›”"
        )
        return None
    
    # ì „êµ­/ìˆ˜ë„ê¶Œ: ì „ì²´ í‰ê·  ê³„ì‚°
    if region_type in ["ì „êµ­", "ìˆ˜ë„ê¶Œ"]:
        # base_ymë³„ë¡œ í‰ê·  index_value ê³„ì‚°
        hpi_by_month = defaultdict(list)
        for row in hpi_data:
            hpi_by_month[row.base_ym].append(row.index_value)
        
        # ì›”ë³„ í‰ê·  ê³„ì‚°
        monthly_avg = {
            base_ym: sum(values) / len(values)
            for base_ym, values in hpi_by_month.items()
        }
        
        # base_ym ìˆœì„œëŒ€ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        # base_ymì€ YYYYMM í˜•ì‹ ë¬¸ìì—´ì´ë¯€ë¡œ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ì •ë ¬
        sorted_months = sorted(
            monthly_avg.keys(), 
            key=lambda x: int(x) if isinstance(x, str) and x.isdigit() else int(x) if isinstance(x, (int, float)) else 0,
            reverse=True
        )
        
        if len(sorted_months) < 6:
            logger.warning(
                f"ê°€ê²© ë³€ë™ë¥  ê³„ì‚°: ë°ì´í„° ë¶€ì¡± - "
                f"í•„ìš”: 6ê°œì›”, ì‹¤ì œ: {len(sorted_months)}ê°œì›” (region_type: {region_type})"
            )
            return None
        
        # ìµœê·¼ 3ê°œì›” í‰ê· 
        recent_3months_values = [monthly_avg[m] for m in sorted_months[:3]]
        current_avg = sum(recent_3months_values) / len(recent_3months_values)
        
        # ì´ì „ 3ê°œì›” í‰ê·  (4~6ê°œì›” ì „)
        previous_3months_values = [monthly_avg[m] for m in sorted_months[3:6]]
        previous_avg = sum(previous_3months_values) / len(previous_3months_values)
        
        if previous_avg == 0:
            logger.warning(
                f"ê°€ê²© ë³€ë™ë¥  ê³„ì‚°: ì´ì „ í‰ê· ì´ 0 - region_type: {region_type}"
            )
            return None
        
        price_change_rate = ((current_avg - previous_avg) / previous_avg) * 100
        return price_change_rate
    
    # ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ: íŠ¹ì • ì§€ì—­ë³„ ê³„ì‚°
    else:
        if not city_name:
            if not hpi_data:
                logger.warning(
                    f"ê°€ê²© ë³€ë™ë¥  ê³„ì‚°: ë°ì´í„° ì—†ìŒ - region_type: {region_type}"
                )
                return None
            # city_nameì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì§€ì—­ ì‚¬ìš©
            city_name = hpi_data[0].city_name
        
        # í•´ë‹¹ ì§€ì—­ì˜ ë°ì´í„°ë§Œ í•„í„°ë§
        region_hpi = [row for row in hpi_data if row.city_name == city_name]
        
        if len(region_hpi) < 6:
            logger.warning(
                f"ê°€ê²© ë³€ë™ë¥  ê³„ì‚°: ë°ì´í„° ë¶€ì¡± - "
                f"í•„ìš”: 6ê°œì›”, ì‹¤ì œ: {len(region_hpi)}ê°œì›” (ì§€ì—­: {city_name})"
            )
            return None
        
        # base_ymë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í‰ê·  ê³„ì‚° (ê°™ì€ base_ymì— ì—¬ëŸ¬ ë°ì´í„°ê°€ ìˆì„ ìˆ˜ ìˆìŒ)
        hpi_by_month = defaultdict(list)
        for row in region_hpi:
            hpi_by_month[row.base_ym].append(float(row.index_value))
        
        # ì›”ë³„ í‰ê·  ê³„ì‚°
        monthly_avg = {
            base_ym: sum(values) / len(values)
            for base_ym, values in hpi_by_month.items()
        }
        
        # base_ym ìˆœì„œëŒ€ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        sorted_months = sorted(
            monthly_avg.keys(), 
            key=lambda x: int(x) if isinstance(x, str) and x.isdigit() else int(x) if isinstance(x, (int, float)) else 0,
            reverse=True
        )
        
        if len(sorted_months) < 6:
            logger.warning(
                f"ê°€ê²© ë³€ë™ë¥  ê³„ì‚°: ë°ì´í„° ë¶€ì¡± - "
                f"í•„ìš”: 6ê°œì›”, ì‹¤ì œ: {len(sorted_months)}ê°œì›” (ì§€ì—­: {city_name})"
            )
            return None
        
        # ìµœê·¼ 3ê°œì›” í‰ê· 
        recent_3months_values = [monthly_avg[m] for m in sorted_months[:3]]
        current_avg = sum(recent_3months_values) / len(recent_3months_values)
        
        # ì´ì „ 3ê°œì›” í‰ê·  (4~6ê°œì›” ì „)
        previous_3months_values = [monthly_avg[m] for m in sorted_months[3:6]]
        previous_avg = sum(previous_3months_values) / len(previous_3months_values)
        
        if previous_avg == 0:
            logger.warning(
                f"ê°€ê²© ë³€ë™ë¥  ê³„ì‚°: ì´ì „ í‰ê· ì´ 0 - ì§€ì—­: {city_name}"
            )
            return None
        
        price_change_rate = ((current_avg - previous_avg) / previous_avg) * 100
        return price_change_rate


@router.get(
    "/rvol",
    response_model=RVOLResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="RVOL(ìƒëŒ€ ê±°ë˜ëŸ‰) ì¡°íšŒ",
    description="""
    RVOL(Relative Volume)ì„ ê³„ì‚°í•˜ì—¬ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ### RVOL ê³„ì‚° ë°©ë²•
    - í˜„ì¬ ê±°ë˜ëŸ‰ì„ ê³¼ê±° ì¼ì • ê¸°ê°„ì˜ í‰ê·  ê±°ë˜ëŸ‰ìœ¼ë¡œ ë‚˜ëˆˆ ê°’
    - ì˜ˆ: ìµœê·¼ 2ê°œì›” ê±°ë˜ëŸ‰ Ã· ì§ì „ 2ê°œì›” í‰ê·  ê±°ë˜ëŸ‰
    
    ### í•´ì„
    - **RVOL > 1**: í‰ì†Œë³´ë‹¤ ê±°ë˜ê°€ í™œë°œí•¨ (í‰ê·  ì´ìƒ)
    - **RVOL = 1**: í‰ì†Œì™€ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì˜ ê±°ë˜ëŸ‰
    - **RVOL < 1**: í‰ì†Œë³´ë‹¤ ê±°ë˜ê°€ í•œì‚°í•¨ (í‰ê·  ì´í•˜)
    
    ### Query Parameters
    - `transaction_type`: ê±°ë˜ ìœ í˜• (sale: ë§¤ë§¤, rent: ì „ì›”ì„¸, ê¸°ë³¸ê°’: sale)
    - `current_period_months`: í˜„ì¬ ê¸°ê°„ (ê°œì›”, ê¸°ë³¸ê°’: 6, ìµœëŒ€: 6)
    - `average_period_months`: í‰ê·  ê³„ì‚° ê¸°ê°„ (ê°œì›”, ê¸°ë³¸ê°’: 6, ìµœëŒ€: 6)
    """
)
async def get_rvol(
    transaction_type: str = Query("sale", description="ê±°ë˜ ìœ í˜•: sale(ë§¤ë§¤), rent(ì „ì›”ì„¸)"),
    current_period_months: int = Query(6, ge=1, le=12, description="í˜„ì¬ ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 12)"),
    average_period_months: int = Query(6, ge=1, le=12, description="í‰ê·  ê³„ì‚° ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 12)"),
    db: AsyncSession = Depends(get_db)
):
    """
    RVOL(ìƒëŒ€ ê±°ë˜ëŸ‰) ì¡°íšŒ - ì„±ëŠ¥ ìµœì í™” ë²„ì „
    
    ì›”ë³„ ì§‘ê³„ë¡œ ê°„ì†Œí™”í•˜ì—¬ ë¹ ë¥¸ ì‘ë‹µ ì œê³µ
    """
    cache_key = build_cache_key(
        "statistics", "rvol_v2", transaction_type, 
        str(current_period_months), str(average_period_months)
    )
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        logger.info(f"âœ… [Statistics RVOL] ìºì‹œì—ì„œ ë°˜í™˜")
        return cached_data
    
    try:
        logger.info(
            f"ğŸ” [Statistics RVOL] RVOL ë°ì´í„° ì¡°íšŒ ì‹œì‘ - "
            f"transaction_type: {transaction_type}, "
            f"current_period_months: {current_period_months}, "
            f"average_period_months: {average_period_months}"
        )
        
        # ê±°ë˜ ìœ í˜•ì— ë”°ë¥¸ í…Œì´ë¸” ë° í•„ë“œ ì„ íƒ
        if transaction_type == "sale":
            trans_table = Sale
            date_field = Sale.contract_date
            base_filter = and_(
                Sale.is_canceled == False,
                (Sale.is_deleted == False) | (Sale.is_deleted.is_(None)),
                Sale.contract_date.isnot(None),
                or_(Sale.remarks != "ë”ë¯¸", Sale.remarks.is_(None))
            )
        else:  # rent
            trans_table = Rent
            date_field = Rent.deal_date
            base_filter = and_(
                (Rent.is_deleted == False) | (Rent.is_deleted.is_(None)),
                Rent.deal_date.isnot(None),
                or_(Rent.remarks != "ë”ë¯¸", Rent.remarks.is_(None))
            )
        
        # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ê¸°ê°„ ì„¤ì • (min/max ì¿¼ë¦¬ ì œê±°)
        today = date.today()
        # í˜„ì¬ ë‹¬ì˜ ì²« ë‚  (í˜„ì¬ ë‹¬ ì œì™¸)
        current_month_start = date(today.year, today.month, 1)
        
        # í˜„ì¬ ê¸°ê°„: ìµœê·¼ current_period_months ê°œì›” (í˜„ì¬ ë‹¬ ì œì™¸)
        current_start = current_month_start - timedelta(days=current_period_months * 30)
        current_end = current_month_start  # í˜„ì¬ ë‹¬ì˜ ì²« ë‚  ì „ê¹Œì§€
        
        # í‰ê·  ê³„ì‚° ê¸°ê°„: current_start ì´ì „ average_period_months ê°œì›”
        average_start = current_start - timedelta(days=average_period_months * 30)
        average_end = current_start
        
        logger.info(
            f"ğŸ“… [Statistics RVOL] ë‚ ì§œ ë²”ìœ„ - "
            f"current_start: {current_start}, current_end: {current_end}, "
            f"average_start: {average_start}, average_end: {average_end}"
        )
        
        # ì›”ë³„ ì§‘ê³„ë¡œ ê°„ì†Œí™” (ì¼ë³„ ëŒ€ì‹  ì›”ë³„)
        # í‰ê·  ê¸°ê°„ ì›”ë³„ ê±°ë˜ëŸ‰
        average_volume_stmt = (
            select(
                extract('year', date_field).label('year'),
                extract('month', date_field).label('month'),
                func.count(trans_table.trans_id).label('count')
            )
            .where(
                and_(
                    base_filter,
                    date_field >= average_start,
                    date_field < average_end
                )
            )
            .group_by(extract('year', date_field), extract('month', date_field))
        )
        
        # í˜„ì¬ ê¸°ê°„ ì›”ë³„ ê±°ë˜ëŸ‰
        current_volume_stmt = (
            select(
                extract('year', date_field).label('year'),
                extract('month', date_field).label('month'),
                func.count(trans_table.trans_id).label('count')
            )
            .where(
                and_(
                    base_filter,
                    date_field >= current_start,
                    date_field < current_end  # í˜„ì¬ ë‹¬ ì œì™¸ (ë¯¸ë§Œìœ¼ë¡œ ë³€ê²½)
                )
            )
            .group_by(extract('year', date_field), extract('month', date_field))
        )
        
        # ë³‘ë ¬ ì‹¤í–‰
        average_result, current_result = await asyncio.gather(
            db.execute(average_volume_stmt),
            db.execute(current_volume_stmt)
        )
        
        average_rows = average_result.fetchall()
        current_rows = current_result.fetchall()
        
        # í‰ê·  ê±°ë˜ëŸ‰ ê³„ì‚°
        if average_rows:
            total_average = sum(row.count for row in average_rows)
            average_monthly_volume = total_average / len(average_rows)
        else:
            average_monthly_volume = 1  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        
        logger.info(
            f"ğŸ“Š [Statistics RVOL] í‰ê·  ê±°ë˜ëŸ‰ ê³„ì‚° - "
            f"average_monthly_volume: {average_monthly_volume}"
        )
        
        # RVOL ë°ì´í„° ìƒì„± (ì›”ë³„) - í˜„ì¬ ë‹¬ ì œì™¸
        rvol_data = []
        current_year = today.year
        current_month = today.month
        
        for row in current_rows:
            year = int(row.year)
            month = int(row.month)
            
            # í˜„ì¬ ë‹¬ ì œì™¸
            if year == current_year and month == current_month:
                continue
                
            count = row.count or 0
            
            # RVOL ê³„ì‚°
            rvol = count / average_monthly_volume if average_monthly_volume > 0 else 0
            
            rvol_data.append(
                RVOLDataPoint(
                    date=f"{year}-{month:02d}-01",
                    current_volume=count,
                    average_volume=round(average_monthly_volume, 2),
                    rvol=round(rvol, 2)
                )
            )
        
        # ë‚ ì§œìˆœ ì •ë ¬
        rvol_data.sort(key=lambda x: x.date)
        
        period_description = f"ìµœê·¼ {current_period_months}ê°œì›” vs ì§ì „ {average_period_months}ê°œì›”"
        
        response_data = RVOLResponse(
            success=True,
            data=rvol_data,
            period=period_description
        )
        
        # ìºì‹œì— ì €ì¥ (TTL: 6ì‹œê°„)
        if len(rvol_data) > 0:
            await set_to_cache(cache_key, response_data.dict(), ttl=STATISTICS_CACHE_TTL)
        
        logger.info(f"âœ… [Statistics RVOL] RVOL ë°ì´í„° ìƒì„± ì™„ë£Œ - ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(rvol_data)}")
        
        return response_data
        
    except Exception as e:
        logger.error(f"âŒ [Statistics RVOL] RVOL ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"RVOL ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/quadrant",
    response_model=QuadrantResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="4ë¶„ë©´ ë¶„ë¥˜ ì¡°íšŒ",
    description="""
    ë§¤ë§¤ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨ê³¼ ì „ì›”ì„¸ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨ì„ ê¸°ë°˜ìœ¼ë¡œ 4ë¶„ë©´ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    ### 4ë¶„ë©´ ë¶„ë¥˜
    - **xì¶•**: ë§¤ë§¤ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨
    - **yì¶•**: ì „ì›”ì„¸ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨
    
    ### í•´ì„
    1. **ë§¤ë§¤â†‘ / ì „ì›”ì„¸â†“**: ë§¤ìˆ˜ ì „í™˜ (ì‚¬ëŠ” ìª½ìœ¼ë¡œ ì´ë™)
    2. **ë§¤ë§¤â†“ / ì „ì›”ì„¸â†‘**: ì„ëŒ€ ì„ í˜¸/ê´€ë§ (ë¹Œë¦¬ëŠ” ìª½ìœ¼ë¡œ ì´ë™)
    3. **ë§¤ë§¤â†“ / ì „ì›”ì„¸â†“**: ì‹œì¥ ìœ„ì¶• (ì „ì²´ ìœ ë™ì„± ê²½ìƒ‰)
    4. **ë§¤ë§¤â†‘ / ì „ì›”ì„¸â†‘**: í™œì„±í™” (ìˆ˜ìš” ìì²´ê°€ ê°•í•¨, ì´ì‚¬/ê±°ë˜ ì¦ê°€)
    
    ### Query Parameters
    - `period_months`: ë¹„êµ ê¸°ê°„ (ê°œì›”, ê¸°ë³¸ê°’: 2, ìµœëŒ€: 6)
    """
)
async def get_quadrant(
    period_months: int = Query(2, ge=1, le=12, description="ë¹„êµ ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 12)"),
    db: AsyncSession = Depends(get_db)
):
    """
    4ë¶„ë©´ ë¶„ë¥˜ ì¡°íšŒ - ì„±ëŠ¥ ìµœì í™” ë²„ì „
    
    ì›”ë³„ ì§‘ê³„ë¡œ ê°„ì†Œí™”í•˜ì—¬ ë¹ ë¥¸ ì‘ë‹µ ì œê³µ
    """
    cache_key = build_cache_key("statistics", "quadrant_v2", str(period_months))
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        logger.info(f"âœ… [Statistics Quadrant] ìºì‹œì—ì„œ ë°˜í™˜")
        return cached_data
    
    try:
        logger.info(
            f"ğŸ” [Statistics Quadrant] 4ë¶„ë©´ ë¶„ë¥˜ ë°ì´í„° ì¡°íšŒ ì‹œì‘ - "
            f"period_months: {period_months}"
        )
        
        # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ê¸°ê°„ ì„¤ì •
        today = date.today()
        # í˜„ì¬ ë‹¬ì˜ ì²« ë‚  (í˜„ì¬ ë‹¬ ì œì™¸)
        current_month_start = date(today.year, today.month, 1)
        
        # ìµœê·¼ ê¸°ê°„ê³¼ ì´ì „ ê¸°ê°„ ì„¤ì • (í˜„ì¬ ë‹¬ ì œì™¸)
        recent_start = current_month_start - timedelta(days=period_months * 30)
        recent_end = current_month_start  # í˜„ì¬ ë‹¬ì˜ ì²« ë‚  ì „ê¹Œì§€
        
        previous_start = recent_start - timedelta(days=period_months * 30)
        previous_end = recent_start
        
        logger.info(
            f"ğŸ“… [Statistics Quadrant] ë‚ ì§œ ë²”ìœ„ - "
            f"previous_start: {previous_start}, previous_end: {previous_end}, "
            f"recent_start: {recent_start}, recent_end: {recent_end}"
        )
        
        # ì›”ë³„ ì§‘ê³„ (to_char ëŒ€ì‹  extract ì‚¬ìš© - ì¸ë±ìŠ¤ í™œìš© ê°€ëŠ¥)
        # ë§¤ë§¤ ê±°ë˜ëŸ‰: ì´ì „ ê¸°ê°„
        sale_previous_stmt = (
            select(
                extract('year', Sale.contract_date).label('year'),
                extract('month', Sale.contract_date).label('month'),
                func.count(Sale.trans_id).label('count')
            )
            .where(
                and_(
                    Sale.is_canceled == False,
                    (Sale.is_deleted == False) | (Sale.is_deleted.is_(None)),
                    Sale.contract_date.isnot(None),
                    Sale.contract_date >= previous_start,
                    Sale.contract_date < previous_end,
                    or_(Sale.remarks != "ë”ë¯¸", Sale.remarks.is_(None))
                )
            )
            .group_by(extract('year', Sale.contract_date), extract('month', Sale.contract_date))
        )
        
        # ë§¤ë§¤ ê±°ë˜ëŸ‰: ìµœê·¼ ê¸°ê°„
        sale_recent_stmt = (
            select(
                extract('year', Sale.contract_date).label('year'),
                extract('month', Sale.contract_date).label('month'),
                func.count(Sale.trans_id).label('count')
            )
            .where(
                and_(
                    Sale.is_canceled == False,
                    (Sale.is_deleted == False) | (Sale.is_deleted.is_(None)),
                    Sale.contract_date.isnot(None),
                    Sale.contract_date >= recent_start,
                    Sale.contract_date < recent_end,  # í˜„ì¬ ë‹¬ ì œì™¸ (ë¯¸ë§Œìœ¼ë¡œ ë³€ê²½)
                    or_(Sale.remarks != "ë”ë¯¸", Sale.remarks.is_(None))
                )
            )
            .group_by(extract('year', Sale.contract_date), extract('month', Sale.contract_date))
        )
        
        # ì „ì›”ì„¸ ê±°ë˜ëŸ‰: ì´ì „ ê¸°ê°„
        rent_previous_stmt = (
            select(
                extract('year', Rent.deal_date).label('year'),
                extract('month', Rent.deal_date).label('month'),
                func.count(Rent.trans_id).label('count')
            )
            .where(
                and_(
                    (Rent.is_deleted == False) | (Rent.is_deleted.is_(None)),
                    Rent.deal_date.isnot(None),
                    Rent.deal_date >= previous_start,
                    Rent.deal_date < previous_end,
                    or_(Rent.remarks != "ë”ë¯¸", Rent.remarks.is_(None))
                )
            )
            .group_by(extract('year', Rent.deal_date), extract('month', Rent.deal_date))
        )
        
        # ì „ì›”ì„¸ ê±°ë˜ëŸ‰: ìµœê·¼ ê¸°ê°„
        rent_recent_stmt = (
            select(
                extract('year', Rent.deal_date).label('year'),
                extract('month', Rent.deal_date).label('month'),
                func.count(Rent.trans_id).label('count')
            )
            .where(
                and_(
                    (Rent.is_deleted == False) | (Rent.is_deleted.is_(None)),
                    Rent.deal_date.isnot(None),
                    Rent.deal_date >= recent_start,
                    Rent.deal_date < recent_end,  # í˜„ì¬ ë‹¬ ì œì™¸ (ë¯¸ë§Œìœ¼ë¡œ ë³€ê²½)
                    or_(Rent.remarks != "ë”ë¯¸", Rent.remarks.is_(None))
                )
            )
            .group_by(extract('year', Rent.deal_date), extract('month', Rent.deal_date))
        )
        
        # ì¿¼ë¦¬ ë³‘ë ¬ ì‹¤í–‰ (ì„±ëŠ¥ ìµœì í™”)
        sale_previous_result, sale_recent_result, rent_previous_result, rent_recent_result = await asyncio.gather(
            db.execute(sale_previous_stmt),
            db.execute(sale_recent_stmt),
            db.execute(rent_previous_stmt),
            db.execute(rent_recent_stmt)
        )
        
        sale_previous_rows = sale_previous_result.fetchall()
        sale_recent_rows = sale_recent_result.fetchall()
        rent_previous_rows = rent_previous_result.fetchall()
        rent_recent_rows = rent_recent_result.fetchall()
        
        # ì´ì „ ê¸°ê°„ í‰ê·  ê³„ì‚°
        sale_previous_total = sum(row.count for row in sale_previous_rows) if sale_previous_rows else 0
        rent_previous_total = sum(row.count for row in rent_previous_rows) if rent_previous_rows else 0
        
        sale_previous_avg = sale_previous_total / len(sale_previous_rows) if sale_previous_rows else 1
        rent_previous_avg = rent_previous_total / len(rent_previous_rows) if rent_previous_rows else 1
        
        # ìµœê·¼ ê¸°ê°„ ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        sale_recent_dict = {f"{int(row.year)}-{int(row.month):02d}": row.count for row in sale_recent_rows}
        rent_recent_dict = {f"{int(row.year)}-{int(row.month):02d}": row.count for row in rent_recent_rows}
        
        # ëª¨ë“  ê¸°ê°„ ìˆ˜ì§‘ (í˜„ì¬ ë‹¬ ì œì™¸)
        all_periods = set(sale_recent_dict.keys()) | set(rent_recent_dict.keys())
        current_year = today.year
        current_month = today.month
        current_period_key = f"{current_year}-{current_month:02d}"
        
        # í˜„ì¬ ë‹¬ ì œì™¸
        all_periods.discard(current_period_key)
        
        quadrant_data = []
        quadrant_counts = {1: 0, 2: 0, 3: 0, 4: 0}
        
        for period in sorted(all_periods):
            sale_recent_count = sale_recent_dict.get(period, 0)
            rent_recent_count = rent_recent_dict.get(period, 0)
            
            # ë³€í™”ìœ¨ ê³„ì‚°
            sale_change_rate = ((sale_recent_count - sale_previous_avg) / sale_previous_avg * 100) if sale_previous_avg > 0 else 0
            rent_change_rate = ((rent_recent_count - rent_previous_avg) / rent_previous_avg * 100) if rent_previous_avg > 0 else 0
            
            # 4ë¶„ë©´ ë¶„ë¥˜
            quadrant_num, quadrant_label = calculate_quadrant(sale_change_rate, rent_change_rate)
            
            if quadrant_num > 0:
                quadrant_counts[quadrant_num] = quadrant_counts.get(quadrant_num, 0) + 1
            
            quadrant_data.append(
                QuadrantDataPoint(
                    date=period,
                    sale_volume_change_rate=round(sale_change_rate, 2),
                    rent_volume_change_rate=round(rent_change_rate, 2),
                    quadrant=quadrant_num,
                    quadrant_label=quadrant_label
                )
            )
        
        summary = {
            "total_periods": len(quadrant_data),
            "quadrant_distribution": quadrant_counts,
            "sale_previous_avg": round(sale_previous_avg, 2),
            "rent_previous_avg": round(rent_previous_avg, 2)
        }
        
        response_data = QuadrantResponse(
            success=True,
            data=quadrant_data,
            summary=summary
        )
        
        # ìºì‹œì— ì €ì¥ (TTL: 6ì‹œê°„)
        if len(quadrant_data) > 0:
            await set_to_cache(cache_key, response_data.dict(), ttl=STATISTICS_CACHE_TTL)
        
        logger.info(f"âœ… [Statistics Quadrant] 4ë¶„ë©´ ë¶„ë¥˜ ë°ì´í„° ìƒì„± ì™„ë£Œ - ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(quadrant_data)}")
        
        return response_data
        
    except Exception as e:
        logger.error(f"âŒ [Statistics Quadrant] 4ë¶„ë©´ ë¶„ë¥˜ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"4ë¶„ë©´ ë¶„ë¥˜ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/hpi",
    response_model=HPIResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="ì£¼íƒê°€ê²©ì§€ìˆ˜(HPI) ì¡°íšŒ",
    description="""
    ì£¼íƒê°€ê²©ì§€ìˆ˜(Housing Price Index, HPI)ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ### ì£¼íƒê°€ê²©ì§€ìˆ˜ë€?
    íŠ¹ì • ì‹œì ì˜ ì£¼íƒ ê°€ê²©ì„ ê¸°ì¤€(100)ìœ¼ë¡œ ì¡ê³ , ì´í›„ ê°€ê²©ì´ ì–¼ë§ˆë‚˜ ë³€í–ˆëŠ”ì§€ë¥¼ ìˆ˜ì¹˜í™”í•œ í†µê³„ ì§€í‘œì…ë‹ˆë‹¤.
    
    ### ì§€ìˆ˜ í•´ì„
    - **ì§€ìˆ˜ > 100**: ê¸°ì¤€ ì‹œì ë³´ë‹¤ ì§‘ê°’ì´ ì˜¬ëìŒ
    - **ì§€ìˆ˜ = 100**: ê¸°ì¤€ ì‹œì ê³¼ ë™ì¼
    - **ì§€ìˆ˜ < 100**: ê¸°ì¤€ ì‹œì ë³´ë‹¤ ì§‘ê°’ì´ ë‚´ë ¸ìŒ
    
    ### Query Parameters
    - `region_id`: ì§€ì—­ ID (ì„ íƒ, ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì „ì²´ ì§€ì—­ í‰ê· )
    - `index_type`: ì§€ìˆ˜ ìœ í˜• (APT: ì•„íŒŒíŠ¸, HOUSE: ë‹¨ë…ì£¼íƒ, ALL: ì „ì²´, ê¸°ë³¸ê°’: APT)
    - `months`: ì¡°íšŒ ê¸°ê°„ (ê°œì›”, ê¸°ë³¸ê°’: 24, ìµœëŒ€: 60)
    """
)
async def get_hpi(
    region_id: Optional[int] = Query(None, description="ì§€ì—­ ID (ì„ íƒ)"),
    index_type: str = Query("APT", description="ì§€ìˆ˜ ìœ í˜•: APT(ì•„íŒŒíŠ¸), HOUSE(ë‹¨ë…ì£¼íƒ), ALL(ì „ì²´)"),
    months: int = Query(24, ge=1, le=60, description="ì¡°íšŒ ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 60)"),
    db: AsyncSession = Depends(get_db)
):
    """
    ì£¼íƒê°€ê²©ì§€ìˆ˜(HPI) ì¡°íšŒ
    
    ì§€ì—­ë³„ ì£¼íƒê°€ê²©ì§€ìˆ˜ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    # ìœ íš¨í•œ index_type ê²€ì¦
    valid_index_types = ["APT", "HOUSE", "ALL"]
    if index_type not in valid_index_types:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ index_typeì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê°’: {', '.join(valid_index_types)}"
        )
    
    cache_key = build_cache_key(
        "statistics", "hpi", 
        str(region_id) if region_id else "all",
        index_type,
        str(months)
    )
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        logger.info(f"âœ… [Statistics HPI] ìºì‹œì—ì„œ ë°˜í™˜")
        return cached_data
    
    try:
        logger.info(
            f"ğŸ” [Statistics HPI] HPI ë°ì´í„° ì¡°íšŒ ì‹œì‘ - "
            f"region_id: {region_id}, index_type: {index_type}, months: {months}"
        )
        
        # ê¸°ì¤€ ë‚ ì§œ ê³„ì‚° (í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ìµœê·¼ monthsê°œì›”)
        today = date.today()
        # base_ymì€ YYYYMM í˜•ì‹ì´ë¯€ë¡œ, í˜„ì¬ ë…„ì›”ì„ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
        current_year = today.year
        current_month = today.month
        
        # ìµœì†Œ base_ym ê³„ì‚° (monthsê°œì›” ì „)
        # ì›” ë‹¨ìœ„ë¡œ ê³„ì‚°
        total_months = current_year * 12 + current_month - 1
        start_total_months = total_months - months + 1  # í˜„ì¬ ë‹¬ í¬í•¨
        start_year = start_total_months // 12
        start_month = (start_total_months % 12) + 1
        
        start_base_ym = f"{start_year:04d}{start_month:02d}"
        end_base_ym = f"{current_year:04d}{current_month:02d}"
        
        logger.info(
            f"ğŸ“… [Statistics HPI] ë‚ ì§œ ë²”ìœ„ - "
            f"start_base_ym: {start_base_ym}, end_base_ym: {end_base_ym}"
        )
        
        # ì¿¼ë¦¬ êµ¬ì„±
        # region_idê°€ ì§€ì •ëœ ê²½ìš°: íŠ¹ì • ì§€ì—­ë§Œ ì¡°íšŒ
        if region_id is not None:
            query = (
                select(
                    HouseScore.base_ym,
                    HouseScore.index_value,
                    HouseScore.index_change_rate,
                    HouseScore.index_type,
                    State.city_name.label('region_name')  # ì‹œë„ëª… ì‚¬ìš©
                )
                .join(State, HouseScore.region_id == State.region_id)
                .where(
                    and_(
                        HouseScore.region_id == region_id,
                        HouseScore.is_deleted == False,
                        HouseScore.index_type == index_type,
                        HouseScore.base_ym >= start_base_ym,
                        HouseScore.base_ym <= end_base_ym
                    )
                )
                .order_by(HouseScore.base_ym)
            )
        else:
            # region_idê°€ ì—†ëŠ” ê²½ìš°: ì‹œë„(city_name) ë ˆë²¨ë¡œ ê·¸ë£¹í™” (ì¸êµ¬ ì´ë™ ë°ì´í„°ì™€ ë™ì¼í•œ ë ˆë²¨)
            query = (
                select(
                    HouseScore.base_ym,
                    func.avg(HouseScore.index_value).label('index_value'),
                    func.avg(HouseScore.index_change_rate).label('index_change_rate'),
                    func.max(HouseScore.index_type).label('index_type'),
                    State.city_name.label('region_name')  # ì‹œë„ëª…ìœ¼ë¡œ ê·¸ë£¹í™”
                )
                .join(State, HouseScore.region_id == State.region_id)
                .where(
                    and_(
                        HouseScore.is_deleted == False,
                        State.is_deleted == False,
                        HouseScore.index_type == index_type,
                        HouseScore.base_ym >= start_base_ym,
                        HouseScore.base_ym <= end_base_ym
                    )
                )
                .group_by(HouseScore.base_ym, State.city_name)
                .order_by(HouseScore.base_ym, State.city_name)
            )
        
        result = await db.execute(query)
        rows = result.fetchall()
        
        logger.info(
            f"ğŸ“Š [Statistics HPI] ì¿¼ë¦¬ ê²°ê³¼ - "
            f"ì´ {len(rows)}ê±´ ì¡°íšŒë¨"
        )
        
        # ì‹œë„ë³„ ë°ì´í„° ê°œìˆ˜ í™•ì¸
        if rows:
            region_counts = {}
            for row in rows:
                region_name = row.region_name if hasattr(row, 'region_name') and row.region_name else "Unknown"
                region_counts[region_name] = region_counts.get(region_name, 0) + 1
            
            logger.info(
                f"ğŸ“‹ [Statistics HPI] ì‹œë„ë³„ ë°ì´í„° ê°œìˆ˜ - "
                f"{', '.join([f'{k}: {v}ê±´' for k, v in sorted(region_counts.items())])}"
            )
        
        # ë°ì´í„° í¬ì¸íŠ¸ ìƒì„±
        hpi_data = []
        for row in rows:
            base_ym = row.base_ym
            # YYYYMM -> YYYY-MM í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            year = base_ym[:4]
            month = base_ym[4:6]
            date_str = f"{year}-{month}"
            
            index_value = float(row.index_value) if row.index_value is not None else 0.0
            index_change_rate = float(row.index_change_rate) if row.index_change_rate is not None else None
            
            # region_name ì²˜ë¦¬: ì‹œë„ëª…(city_name) ì‚¬ìš©
            region_name = row.region_name if hasattr(row, 'region_name') and row.region_name else None
            
            hpi_data.append(
                HPIDataPoint(
                    date=date_str,
                    index_value=round(index_value, 2),
                    index_change_rate=round(index_change_rate, 2) if index_change_rate is not None else None,
                    region_name=region_name,
                    index_type=index_type
                )
            )
        
        # ë‚ ì§œìˆœ ì •ë ¬ (ì´ë¯¸ ì •ë ¬ë˜ì–´ ìˆì§€ë§Œ í™•ì‹¤íˆ)
        hpi_data.sort(key=lambda x: x.date)
        
        # ì§€ì—­ë³„/ë‚ ì§œë³„ ë°ì´í„° ê°œìˆ˜ í™•ì¸
        if hpi_data:
            date_counts = {}
            region_date_counts = {}
            for item in hpi_data:
                date_counts[item.date] = date_counts.get(item.date, 0) + 1
                if item.region_name:
                    key = f"{item.region_name}-{item.date}"
                    region_date_counts[key] = region_date_counts.get(key, 0) + 1
            
            logger.info(
                f"ğŸ“ˆ [Statistics HPI] ë°ì´í„° í¬ì¸íŠ¸ ìƒì„¸ - "
                f"ì´ {len(hpi_data)}ê±´, "
                f"ë‚ ì§œë³„ ê°œìˆ˜: {dict(sorted(date_counts.items())[:5])}... (ìµœì‹  5ê°œë§Œ í‘œì‹œ), "
                f"ì‹œë„ ìˆ˜: {len(set(item.region_name for item in hpi_data if item.region_name))}ê°œ"
            )
            
            # ê° ì‹œë„ë³„ ìµœì‹  ë°ì´í„° ìƒ˜í”Œ ë¡œê¹…
            latest_by_region = {}
            for item in reversed(hpi_data):  # ìµœì‹ ë¶€í„°
                if item.region_name and item.region_name not in latest_by_region:
                    latest_by_region[item.region_name] = item
            
            if latest_by_region:
                sample_regions = list(latest_by_region.items())[:5]  # ìµœëŒ€ 5ê°œë§Œ
                logger.info(
                    f"ğŸ“ [Statistics HPI] ì‹œë„ë³„ ìµœì‹  ë°ì´í„° ìƒ˜í”Œ - "
                    f"{', '.join([f'{r}: {d.date} {d.index_value}' for r, d in sample_regions])}"
                )
        
        region_desc = f"ì§€ì—­ ID {region_id}" if region_id else "ì „ì²´ ì§€ì—­ í‰ê· "
        period_desc = f"{months}ê°œì›” ({hpi_data[0].date if hpi_data else 'N/A'} ~ {hpi_data[-1].date if hpi_data else 'N/A'})"
        
        response_data = HPIResponse(
            success=True,
            data=hpi_data,
            region_id=region_id,
            index_type=index_type,
            period=f"{region_desc}, {index_type}, {period_desc}"
        )
        
        # ìºì‹œì— ì €ì¥ (TTL: 6ì‹œê°„)
        if len(hpi_data) > 0:
            await set_to_cache(cache_key, response_data.dict(), ttl=STATISTICS_CACHE_TTL)
        
        logger.info(f"âœ… [Statistics HPI] HPI ë°ì´í„° ìƒì„± ì™„ë£Œ - ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(hpi_data)}")
        
        return response_data
        
    except Exception as e:
        logger.error(f"âŒ [Statistics HPI] HPI ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"HPI ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/hpi/heatmap",
    response_model=HPIHeatmapResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="ì£¼íƒê°€ê²©ì§€ìˆ˜(HPI) íˆíŠ¸ë§µ ì¡°íšŒ",
    description="""
    ê´‘ì—­ì‹œ/íŠ¹ë³„ì‹œ/ë„ë³„ ì£¼íƒê°€ê²©ì§€ìˆ˜ë¥¼ íˆíŠ¸ë§µ í˜•ì‹ìœ¼ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ê° ë„/ì‹œì˜ ìµœì‹  HPI ê°’ì„ ë°˜í™˜í•˜ì—¬ ì§€ì—­ë³„ ê°€ê²© ì¶”ì´ë¥¼ í•œëˆˆì— ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    ### Query Parameters
    - `index_type`: ì§€ìˆ˜ ìœ í˜• (APT: ì•„íŒŒíŠ¸, HOUSE: ë‹¨ë…ì£¼íƒ, ALL: ì „ì²´, ê¸°ë³¸ê°’: APT)
    """
)
async def get_hpi_heatmap(
    index_type: str = Query("APT", description="ì§€ìˆ˜ ìœ í˜•: APT(ì•„íŒŒíŠ¸), HOUSE(ë‹¨ë…ì£¼íƒ), ALL(ì „ì²´)"),
    db: AsyncSession = Depends(get_db)
):
    """
    ì£¼íƒê°€ê²©ì§€ìˆ˜(HPI) íˆíŠ¸ë§µ ì¡°íšŒ
    
    ë„/ì‹œë³„ ìµœì‹  HPI ê°’ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    # ìœ íš¨í•œ index_type ê²€ì¦
    valid_index_types = ["APT", "HOUSE", "ALL"]
    if index_type not in valid_index_types:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ index_typeì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê°’: {', '.join(valid_index_types)}"
        )
    
    cache_key = build_cache_key("statistics", "hpi_heatmap", index_type)
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        logger.info(f"âœ… [Statistics HPI Heatmap] ìºì‹œì—ì„œ ë°˜í™˜")
        return cached_data
    
    try:
        logger.info(
            f"ğŸ” [Statistics HPI Heatmap] HPI íˆíŠ¸ë§µ ë°ì´í„° ì¡°íšŒ ì‹œì‘ - "
            f"index_type: {index_type}"
        )
        
        # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ìµœì‹  base_ym ì°¾ê¸°
        today = date.today()
        current_year = today.year
        current_month = today.month
        current_base_ym = f"{current_year:04d}{current_month:02d}"
        
        # ìµœì‹  base_ymë¶€í„° ì—­ìˆœìœ¼ë¡œ ì°¾ê¸° (ìµœëŒ€ 12ê°œì›” ì „ê¹Œì§€)
        found_base_ym = None
        for i in range(12):
            check_year = current_year
            check_month = current_month - i
            if check_month <= 0:
                check_year -= 1
                check_month += 12
            check_base_ym = f"{check_year:04d}{check_month:02d}"
            
            # í•´ë‹¹ base_ymì— ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
            check_query = (
                select(func.count(HouseScore.index_id))
                .where(
                    and_(
                        HouseScore.is_deleted == False,
                        HouseScore.index_type == index_type,
                        HouseScore.base_ym == check_base_ym
                    )
                )
            )
            check_result = await db.execute(check_query)
            count = check_result.scalar() or 0
            
            if count > 0:
                found_base_ym = check_base_ym
                break
        
        if not found_base_ym:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="HPI ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        logger.info(f"ğŸ“… [Statistics HPI Heatmap] ì‚¬ìš©í•  base_ym: {found_base_ym}")
        
        # ë„/ì‹œë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í‰ê·  HPI ê³„ì‚°
        query = (
            select(
                State.city_name,
                func.avg(HouseScore.index_value).label('index_value'),
                func.avg(HouseScore.index_change_rate).label('index_change_rate'),
                func.count(HouseScore.index_id).label('region_count')
            )
            .join(State, HouseScore.region_id == State.region_id)
            .where(
                and_(
                    HouseScore.is_deleted == False,
                    State.is_deleted == False,
                    HouseScore.index_type == index_type,
                    HouseScore.base_ym == found_base_ym
                )
            )
            .group_by(State.city_name)
            .order_by(State.city_name)
        )
        
        result = await db.execute(query)
        rows = result.fetchall()
        
        # ë°ì´í„° í¬ì¸íŠ¸ ìƒì„±
        heatmap_data = []
        for row in rows:
            city_name = row.city_name
            index_value = float(row.index_value) if row.index_value is not None else 0.0
            index_change_rate = float(row.index_change_rate) if row.index_change_rate is not None else None
            region_count = int(row.region_count) if row.region_count else 0
            
            heatmap_data.append(
                HPIHeatmapDataPoint(
                    city_name=city_name,
                    index_value=round(index_value, 2),
                    index_change_rate=round(index_change_rate, 2) if index_change_rate is not None else None,
                    base_ym=found_base_ym,
                    region_count=region_count
                )
            )
        
        # ë„/ì‹œëª… ìˆœìœ¼ë¡œ ì •ë ¬
        heatmap_data.sort(key=lambda x: x.city_name)
        
        response_data = HPIHeatmapResponse(
            success=True,
            data=heatmap_data,
            index_type=index_type,
            base_ym=found_base_ym
        )
        
        # ìºì‹œì— ì €ì¥ (TTL: 6ì‹œê°„)
        if len(heatmap_data) > 0:
            await set_to_cache(cache_key, response_data.dict(), ttl=STATISTICS_CACHE_TTL)
        
        logger.info(f"âœ… [Statistics HPI Heatmap] HPI íˆíŠ¸ë§µ ë°ì´í„° ìƒì„± ì™„ë£Œ - ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(heatmap_data)}")
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [Statistics HPI Heatmap] HPI íˆíŠ¸ë§µ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"HPI íˆíŠ¸ë§µ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/summary",
    response_model=StatisticsSummaryResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="í†µê³„ ìš”ì•½ ì¡°íšŒ",
    description="""
    RVOLê³¼ 4ë¶„ë©´ ë¶„ë¥˜ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¡°íšŒí•©ë‹ˆë‹¤.
    """
)
async def get_statistics_summary(
    transaction_type: str = Query("sale", description="ê±°ë˜ ìœ í˜•: sale(ë§¤ë§¤), rent(ì „ì›”ì„¸)"),
    current_period_months: int = Query(6, ge=1, le=12, description="í˜„ì¬ ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 12)"),
    average_period_months: int = Query(6, ge=1, le=12, description="í‰ê·  ê³„ì‚° ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 12)"),
    quadrant_period_months: int = Query(2, ge=1, le=12, description="4ë¶„ë©´ ë¹„êµ ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 12)"),
    db: AsyncSession = Depends(get_db)
):
    """
    í†µê³„ ìš”ì•½ ì¡°íšŒ
    
    RVOLê³¼ 4ë¶„ë©´ ë¶„ë¥˜ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    # RVOLê³¼ 4ë¶„ë©´ ë¶„ë¥˜ë¥¼ ë³‘ë ¬ë¡œ ì¡°íšŒ
    rvol_task = get_rvol(transaction_type, current_period_months, average_period_months, db)
    quadrant_task = get_quadrant(quadrant_period_months, db)
    
    rvol_response, quadrant_response = await asyncio.gather(rvol_task, quadrant_task)
    
    return StatisticsSummaryResponse(
        success=True,
        rvol=rvol_response,
        quadrant=quadrant_response
    )


@router.get(
    "/population-movements",
    response_model=PopulationMovementResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="ì¸êµ¬ ì´ë™ ë°ì´í„° ì¡°íšŒ",
    description="""
    ì§€ì—­ë³„ ì¸êµ¬ ì´ë™ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ### Query Parameters
    - `region_id`: ì§€ì—­ ID (ì„ íƒ, ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì „ì²´)
    - `start_ym`: ì‹œì‘ ë…„ì›” (YYYYMM, ê¸°ë³¸ê°’: ìµœê·¼ 12ê°œì›”)
    - `end_ym`: ì¢…ë£Œ ë…„ì›” (YYYYMM, ê¸°ë³¸ê°’: í˜„ì¬)
    """
)
async def get_population_movements(
    region_id: Optional[int] = Query(None, description="ì§€ì—­ ID (ì„ íƒ)"),
    start_ym: Optional[str] = Query(None, description="ì‹œì‘ ë…„ì›” (YYYYMM)"),
    end_ym: Optional[str] = Query(None, description="ì¢…ë£Œ ë…„ì›” (YYYYMM)"),
    db: AsyncSession = Depends(get_db)
):
    """
    ì¸êµ¬ ì´ë™ ë°ì´í„° ì¡°íšŒ
    """
    try:
        # ê¸°ë³¸ ê¸°ê°„ ì„¤ì • (ìµœê·¼ 12ê°œì›”)
        if not end_ym:
            end_date = datetime.now()
            end_ym = end_date.strftime("%Y%m")
        
        if not start_ym:
            start_date = datetime.now() - timedelta(days=365)
            start_ym = start_date.strftime("%Y%m")
        
        # ì¿¼ë¦¬ êµ¬ì„±: ì‹œë„ ë ˆë²¨ ë°ì´í„°ë§Œ ì¡°íšŒ (city_name ì‚¬ìš©)
        query = select(
            PopulationMovement,
            State.city_name  # ì‹œë„ëª… ì‚¬ìš© (ì˜ˆ: ì„œìš¸íŠ¹ë³„ì‹œ, ë¶€ì‚°ê´‘ì—­ì‹œ)
        ).join(
            State, PopulationMovement.region_id == State.region_id
        ).where(
            and_(
                PopulationMovement.base_ym >= start_ym,
                PopulationMovement.base_ym <= end_ym,
                PopulationMovement.is_deleted == False
            )
        )
        
        if region_id:
            query = query.where(PopulationMovement.region_id == region_id)
        
        query = query.order_by(PopulationMovement.base_ym.desc())
        
        result = await db.execute(query)
        rows = result.all()
        
        logger.info(
            f"ğŸ“Š [Statistics Population Movement] ì¸êµ¬ ì´ë™ ë°ì´í„° ì¡°íšŒ - "
            f"ì´ {len(rows)}ê±´ ì¡°íšŒë¨"
        )
        
        # ì§€ì—­ë³„ ë°ì´í„° ê°œìˆ˜ í™•ì¸
        if rows:
            region_counts = {}
            region_net_totals = {}  # ì§€ì—­ë³„ ìˆœì´ë™ í•©ê³„
            for movement, city_name in rows:
                region_name = city_name or "Unknown"
                region_counts[region_name] = region_counts.get(region_name, 0) + 1
                # ìˆœì´ë™ í•©ê³„ ê³„ì‚°
                if region_name not in region_net_totals:
                    region_net_totals[region_name] = 0
                region_net_totals[region_name] += movement.net_migration or 0
            
            logger.info(
                f"ğŸ“‹ [Statistics Population Movement] ì‹œë„ë³„ ë°ì´í„° ê°œìˆ˜ - "
                f"{', '.join([f'{k}: {v}ê±´' for k, v in sorted(region_counts.items())])}"
            )
            
            logger.info(
                f"ğŸ“Š [Statistics Population Movement] ì‹œë„ë³„ ìˆœì´ë™ í•©ê³„ - "
                f"{', '.join([f'{k}: {v}ëª…' for k, v in sorted(region_net_totals.items())])}"
            )
        
        data_points = []
        for movement, city_name in rows:
            # YYYYMM -> YYYY-MM ë³€í™˜
            year = movement.base_ym[:4]
            month = movement.base_ym[4:]
            date_str = f"{year}-{month}"
            
            data_points.append(PopulationMovementDataPoint(
                date=date_str,
                region_id=movement.region_id,
                region_name=city_name,  # ì‹œë„ëª… ë°˜í™˜
                in_migration=movement.in_migration,
                out_migration=movement.out_migration,
                net_migration=movement.net_migration
            ))
        
        return PopulationMovementResponse(
            success=True,
            data=data_points,
            period=f"{start_ym} ~ {end_ym}"
        )
        
    except Exception as e:
        logger.error(f"âŒ ì¸êµ¬ ì´ë™ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì¸êµ¬ ì´ë™ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        )


# ============================================================
# ì£¼íƒ ìˆ˜ìš” í˜ì´ì§€ìš© ìƒˆë¡œìš´ API
# ============================================================

@router.get(
    "/hpi/by-region-type",
    response_model=HPIRegionTypeResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="ì§€ì—­ ìœ í˜•ë³„ ì£¼íƒ ê°€ê²© ì§€ìˆ˜ ì¡°íšŒ",
    description="""
    ì§€ì—­ ìœ í˜•ë³„ë¡œ ì£¼íƒ ê°€ê²© ì§€ìˆ˜ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ### Query Parameters
    - `region_type`: ì§€ì—­ ìœ í˜• (required)
    - `index_type`: ì§€ìˆ˜ ìœ í˜• (optional, ê¸°ë³¸ê°’: "APT")
    - `base_ym`: ê¸°ì¤€ ë…„ì›” (optional, ê¸°ë³¸ê°’: ìµœì‹ )
    """
)
async def get_hpi_by_region_type(
    region_type: str = Query(..., description="ì§€ì—­ ìœ í˜•: ì „êµ­, ìˆ˜ë„ê¶Œ, ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"),
    index_type: str = Query("APT", description="ì§€ìˆ˜ ìœ í˜•: APT(ì•„íŒŒíŠ¸), HOUSE(ë‹¨ë…ì£¼íƒ), ALL(ì „ì²´)"),
    base_ym: Optional[str] = Query(None, description="ê¸°ì¤€ ë…„ì›” (YYYYMM, ê¸°ë³¸ê°’: ìµœì‹ )"),
    db: AsyncSession = Depends(get_db)
):
    """
    ì§€ì—­ ìœ í˜•ë³„ ì£¼íƒ ê°€ê²© ì§€ìˆ˜ ì¡°íšŒ
    """
    # ìœ íš¨ì„± ê²€ì¦
    valid_region_types = ["ì „êµ­", "ìˆ˜ë„ê¶Œ", "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"]
    if region_type not in valid_region_types:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ region_typeì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê°’: {', '.join(valid_region_types)}"
        )
    
    valid_index_types = ["APT", "HOUSE", "ALL"]
    if index_type not in valid_index_types:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ index_typeì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê°’: {', '.join(valid_index_types)}"
        )
    
    cache_key = build_cache_key(
        "statistics", "hpi-by-region-type", region_type, index_type,
        base_ym if base_ym else "latest"
    )
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        logger.info(f"âœ… [Statistics HPI Region Type] ìºì‹œì—ì„œ ë°˜í™˜")
        return cached_data
    
    try:
        logger.info(
            f"ğŸ” [Statistics HPI Region Type] HPI ë°ì´í„° ì¡°íšŒ ì‹œì‘ - "
            f"region_type: {region_type}, index_type: {index_type}, base_ym: {base_ym}"
        )
        
        # base_ymì´ ì—†ìœ¼ë©´ ìµœì‹  ë°ì´í„° ì°¾ê¸°
        if not base_ym:
            today = date.today()
            current_year = today.year
            current_month = today.month
            
            # ìµœì‹  base_ym ì°¾ê¸° (ìµœëŒ€ 12ê°œì›” ì „ê¹Œì§€)
            found_base_ym = None
            for i in range(12):
                check_year = current_year
                check_month = current_month - i
                if check_month <= 0:
                    check_year -= 1
                    check_month += 12
                check_base_ym = f"{check_year:04d}{check_month:02d}"
                
                # í•´ë‹¹ base_ymì— ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
                check_query = (
                    select(func.count(HouseScore.index_id))
                    .where(
                        and_(
                            HouseScore.is_deleted == False,
                            HouseScore.index_type == index_type,
                            HouseScore.base_ym == check_base_ym
                        )
                    )
                )
                check_result = await db.execute(check_query)
                count = check_result.scalar() or 0
                
                if count > 0:
                    found_base_ym = check_base_ym
                    break
            
            if not found_base_ym:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail="HPI ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            
            base_ym = found_base_ym
        
        # ì§€ì—­ í•„í„° ì¡°ê±´
        region_filter = get_region_type_filter(region_type)
        
        # ìˆ˜ë„ê¶Œì˜ ê²½ìš° ì‹œ/êµ° ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”, ê·¸ ì™¸ëŠ” ì‹œë„ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”
        if region_type == "ìˆ˜ë„ê¶Œ":
            # ìˆ˜ë„ê¶Œ: ì‹œ/êµ° ë‹¨ìœ„ë¡œ ê·¸ë£¹í™” (ì„œìš¸íŠ¹ë³„ì‹œëŠ” "ì„œìš¸", ì¸ì²œê´‘ì—­ì‹œëŠ” "ì¸ì²œ", ê²½ê¸°ë„ëŠ” ì‹œ/êµ°ëª…)
            query = (
                select(
                    State.city_name,
                    State.region_name,
                    func.avg(HouseScore.index_value).label('index_value'),
                    func.avg(HouseScore.index_change_rate).label('index_change_rate'),
                    func.count(HouseScore.index_id).label('region_count')
                )
                .join(State, HouseScore.region_id == State.region_id)
                .where(
                    and_(
                        HouseScore.is_deleted == False,
                        State.is_deleted == False,
                        HouseScore.index_type == index_type,
                        HouseScore.base_ym == base_ym
                    )
                )
                .group_by(State.city_name, State.region_name)
            )
            
            # ì§€ì—­ í•„í„° ì ìš©
            if region_filter is not None:
                query = query.where(region_filter)
            
            query = query.order_by(State.city_name, State.region_name)
            
            result = await db.execute(query)
            rows = result.fetchall()
            
            # ì‘ë‹µ ë°ì´í„° ìƒì„±: ì‹œ/êµ° ë‹¨ìœ„
            # êµ¬ ë‹¨ìœ„ ë°ì´í„°ë¥¼ ì‹œ/êµ° ë‹¨ìœ„ë¡œ ì§‘ê³„
            region_data_map: Dict[str, Dict[str, Any]] = {}
            fallback_data_map: Dict[str, Dict[str, Any]] = {}  # ì˜ˆì™¸ì²˜ë¦¬ìš©: ë¦¬, í¬, ê¸°í¥ ë°ì´í„° ì €ì¥
            
            for row in rows:
                city_name = row.city_name
                region_name = row.region_name or ""
                
                # ì›ë³¸ region_name í™•ì¸ (ì˜ˆì™¸ì²˜ë¦¬ìš©)
                original_normalized = region_name.replace("ì‹œ", "").replace("êµ°", "").replace("êµ¬", "").strip()
                
                # êµ¬ ë‹¨ìœ„ë¥¼ ì‹œ/êµ° ë‹¨ìœ„ë¡œ ì •ê·œí™” (ì˜ˆì™¸ì²˜ë¦¬ ì œì™¸)
                normalized_name = normalize_metropolitan_region_name_without_fallback(city_name, region_name)
                
                # ë¶ˆì™„ì „í•œ ì´ë¦„ í•„í„°ë§ (1ê¸€ì ë˜ëŠ” ì´ìƒí•œ ë°ì´í„°)
                if len(normalized_name) <= 1 or normalized_name == "í¥":
                    continue
                
                # "ê²½ê¸°ë„" ê°™ì€ ë„ ë‹¨ìœ„ ë°ì´í„° ì œì™¸
                if normalized_name == "ê²½ê¸°ë„" or normalized_name == "ê²½ê¸°":
                    continue
                
                index_value = float(row.index_value or 0)
                index_change_rate = float(row.index_change_rate) if row.index_change_rate is not None else None
                region_count = row.region_count or 0
                
                # ì˜ˆì™¸ì²˜ë¦¬ìš© ë°ì´í„° ì €ì¥ (ë¦¬, í¬, ê¸°í¥)
                if original_normalized == "ë¦¬":
                    if "ë¦¬" not in fallback_data_map:
                        fallback_data_map["ë¦¬"] = {
                            "total_value": index_value * region_count,
                            "total_count": region_count,
                            "index_change_rate": index_change_rate
                        }
                    else:
                        fallback_data_map["ë¦¬"]["total_value"] += index_value * region_count
                        fallback_data_map["ë¦¬"]["total_count"] += region_count
                    continue
                elif original_normalized == "í¬":
                    if "í¬" not in fallback_data_map:
                        fallback_data_map["í¬"] = {
                            "total_value": index_value * region_count,
                            "total_count": region_count,
                            "index_change_rate": index_change_rate
                        }
                    else:
                        fallback_data_map["í¬"]["total_value"] += index_value * region_count
                        fallback_data_map["í¬"]["total_count"] += region_count
                    continue
                elif original_normalized == "ê¸°í¥":
                    if "ê¸°í¥" not in fallback_data_map:
                        fallback_data_map["ê¸°í¥"] = {
                            "total_value": index_value * region_count,
                            "total_count": region_count,
                            "index_change_rate": index_change_rate
                        }
                    else:
                        fallback_data_map["ê¸°í¥"]["total_value"] += index_value * region_count
                        fallback_data_map["ê¸°í¥"]["total_count"] += region_count
                    continue
                
                # ê°™ì€ ì‹œ/êµ°ì˜ ë°ì´í„°ë¥¼ ì§‘ê³„ (í‰ê· )
                if normalized_name not in region_data_map:
                    region_data_map[normalized_name] = {
                        "total_value": index_value * region_count,
                        "total_count": region_count,
                        "index_change_rate": index_change_rate
                    }
                else:
                    region_data_map[normalized_name]["total_value"] += index_value * region_count
                    region_data_map[normalized_name]["total_count"] += region_count
                    # index_change_rateëŠ” ì²« ë²ˆì§¸ ê°’ ì‚¬ìš© (ë˜ëŠ” í‰ê·  ê³„ì‚° ê°€ëŠ¥)
            
            # ì˜ˆì™¸ì²˜ë¦¬: êµ¬ë¦¬, êµ°í¬, ì‹œí¥ì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ ë¦¬, í¬, ê¸°í¥ ë°ì´í„° ì‚¬ìš©
            if "êµ¬ë¦¬" not in region_data_map and "ë¦¬" in fallback_data_map:
                region_data_map["êµ¬ë¦¬"] = fallback_data_map["ë¦¬"]
            if "êµ°í¬" not in region_data_map and "í¬" in fallback_data_map:
                region_data_map["êµ°í¬"] = fallback_data_map["í¬"]
            if "ì‹œí¥" not in region_data_map and "ê¸°í¥" in fallback_data_map:
                region_data_map["ì‹œí¥"] = fallback_data_map["ê¸°í¥"]
            
            # ì§‘ê³„ëœ ë°ì´í„°ë¥¼ ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            # í—ˆìš©ëœ ìˆ˜ë„ê¶Œ ì§€ì—­ ëª©ë¡
            allowed_metropolitan_regions = {
                "ì—°ì²œ", "í¬ì²œ", "íŒŒì£¼", "ì–‘ì£¼", "ë™ë‘ì²œ", "ê°€í‰", "ê³ ì–‘", "ì˜ì •ë¶€", 
                "ë‚¨ì–‘ì£¼", "ì–‘í‰", "ê¹€í¬", "ì„œìš¸", "êµ¬ë¦¬", "í•˜ë‚¨", "ì¸ì²œ", "ë¶€ì²œ", 
                "ê´‘ëª…", "ê³¼ì²œ", "ê´‘ì£¼", "ì‹œí¥", "ì•ˆì–‘", "ì„±ë‚¨", "ì´ì²œ", "ì—¬ì£¼", 
                "ì•ˆì‚°", "êµ°í¬", "ì˜ì™•", "ìš©ì¸", "í™”ì„±", "ìˆ˜ì›", "ì•ˆì„±", "ì˜¤ì‚°", "í‰íƒ"
            }
            
            hpi_data = []
            for normalized_name, data in region_data_map.items():
                # í—ˆìš©ëœ ì§€ì—­ë§Œ í¬í•¨
                if normalized_name not in allowed_metropolitan_regions:
                    continue
                
                avg_value = data["total_value"] / data["total_count"] if data["total_count"] > 0 else 0
                
                hpi_data.append(HPIRegionTypeDataPoint(
                    id=None,
                    name=normalized_name,
                    value=round(avg_value, 2),
                    index_change_rate=round(data["index_change_rate"], 2) if data["index_change_rate"] is not None else None
                ))
        else:
            # ì „êµ­, ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ: ì‹œë„ ë ˆë²¨ë¡œ ê·¸ë£¹í™”
            query = (
                select(
                    State.city_name,
                    func.avg(HouseScore.index_value).label('index_value'),
                    func.avg(HouseScore.index_change_rate).label('index_change_rate'),
                    func.count(HouseScore.index_id).label('region_count')
                )
                .join(State, HouseScore.region_id == State.region_id)
                .where(
                    and_(
                        HouseScore.is_deleted == False,
                        State.is_deleted == False,
                        HouseScore.index_type == index_type,
                        HouseScore.base_ym == base_ym
                    )
                )
                .group_by(State.city_name)
            )
            
            # ì§€ì—­ í•„í„° ì ìš©
            if region_filter is not None:
                query = query.where(region_filter)
            
            query = query.order_by(State.city_name)
            
            result = await db.execute(query)
            rows = result.fetchall()
            
            # ì‘ë‹µ ë°ì´í„° ìƒì„±: ì‹œë„ ë‹¨ìœ„
            hpi_data = []
            for row in rows:
                city_name = row.city_name
                normalized_name = normalize_city_name(city_name)
                index_value = float(row.index_value or 0)
                index_change_rate = float(row.index_change_rate) if row.index_change_rate is not None else None
                
                hpi_data.append(HPIRegionTypeDataPoint(
                    id=None,
                    name=normalized_name,
                    value=round(index_value, 2),
                    index_change_rate=round(index_change_rate, 2) if index_change_rate is not None else None
                ))
        
        response_data = HPIRegionTypeResponse(
            success=True,
            data=hpi_data,
            region_type=region_type,
            index_type=index_type,
            base_ym=base_ym
        )
        
        # ìºì‹œì— ì €ì¥
        if len(hpi_data) > 0:
            await set_to_cache(cache_key, response_data.dict(), ttl=STATISTICS_CACHE_TTL)
        
        logger.info(f"âœ… [Statistics HPI Region Type] HPI ë°ì´í„° ìƒì„± ì™„ë£Œ - ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(hpi_data)}")
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [Statistics HPI Region Type] HPI ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"HPI ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/transaction-volume",
    response_model=TransactionVolumeResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="ê±°ë˜ëŸ‰ ì¡°íšŒ (ì›”ë³„ ë°ì´í„°)",
    description="""
    ì „êµ­, ìˆ˜ë„ê¶Œ, ì§€ë°©5ëŒ€ê´‘ì—­ì‹œì˜ ì›”ë³„ ê±°ë˜ëŸ‰ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ### ì§€ì—­ ìœ í˜•
    - **ì „êµ­**: ì „ì²´ ì§€ì—­
    - **ìˆ˜ë„ê¶Œ**: ì„œìš¸íŠ¹ë³„ì‹œ, ê²½ê¸°ë„, ì¸ì²œê´‘ì—­ì‹œ
    - **ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ**: ë¶€ì‚°ê´‘ì—­ì‹œ, ëŒ€êµ¬ê´‘ì—­ì‹œ, ê´‘ì£¼ê´‘ì—­ì‹œ, ëŒ€ì „ê´‘ì—­ì‹œ, ìš¸ì‚°ê´‘ì—­ì‹œ
    
    ### ë°ì´í„° í˜•ì‹
    - ì›”ë³„ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤ (ì—°ë„, ì›”, ê±°ë˜ëŸ‰)
    - ì§€ë°©5ëŒ€ê´‘ì—­ì‹œì˜ ê²½ìš° `city_name` í•„ë“œê°€ í¬í•¨ë©ë‹ˆë‹¤
    - í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì—°ë„ë³„ ì§‘ê³„ ë˜ëŠ” ì›”ë³„ ë·°ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤
    
    ### Query Parameters
    - `region_type`: ì§€ì—­ ìœ í˜• (í•„ìˆ˜) - "ì „êµ­", "ìˆ˜ë„ê¶Œ", "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"
    - `transaction_type`: ê±°ë˜ ìœ í˜• (ì„ íƒ) - "sale"(ë§¤ë§¤), "rent"(ì „ì›”ì„¸), ê¸°ë³¸ê°’: "sale"
    - `max_years`: ìµœëŒ€ ì—°ë„ ìˆ˜ (ì„ íƒ) - 1~10, ê¸°ë³¸ê°’: 10
    """
)
async def get_transaction_volume(
    region_type: str = Query(..., description="ì§€ì—­ ìœ í˜•: ì „êµ­, ìˆ˜ë„ê¶Œ, ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"),
    transaction_type: str = Query("sale", description="ê±°ë˜ ìœ í˜•: sale(ë§¤ë§¤), rent(ì „ì›”ì„¸)"),
    max_years: int = Query(10, ge=1, le=10, description="ìµœëŒ€ ì—°ë„ ìˆ˜ (1~10)"),
    db: AsyncSession = Depends(get_db)
):
    """
    ê±°ë˜ëŸ‰ ì¡°íšŒ (ì›”ë³„ ë°ì´í„°)
    
    ìµœê·¼ Në…„ì¹˜ ì›”ë³„ ê±°ë˜ëŸ‰ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì—°ë„ë³„ ì§‘ê³„ ë˜ëŠ” ì›”ë³„ ë·°ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    # íŒŒë¼ë¯¸í„° ê²€ì¦
    if region_type not in ["ì „êµ­", "ìˆ˜ë„ê¶Œ", "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"]:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ region_type: {region_type}. í—ˆìš© ê°’: ì „êµ­, ìˆ˜ë„ê¶Œ, ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"
        )
    
    if transaction_type not in ["sale", "rent"]:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ transaction_type: {transaction_type}. í—ˆìš© ê°’: sale, rent"
        )
    
    # ìºì‹œ í‚¤ ìƒì„±
    cache_key = build_cache_key(
        "statistics", "volume", region_type, transaction_type, str(max_years)
    )
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„ (ì¼ì‹œì ìœ¼ë¡œ ë¹„í™œì„±í™”í•˜ì—¬ DB ì§ì ‘ ì¡°íšŒ)
    # TODO: ì›ì¸ íŒŒì•… í›„ ì¬í™œì„±í™”
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        # ìºì‹œëœ ë°ì´í„°ì˜ ì—°ë„ ë²”ìœ„ í™•ì¸ (ë””ë²„ê¹…ìš©)
        if cached_data.get("data"):
            cached_years = sorted(set(int(item.get("year", 0)) for item in cached_data.get("data", [])), reverse=True)
            cached_data_count = len(cached_data.get("data", []))
            logger.warning(
                f"âš ï¸ [Statistics Transaction Volume] ìºì‹œ ë°œê²¬ (ë¬´ì‹œí•˜ê³  DB ì¡°íšŒ) - "
                f"region_type: {region_type}, "
                f"ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {cached_data_count}, "
                f"ì—°ë„ ë²”ìœ„: {cached_years[0] if cached_years else 'N/A'} ~ {cached_years[-1] if cached_years else 'N/A'}, "
                f"ìºì‹œ í‚¤: {cache_key}"
            )
            # ìºì‹œ ë¬´ì‹œí•˜ê³  DBì—ì„œ ì§ì ‘ ì¡°íšŒ (ë””ë²„ê¹…ìš©)
            # return cached_data
        else:
            logger.info(f"âœ… [Statistics Transaction Volume] ìºì‹œì—ì„œ ë°˜í™˜ (ë°ì´í„° ì—†ìŒ) - region_type: {region_type}, ìºì‹œ í‚¤: {cache_key}")
            return cached_data
    
    try:
        logger.info(
            f"ğŸ” [Statistics Transaction Volume] ê±°ë˜ëŸ‰ ë°ì´í„° ì¡°íšŒ ì‹œì‘ - "
            f"region_type: {region_type}, transaction_type: {transaction_type}, max_years: {max_years}"
        )
        
        # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì—°ë„ ë²”ìœ„ ê³„ì‚°
        current_date = date.today()
        start_year = current_date.year - max_years + 1
        start_date = date(start_year, 1, 1)
        end_date = current_date
        
        logger.info(
            f"ğŸ“… [Statistics Transaction Volume] ë‚ ì§œ ë²”ìœ„ ì„¤ì • - "
            f"start_date: {start_date}, end_date: {end_date}, "
            f"start_year: {start_year}, max_years: {max_years}"
        )
        
        # ê±°ë˜ ìœ í˜•ì— ë”°ë¥¸ í…Œì´ë¸” ë° í•„ë“œ ì„ íƒ
        if transaction_type == "sale":
            trans_table = Sale
            date_field = Sale.contract_date
            base_filter = and_(
                Sale.is_canceled == False,
                (Sale.is_deleted == False) | (Sale.is_deleted.is_(None)),
                Sale.contract_date.isnot(None),
                # remarks í•„í„°: í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ëª¨ë‘ 'ë”ë¯¸'ì´ë¯€ë¡œ ì¼ë‹¨ ì œê±°
                # TODO: ì‹¤ì œ ìš´ì˜ ë°ì´í„°ì—ì„œ ë”ë¯¸ ë°ì´í„° ì œì™¸ í•„ìš” ì‹œ ì¬í™œì„±í™”
                # or_(Sale.remarks != "ë”ë¯¸", Sale.remarks.is_(None)),
                Sale.contract_date >= start_date,
                Sale.contract_date <= end_date
            )
        else:  # rent
            trans_table = Rent
            date_field = Rent.deal_date
            base_filter = and_(
                (Rent.is_deleted == False) | (Rent.is_deleted.is_(None)),
                Rent.deal_date.isnot(None),
                # remarks í•„í„°: í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ëª¨ë‘ 'ë”ë¯¸'ì´ë¯€ë¡œ ì¼ë‹¨ ì œê±°
                # TODO: ì‹¤ì œ ìš´ì˜ ë°ì´í„°ì—ì„œ ë”ë¯¸ ë°ì´í„° ì œì™¸ í•„ìš” ì‹œ ì¬í™œì„±í™”
                # or_(Rent.remarks != "ë”ë¯¸", Rent.remarks.is_(None)),
                Rent.deal_date >= start_date,
                Rent.deal_date <= end_date
            )
        
        # ë””ë²„ê¹…: ì‹¤ì œ DBì— ì¡´ì¬í•˜ëŠ” ìµœì‹  ë°ì´í„° ì—°ë„ í™•ì¸ (í•„í„° ì „)
        max_date_query_all = select(
            func.max(date_field).label('max_date'),
            func.min(date_field).label('min_date'),
            func.count().label('total_count')
        ).select_from(trans_table).where(
            and_(
                trans_table.is_canceled == False if transaction_type == "sale" else True,
                (trans_table.is_deleted == False) | (trans_table.is_deleted.is_(None)),
                date_field.isnot(None)
            )
        )
        max_date_result_all = await db.execute(max_date_query_all)
        max_date_row_all = max_date_result_all.first()
        
        # ë””ë²„ê¹…: remarks ê°’ ë¶„í¬ í™•ì¸
        remarks_dist_query = select(
            trans_table.remarks,
            func.count().label('count')
        ).select_from(trans_table).where(
            and_(
                trans_table.is_canceled == False if transaction_type == "sale" else True,
                (trans_table.is_deleted == False) | (trans_table.is_deleted.is_(None)),
                date_field.isnot(None),
                date_field >= start_date,
                date_field <= end_date
            )
        ).group_by(trans_table.remarks).limit(10)
        remarks_dist_result = await db.execute(remarks_dist_query)
        remarks_dist_rows = remarks_dist_result.all()
        remarks_dist = {row.remarks or 'NULL': row.count for row in remarks_dist_rows}
        logger.info(
            f"ğŸ” [Statistics Transaction Volume] remarks ê°’ ë¶„í¬ í™•ì¸ - "
            f"{remarks_dist}"
        )
        
        # ë””ë²„ê¹…: í•„í„° ì ìš© í›„ ë°ì´í„° í™•ì¸ (remarks í•„í„° ì œì™¸)
        max_date_query_no_remarks = select(
            func.max(date_field).label('max_date'),
            func.min(date_field).label('min_date'),
            func.count().label('total_count')
        ).select_from(trans_table).where(
            and_(
                trans_table.is_canceled == False if transaction_type == "sale" else True,
                (trans_table.is_deleted == False) | (trans_table.is_deleted.is_(None)),
                date_field.isnot(None),
                date_field >= start_date,
                date_field <= end_date
            )
        )
        max_date_result_no_remarks = await db.execute(max_date_query_no_remarks)
        max_date_row_no_remarks = max_date_result_no_remarks.first()
        
        # ë””ë²„ê¹…: base_filter ì ìš© í›„ ë°ì´í„° í™•ì¸ (remarks í•„í„° ì œê±°ë¨)
        max_date_query = select(
            func.max(date_field).label('max_date'),
            func.min(date_field).label('min_date'),
            func.count().label('total_count')
        ).select_from(trans_table).where(
            and_(
                trans_table.is_canceled == False if transaction_type == "sale" else True,
                (trans_table.is_deleted == False) | (trans_table.is_deleted.is_(None)),
                date_field.isnot(None),
                # remarks í•„í„° ì œê±°ë¨ (í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ëª¨ë‘ 'ë”ë¯¸')
                date_field >= start_date,
                date_field <= end_date
            )
        )
        max_date_result = await db.execute(max_date_query)
        max_date_row = max_date_result.first()
        
        if max_date_row_all:
            logger.info(
                f"ğŸ” [Statistics Transaction Volume] DB ì „ì²´ ë°ì´í„° ë²”ìœ„ (í•„í„° ì „) - "
                f"ìµœì‹  ë‚ ì§œ: {max_date_row_all.max_date}, "
                f"ìµœ old ë‚ ì§œ: {max_date_row_all.min_date}, "
                f"ì „ì²´ ê±°ë˜ ìˆ˜: {max_date_row_all.total_count}"
            )
        
        if max_date_row_no_remarks:
            logger.info(
                f"ğŸ” [Statistics Transaction Volume] DB ë°ì´í„° ë²”ìœ„ (remarks í•„í„° ì œì™¸) - "
                f"ìµœì‹  ë‚ ì§œ: {max_date_row_no_remarks.max_date}, "
                f"ìµœ old ë‚ ì§œ: {max_date_row_no_remarks.min_date}, "
                f"ê±°ë˜ ìˆ˜: {max_date_row_no_remarks.total_count}"
            )
        
        if max_date_row and max_date_row.max_date:
            logger.info(
                f"ğŸ” [Statistics Transaction Volume] DB ì‹¤ì œ ë°ì´í„° ë²”ìœ„ (base_filter ì ìš©) - "
                f"ìµœì‹  ë‚ ì§œ: {max_date_row.max_date}, "
                f"ìµœ old ë‚ ì§œ: {max_date_row.min_date}, "
                f"í•„í„°ë§ëœ ê±°ë˜ ìˆ˜: {max_date_row.total_count}, "
                f"ë‚ ì§œ ë²”ìœ„: {start_date} ~ {end_date}"
            )
            
            # ë‚ ì§œ ë²”ìœ„ì™€ ì‹¤ì œ ë°ì´í„° ë²”ìœ„ ë¹„êµ
            if max_date_row.min_date and max_date_row.min_date > start_date:
                logger.warning(
                    f"âš ï¸ [Statistics Transaction Volume] ë‚ ì§œ ë²”ìœ„ ë¶ˆì¼ì¹˜ - "
                    f"ìš”ì²­í•œ ì‹œì‘ ë‚ ì§œ: {start_date}, "
                    f"ì‹¤ì œ ë°ì´í„° ìµœì†Œ ë‚ ì§œ: {max_date_row.min_date}, "
                    f"ì°¨ì´: {(max_date_row.min_date - start_date).days}ì¼"
                )
        
        # ì§€ì—­ í•„í„°ë§ ì¡°ê±´ ê°€ì ¸ì˜¤ê¸°
        region_filter = get_region_type_filter(region_type)
        
        # ë””ë²„ê¹…: ì‹¤ì œ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ìˆ˜ë„ê¶Œ, ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ)
        if region_type in ["ìˆ˜ë„ê¶Œ", "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"]:
            debug_query = select(
                extract('year', date_field).label('year'),
                func.count().label('count')
            ).select_from(
                trans_table
            ).join(
                Apartment, trans_table.apt_id == Apartment.apt_id
            ).join(
                State, Apartment.region_id == State.region_id
            ).where(
                and_(base_filter, region_filter)
            ).group_by(
                extract('year', date_field)
            ).order_by(
                desc(extract('year', date_field))
            )
            debug_result = await db.execute(debug_query)
            debug_rows = debug_result.all()
            debug_years = [int(row.year) for row in debug_rows[:10]]  # ìµœì‹  10ê°œ ì—°ë„
            logger.info(
                f"ğŸ” [Statistics Transaction Volume] {region_type} ì‹¤ì œ ë°ì´í„° ì—°ë„ í™•ì¸ - "
                f"ìµœì‹  ì—°ë„: {debug_years[0] if debug_years else 'N/A'}, "
                f"ì—°ë„ ëª©ë¡: {debug_years}, "
                f"ì´ {len(debug_rows)}ê°œ ì—°ë„ ë°ì´í„° ì¡´ì¬"
            )
            
            # JOIN ì „ ë°ì´í„° í™•ì¸ (ë””ë²„ê¹…ìš©) - í•­ìƒ ì‹¤í–‰
            # JOIN ì—†ì´ ê±°ë˜ ë°ì´í„°ë§Œ í™•ì¸
            no_join_query = select(
                extract('year', date_field).label('year'),
                func.count().label('count')
            ).select_from(
                trans_table
            ).where(
                base_filter
            ).group_by(
                extract('year', date_field)
            ).order_by(
                desc(extract('year', date_field))
            )
            no_join_result = await db.execute(no_join_query)
            no_join_rows = no_join_result.all()
            no_join_years = [int(row.year) for row in no_join_rows[:10]]
            
            if len(debug_rows) == 0 and len(no_join_rows) > 0:
                logger.warning(
                    f"âš ï¸ [Statistics Transaction Volume] {region_type} JOINìœ¼ë¡œ ì¸í•œ ë°ì´í„° ì†ì‹¤ í™•ì¸ - "
                    f"JOIN ì „: {len(no_join_rows)}ê°œ ì—°ë„ (ìµœì‹ : {no_join_years[0] if no_join_years else 'N/A'}), "
                    f"JOIN í›„: 0ê°œ ì—°ë„ (JOIN ì¡°ê±´ ë¬¸ì œ ê°€ëŠ¥ì„±)"
                )
            elif len(debug_rows) > 0:
                logger.info(
                    f"âœ… [Statistics Transaction Volume] {region_type} JOIN ì „/í›„ ë°ì´í„° ë¹„êµ - "
                    f"JOIN ì „: {len(no_join_rows)}ê°œ ì—°ë„, JOIN í›„: {len(debug_rows)}ê°œ ì—°ë„"
                )
        
        # ì¿¼ë¦¬ êµ¬ì„±
        if region_type == "ì „êµ­":
            # ì „êµ­: JOIN ì—†ì´ ê±°ë˜ í…Œì´ë¸”ë§Œ ì‚¬ìš©
            # ë””ë²„ê¹…: ì „êµ­ ì¿¼ë¦¬ ì „ì— base_filter ì ìš© ê²°ê³¼ í™•ì¸
            debug_national_query = select(
                extract('year', date_field).label('year'),
                func.count().label('count')
            ).select_from(
                trans_table
            ).where(
                base_filter
            ).group_by(
                extract('year', date_field)
            ).order_by(
                desc(extract('year', date_field))
            )
            debug_national_result = await db.execute(debug_national_query)
            debug_national_rows = debug_national_result.all()
            debug_national_years = [int(row.year) for row in debug_national_rows]
            logger.info(
                f"ğŸ” [Statistics Transaction Volume] ì „êµ­ base_filter ì ìš© í›„ ì—°ë„ í™•ì¸ - "
                f"ì—°ë„ ëª©ë¡: {debug_national_years[:10]}, "
                f"ì´ {len(debug_national_rows)}ê°œ ì—°ë„"
            )
            
            query = select(
                extract('year', date_field).label('year'),
                extract('month', date_field).label('month'),
                func.count().label('volume')
            ).select_from(
                trans_table
            ).where(
                base_filter
            ).group_by(
                extract('year', date_field),
                extract('month', date_field)
            ).order_by(
                desc(extract('year', date_field)),
                extract('month', date_field)
            )
        elif region_type == "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ":
            # ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ: city_name í¬í•¨í•˜ì—¬ ê·¸ë£¹í™”
            # ë””ë²„ê¹…: ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ JOIN ì „ ë°ì´í„° í™•ì¸
            debug_local_before_join = select(
                extract('year', date_field).label('year'),
                func.count().label('count')
            ).select_from(
                trans_table
            ).where(
                base_filter
            ).group_by(
                extract('year', date_field)
            ).order_by(
                desc(extract('year', date_field))
            )
            debug_local_before_result = await db.execute(debug_local_before_join)
            debug_local_before_rows = debug_local_before_result.all()
            debug_local_before_years = [int(row.year) for row in debug_local_before_rows]
            logger.info(
                f"ğŸ” [Statistics Transaction Volume] ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ JOIN ì „ ì—°ë„ í™•ì¸ - "
                f"ì—°ë„ ëª©ë¡: {debug_local_before_years[:10]}, "
                f"ì´ {len(debug_local_before_rows)}ê°œ ì—°ë„"
            )
            
            query = select(
                extract('year', date_field).label('year'),
                extract('month', date_field).label('month'),
                State.city_name.label('city_name'),
                func.count().label('volume')
            ).select_from(
                trans_table
            ).join(
                Apartment, trans_table.apt_id == Apartment.apt_id
            ).join(
                State, Apartment.region_id == State.region_id
            ).where(
                and_(base_filter, region_filter)
            ).group_by(
                extract('year', date_field),
                extract('month', date_field),
                State.city_name
            ).order_by(
                desc(extract('year', date_field)),
                extract('month', date_field),
                State.city_name
            )
        else:  # ìˆ˜ë„ê¶Œ
            # ìˆ˜ë„ê¶Œ: JOIN ì‚¬ìš©í•˜ì§€ë§Œ city_nameì€ ê·¸ë£¹í™”í•˜ì§€ ì•ŠìŒ
            query = select(
                extract('year', date_field).label('year'),
                extract('month', date_field).label('month'),
                func.count().label('volume')
            ).select_from(
                trans_table
            ).join(
                Apartment, trans_table.apt_id == Apartment.apt_id
            ).join(
                State, Apartment.region_id == State.region_id
            ).where(
                and_(base_filter, region_filter)
            ).group_by(
                extract('year', date_field),
                extract('month', date_field)
            ).order_by(
                desc(extract('year', date_field)),
                extract('month', date_field)
            )
        
        # ì¿¼ë¦¬ ì‹¤í–‰
        result = await db.execute(query)
        rows = result.all()
        
        logger.info(
            f"ğŸ“Š [Statistics Transaction Volume] ì¿¼ë¦¬ ê²°ê³¼ - "
            f"ì´ {len(rows)}ê°œ í–‰ ë°˜í™˜, region_type: {region_type}"
        )
        
        # ë°ì´í„°ê°€ ì—†ì„ ë•Œ ìƒì„¸ ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
        if len(rows) == 0 and region_type in ["ìˆ˜ë„ê¶Œ", "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"]:
            # JOIN ì—†ì´ ì „ì²´ ê±°ë˜ ë°ì´í„° í™•ì¸
            total_count_query = select(func.count()).select_from(trans_table).where(base_filter)
            total_result = await db.execute(total_count_query)
            total_count = total_result.scalar() or 0
            
            logger.warning(
                f"âš ï¸ [Statistics Transaction Volume] {region_type} ë°ì´í„° ì—†ìŒ - "
                f"base_filter ì ìš© í›„ ì „ì²´ ê±°ë˜ ìˆ˜: {total_count}, "
                f"JOIN í›„ ë°ì´í„°: 0ê°œ (JOIN ì¡°ê±´ ë¬¸ì œ ê°€ëŠ¥ì„± ë†’ìŒ)"
            )
        
        # ì—°ë„ë³„ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ë””ë²„ê¹…ìš©)
        if rows:
            years = sorted(set(int(row.year) for row in rows), reverse=True)
            logger.info(
                f"ğŸ“Š [Statistics Transaction Volume] DB ì¿¼ë¦¬ ê²°ê³¼ - "
                f"region_type: {region_type}, "
                f"ë°ì´í„° í–‰ ìˆ˜: {len(rows)}, "
                f"ì—°ë„ ë²”ìœ„: {years[0] if years else 'N/A'} ~ {years[-1] if years else 'N/A'}, "
                f"ì „ì²´ ì—°ë„: {years[:10] if len(years) <= 10 else years[:10] + ['...']}, "
                f"ìºì‹œ í‚¤: {cache_key}"
            )
        
        # ë°ì´í„° í¬ì¸íŠ¸ ìƒì„±
        data_points = []
        for row in rows:
            data_point = TransactionVolumeDataPoint(
                year=int(row.year),
                month=int(row.month),
                volume=int(row.volume),
                city_name=row.city_name if hasattr(row, 'city_name') and row.city_name else None
            )
            data_points.append(data_point)
        
        # ê¸°ê°„ ë¬¸ìì—´ ìƒì„±
        period_str = f"{start_date.strftime('%Y-%m')} ~ {end_date.strftime('%Y-%m')}"
        
        # ì‘ë‹µ ìƒì„±
        response_data = TransactionVolumeResponse(
            success=True,
            data=data_points,
            region_type=region_type,
            period=period_str,
            max_years=max_years
        )
        
        # ìºì‹œì— ì €ì¥
        if len(data_points) > 0:
            await set_to_cache(cache_key, response_data.dict(), ttl=STATISTICS_CACHE_TTL)
            logger.info(
                f"ğŸ’¾ [Statistics Transaction Volume] ìºì‹œ ì €ì¥ ì™„ë£Œ - "
                f"region_type: {region_type}, "
                f"ì—°ë„ ë²”ìœ„: {years[0] if years else 'N/A'} ~ {years[-1] if years else 'N/A'}"
            )
        
        logger.info(
            f"âœ… [Statistics Transaction Volume] ê±°ë˜ëŸ‰ ë°ì´í„° ìƒì„± ì™„ë£Œ - "
            f"ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(data_points)}, ê¸°ê°„: {period_str}"
        )
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(
            f"âŒ [Statistics Transaction Volume] ê±°ë˜ëŸ‰ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}",
            exc_info=True
        )
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ê±°ë˜ëŸ‰ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


# ============================================================
# ì‹œì¥ êµ­ë©´ ì§€í‘œ API
# ============================================================

@router.get(
    "/market-phase",
    response_model=Union[MarketPhaseResponse, MarketPhaseListResponse],
    summary="ì‹œì¥ êµ­ë©´ ì§€í‘œ ì¡°íšŒ",
    description="ë²Œì§‘ ìˆœí™˜ ëª¨í˜•(Honeycomb Cycle) ê¸°ë°˜ìœ¼ë¡œ ì‹œì¥ êµ­ë©´ì„ íŒë³„í•©ë‹ˆë‹¤."
)
async def get_market_phase(
    region_type: str = Query(..., description="ì§€ì—­ ìœ í˜• (ì „êµ­, ìˆ˜ë„ê¶Œ, ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ)"),
    volume_calculation_method: str = Query("average", description="ê±°ë˜ëŸ‰ ê³„ì‚° ë°©ë²• (average, month_over_month)"),
    average_period_months: int = Query(6, ge=1, le=12, description="í‰ê·  ê³„ì‚° ê¸°ê°„ (ê°œì›”)"),
    volume_threshold: Optional[float] = Query(None, description="ê±°ë˜ëŸ‰ ë³€ë™ ì„ê³„ê°’ (%)"),
    price_threshold: Optional[float] = Query(None, description="ê°€ê²© ë³€ë™ ì„ê³„ê°’ (%)"),
    min_transaction_count: int = Query(5, ge=1, description="ìµœì†Œ ê±°ë˜ ê±´ìˆ˜"),
    db: AsyncSession = Depends(get_db)
):
    """
    ì‹œì¥ êµ­ë©´ ì§€í‘œ ì¡°íšŒ
    
    ë²Œì§‘ ìˆœí™˜ ëª¨í˜•(Honeycomb Cycle) ê¸°ë°˜ìœ¼ë¡œ ì‹œì¥ êµ­ë©´ì„ íŒë³„í•©ë‹ˆë‹¤.
    
    **6ê°œ êµ­ë©´:**
    1. íšŒë³µ (Recovery): ê±°ë˜ëŸ‰ ì¦ê°€ â†‘ / ê°€ê²© í•˜ë½ í˜¹ì€ ë³´í•© â†’
    2. ìƒìŠ¹ (Expansion): ê±°ë˜ëŸ‰ ì¦ê°€ â†‘ / ê°€ê²© ìƒìŠ¹ â†‘
    3. ë‘”í™” (Slowdown): ê±°ë˜ëŸ‰ ê°ì†Œ â†“ / ê°€ê²© ìƒìŠ¹ â†‘
    4. í›„í‡´ (Recession): ê±°ë˜ëŸ‰ ê°ì†Œ â†“ / ê°€ê²© í•˜ë½ â†“
    5. ì¹¨ì²´ (Depression): ê±°ë˜ëŸ‰ ê¸‰ê° â†“ / ê°€ê²© í•˜ë½ì„¸ ì§€ì† â†“
    6. ì²œì°© (Trough): ê±°ë˜ëŸ‰ ë¯¸ì„¸ ì¦ê°€ â†‘ / ê°€ê²© í•˜ë½ â†“
    """
    try:
        # íŒŒë¼ë¯¸í„° ê²€ì¦
        if region_type not in ["ì „êµ­", "ìˆ˜ë„ê¶Œ", "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"]:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ region_type: {region_type}. í—ˆìš© ê°’: ì „êµ­, ìˆ˜ë„ê¶Œ, ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"
            )
        
        if volume_calculation_method not in ["average", "month_over_month"]:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ volume_calculation_method: {volume_calculation_method}. í—ˆìš© ê°’: average, month_over_month"
            )
        
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = build_cache_key(
            "statistics",
            "market-phase",
            region_type,
            volume_calculation_method,
            str(average_period_months),
            str(volume_threshold) if volume_threshold is not None else "default",
            str(price_threshold) if price_threshold is not None else "default",
            str(min_transaction_count)
        )
        
        # ìºì‹œ í™•ì¸
        cached_result = await get_from_cache(cache_key)
        if cached_result:
            logger.info(
                f"[Market Phase] Cache hit - region_type: {region_type}"
            )
            # ìºì‹œëœ ê²°ê³¼ì—ë„ ì„ê³„ê°’ì´ í¬í•¨ë˜ì–´ ìˆì§€ë§Œ, ë¡œê¹…ì„ ìœ„í•´ í™•ì¸
            if isinstance(cached_result, dict) and 'thresholds' in cached_result:
                thresholds = cached_result.get('thresholds', {})
                logger.info(
                    f"[Market Phase] Cached thresholds - "
                    f"vol={thresholds.get('volume_threshold')}, "
                    f"price={thresholds.get('price_threshold')}"
                )
            return cached_result
        
        # ì„ê³„ê°’ ì¡°íšŒ (ì‘ë‹µì— ì‚¬ìš©ë  ì„ê³„ê°’)
        vol_threshold, price_thresh = await get_thresholds(
            db, region_type, None, volume_threshold, price_threshold
        )
        
        logger.info(
            f"[Market Phase] Calculation started - "
            f"region_type: {region_type}, "
            f"volume_method: {volume_calculation_method}, "
            f"thresholds: vol={vol_threshold}, price={price_thresh} "
            f"(API params: vol={volume_threshold}, price={price_threshold})"
        )
        
        # ì „êµ­/ìˆ˜ë„ê¶Œ: ë‹¨ì¼ ë°ì´í„°
        if region_type in ["ì „êµ­", "ìˆ˜ë„ê¶Œ"]:
            # ê±°ë˜ëŸ‰ ë³€ë™ë¥  ê³„ì‚°
            if volume_calculation_method == "average":
                volume_change_rate, current_volume = await calculate_volume_change_rate_average(
                    db, region_type, None, average_period_months
                )
            else:
                volume_change_rate, current_volume = await calculate_volume_change_rate_mom(
                    db, region_type, None
                )
            
            # ê°€ê²© ë³€ë™ë¥  ê³„ì‚°
            price_change_rate = await calculate_price_change_rate_moving_average(
                db, region_type, None
            )
            
            # êµ­ë©´ íŒë³„
            phase_data = calculate_market_phase(
                volume_change_rate,
                price_change_rate,
                current_volume,
                min_transaction_count,
                vol_threshold,
                price_thresh
            )
            
            # ì‘ë‹µ ìƒì„±
            response = MarketPhaseResponse(
                success=True,
                data=MarketPhaseDataPoint(
                    region=None,
                    volume_change_rate=volume_change_rate,
                    price_change_rate=price_change_rate,
                    **phase_data
                ),
                calculation_method=MarketPhaseCalculationMethod(
                    volume_method=volume_calculation_method,
                    average_period_months=average_period_months if volume_calculation_method == "average" else None,
                    price_method="moving_average_3months"
                ),
                thresholds=MarketPhaseThresholds(
                    volume_threshold=vol_threshold,
                    price_threshold=price_thresh
                )
            )
            
            # ìºì‹œ ì €ì¥ (TTL: 1ì‹œê°„)
            await set_to_cache(cache_key, response.dict(), ttl=3600)
            
            logger.info(
                f"[Market Phase] Calculation completed - "
                f"region_type: {region_type}, "
                f"phase: {phase_data.get('phase')}, "
                f"phase_label: {phase_data.get('phase_label')}, "
                f"Response thresholds: vol={vol_threshold}, price={price_thresh}"
            )
            
            return response
        
        # ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ: ì§€ì—­ë³„ ë°ì´í„°
        else:
            regions = ['ë¶€ì‚°ê´‘ì—­ì‹œ', 'ëŒ€êµ¬ê´‘ì—­ì‹œ', 'ê´‘ì£¼ê´‘ì—­ì‹œ', 'ëŒ€ì „ê´‘ì—­ì‹œ', 'ìš¸ì‚°ê´‘ì—­ì‹œ']
            data_list = []
            
            # ìˆœì°¨ ì²˜ë¦¬ë¡œ ë³€ê²½ (SQLAlchemy AsyncSessionì€ ë™ì‹œ ì¿¼ë¦¬ ë¶ˆê°€)
            # ë³‘ë ¬ ì²˜ë¦¬ëŠ” ê°™ì€ ì„¸ì…˜ì„ ê³µìœ í•˜ë©´ ì„¸ì…˜ ì¶©ëŒ ë°œìƒ
            for region in regions:
                logger.info(
                    f"[Market Phase] Region calculation started - region: {region}"
                )
                
                # ê±°ë˜ëŸ‰ ë³€ë™ë¥  ê³„ì‚°
                if volume_calculation_method == "average":
                    volume_change_rate, current_volume = await calculate_volume_change_rate_average(
                        db, region_type, region, average_period_months
                    )
                else:
                    volume_change_rate, current_volume = await calculate_volume_change_rate_mom(
                        db, region_type, region
                    )
                
                # ê°€ê²© ë³€ë™ë¥  ê³„ì‚°
                price_change_rate = await calculate_price_change_rate_moving_average(
                    db, region_type, region
                )
                
                # ì§€ì—­ë³„ ì„ê³„ê°’ ì¡°íšŒ (ì§€ë°©5ëŒ€ê´‘ì—­ì‹œëŠ” ê° ì§€ì—­ë³„ë¡œ ë™ì¼í•œ ì„ê³„ê°’ ì‚¬ìš©)
                # region_nameì€ ì •ê·œí™”ëœ ì´ë¦„ ì‚¬ìš© (ì˜ˆ: "ê´‘ì£¼" ëŒ€ì‹  "ê´‘ì£¼ê´‘ì—­ì‹œ")
                region_vol_threshold, region_price_thresh = await get_thresholds(
                    db, region_type, region, volume_threshold, price_threshold
                )
                
                # êµ­ë©´ íŒë³„
                phase_data = calculate_market_phase(
                    volume_change_rate,
                    price_change_rate,
                    current_volume,
                    min_transaction_count,
                    region_vol_threshold,
                    region_price_thresh
                )
                
                # ì§€ì—­ëª… ì •ê·œí™”
                normalized_region = normalize_city_name(region)
                
                data_list.append(
                    MarketPhaseDataPoint(
                        region=normalized_region,
                        volume_change_rate=volume_change_rate,
                        price_change_rate=price_change_rate,
                        **phase_data
                    )
                )
                
                logger.info(
                    f"âœ… [Market Phase] ì§€ì—­ ê³„ì‚° ì™„ë£Œ - "
                    f"region: {region}, "
                    f"phase: {phase_data.get('phase')}, "
                    f"volume: {current_volume}"
                )
            
            # ì§€ë°©5ëŒ€ê´‘ì—­ì‹œëŠ” ì§€ì—­ë³„ë¡œ ë™ì¼í•œ ì„ê³„ê°’ ì‚¬ìš© (1.7%, 0.4%)
            # vol_threshold, price_threshëŠ” ì´ë¯¸ get_thresholdsì—ì„œ ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •ë¨
            # ì‘ë‹µì— ì‚¬ìš©ëœ ì„ê³„ê°’ ë¡œê¹…
            logger.info(
                f"[Market Phase] Response thresholds for ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ - "
                f"volume_threshold: {vol_threshold}, price_threshold: {price_thresh}"
            )
            
            response = MarketPhaseListResponse(
                success=True,
                data=data_list,
                region_type=region_type,
                calculation_method=MarketPhaseCalculationMethod(
                    volume_method=volume_calculation_method,
                    average_period_months=average_period_months if volume_calculation_method == "average" else None,
                    price_method="moving_average_3months"
                ),
                thresholds=MarketPhaseThresholds(
                    volume_threshold=vol_threshold,  # ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ: 1.7%
                    price_threshold=price_thresh     # ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ: 0.4%
                )
            )
            
            # ìºì‹œ ì €ì¥ (TTL: 1ì‹œê°„)
            await set_to_cache(cache_key, response.dict(), ttl=3600)
            
            logger.info(
                f"âœ… [Market Phase] ê³„ì‚° ì™„ë£Œ - "
                f"region_type: {region_type}, "
                f"ì§€ì—­ ìˆ˜: {len(data_list)}"
            )
            
            return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(
            f"âŒ [Market Phase] ì‹œì¥ êµ­ë©´ ì§€í‘œ ì¡°íšŒ ì‹¤íŒ¨: {e}",
            exc_info=True
        )
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì‹œì¥ êµ­ë©´ ì§€í‘œ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )
