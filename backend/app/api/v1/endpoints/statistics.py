"""
í†µê³„ ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸

ë‹´ë‹¹ ê¸°ëŠ¥:
- RVOL(ìƒëŒ€ ê±°ë˜ëŸ‰) ê³„ì‚° ë° ì¡°íšŒ
- 4ë¶„ë©´ ë¶„ë¥˜ (ë§¤ë§¤/ì „ì›”ì„¸ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨ ê¸°ë°˜)

ì„±ëŠ¥ ìµœì í™”:
- ê¸°ê°„ ì œí•œ: ìµœëŒ€ 2~3ê°œì›”
- ì›”ë³„ ì§‘ê³„ë¡œ ê°„ì†Œí™”
- ê¸´ ìºì‹œ TTL (6ì‹œê°„)
"""
import logging
import sys
import asyncio
from datetime import date, datetime, timedelta
from typing import Optional, List, Dict, Any
from fastapi import APIRouter, Depends, HTTPException, Query, status
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func, and_, or_, case, desc, text, extract
from sqlalchemy.orm import selectinload

from app.api.v1.deps import get_db
from app.models.sale import Sale
from app.models.rent import Rent
from app.models.apartment import Apartment
from app.models.state import State
from app.models.house_score import HouseScore
from app.models.population_movement import PopulationMovement
from app.schemas.statistics import (
    RVOLResponse,
    RVOLDataPoint,
    QuadrantResponse,
    QuadrantDataPoint,
    StatisticsSummaryResponse,
    HPIResponse,
    HPIDataPoint,
    HPIHeatmapResponse,
    HPIHeatmapDataPoint,
    PopulationMovementResponse,
    PopulationMovementDataPoint,
    PopulationMovementSankeyResponse,
    PopulationMovementSankeyDataPoint,
    CorrelationAnalysisResponse
)
from app.utils.cache import get_from_cache, set_to_cache, build_cache_key

# ë¡œê±° ì„¤ì • (Docker ë¡œê·¸ì— ì¶œë ¥ë˜ë„ë¡)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
if not logger.handlers:
    handler = logging.StreamHandler(sys.stdout)
    handler.setLevel(logging.INFO)
    formatter = logging.Formatter(
        '%(asctime)s | %(levelname)s | %(name)s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.propagate = True  # ë£¨íŠ¸ ë¡œê±°ë¡œë„ ì „íŒŒ

router = APIRouter()

# ìºì‹œ TTL: 6ì‹œê°„ (í†µê³„ ë°ì´í„°ëŠ” ìì£¼ ë³€í•˜ì§€ ì•ŠìŒ)
STATISTICS_CACHE_TTL = 21600


def calculate_quadrant(sale_change_rate: float, rent_change_rate: float) -> tuple[int, str]:
    """
    4ë¶„ë©´ ë¶„ë¥˜ ê³„ì‚°
    
    Args:
        sale_change_rate: ë§¤ë§¤ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨ (%)
        rent_change_rate: ì „ì›”ì„¸ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨ (%)
    
    Returns:
        (quadrant_number, quadrant_label) íŠœí”Œ
    """
    if sale_change_rate > 0 and rent_change_rate < 0:
        return (1, "ë§¤ìˆ˜ ì „í™˜")
    elif sale_change_rate < 0 and rent_change_rate > 0:
        return (2, "ì„ëŒ€ ì„ í˜¸/ê´€ë§")
    elif sale_change_rate < 0 and rent_change_rate < 0:
        return (3, "ì‹œì¥ ìœ„ì¶•")
    elif sale_change_rate > 0 and rent_change_rate > 0:
        return (4, "í™œì„±í™”")
    else:
        # ë³€í™”ìœ¨ì´ 0ì¸ ê²½ìš°ëŠ” ì¤‘ë¦½ìœ¼ë¡œ ì²˜ë¦¬
        if sale_change_rate == 0 and rent_change_rate == 0:
            return (0, "ì¤‘ë¦½")
        elif sale_change_rate == 0:
            return (2 if rent_change_rate > 0 else 3, "ì„ëŒ€ ì„ í˜¸/ê´€ë§" if rent_change_rate > 0 else "ì‹œì¥ ìœ„ì¶•")
        else:
            return (1 if sale_change_rate > 0 else 3, "ë§¤ìˆ˜ ì „í™˜" if sale_change_rate > 0 else "ì‹œì¥ ìœ„ì¶•")


@router.get(
    "/rvol",
    response_model=RVOLResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="RVOL(ìƒëŒ€ ê±°ë˜ëŸ‰) ì¡°íšŒ",
    description="""
    RVOL(Relative Volume)ì„ ê³„ì‚°í•˜ì—¬ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ### RVOL ê³„ì‚° ë°©ë²•
    - í˜„ì¬ ê±°ë˜ëŸ‰ì„ ê³¼ê±° ì¼ì • ê¸°ê°„ì˜ í‰ê·  ê±°ë˜ëŸ‰ìœ¼ë¡œ ë‚˜ëˆˆ ê°’
    - ì˜ˆ: ìµœê·¼ 2ê°œì›” ê±°ë˜ëŸ‰ Ã· ì§ì „ 2ê°œì›” í‰ê·  ê±°ë˜ëŸ‰
    
    ### í•´ì„
    - **RVOL > 1**: í‰ì†Œë³´ë‹¤ ê±°ë˜ê°€ í™œë°œí•¨ (í‰ê·  ì´ìƒ)
    - **RVOL = 1**: í‰ì†Œì™€ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì˜ ê±°ë˜ëŸ‰
    - **RVOL < 1**: í‰ì†Œë³´ë‹¤ ê±°ë˜ê°€ í•œì‚°í•¨ (í‰ê·  ì´í•˜)
    
    ### Query Parameters
    - `transaction_type`: ê±°ë˜ ìœ í˜• (sale: ë§¤ë§¤, rent: ì „ì›”ì„¸, ê¸°ë³¸ê°’: sale)
    - `current_period_months`: í˜„ì¬ ê¸°ê°„ (ê°œì›”, ê¸°ë³¸ê°’: 6, ìµœëŒ€: 6)
    - `average_period_months`: í‰ê·  ê³„ì‚° ê¸°ê°„ (ê°œì›”, ê¸°ë³¸ê°’: 6, ìµœëŒ€: 6)
    """
)
async def get_rvol(
    transaction_type: str = Query("sale", description="ê±°ë˜ ìœ í˜•: sale(ë§¤ë§¤), rent(ì „ì›”ì„¸)"),
    current_period_months: int = Query(6, ge=1, le=12, description="í˜„ì¬ ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 12)"),
    average_period_months: int = Query(6, ge=1, le=12, description="í‰ê·  ê³„ì‚° ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 12)"),
    db: AsyncSession = Depends(get_db)
):
    """
    RVOL(ìƒëŒ€ ê±°ë˜ëŸ‰) ì¡°íšŒ - ì„±ëŠ¥ ìµœì í™” ë²„ì „
    
    ì›”ë³„ ì§‘ê³„ë¡œ ê°„ì†Œí™”í•˜ì—¬ ë¹ ë¥¸ ì‘ë‹µ ì œê³µ
    """
    cache_key = build_cache_key(
        "statistics", "rvol_v2", transaction_type, 
        str(current_period_months), str(average_period_months)
    )
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        logger.info(f"âœ… [Statistics RVOL] ìºì‹œì—ì„œ ë°˜í™˜")
        return cached_data
    
    try:
        logger.info(
            f"ğŸ” [Statistics RVOL] RVOL ë°ì´í„° ì¡°íšŒ ì‹œì‘ - "
            f"transaction_type: {transaction_type}, "
            f"current_period_months: {current_period_months}, "
            f"average_period_months: {average_period_months}"
        )
        
        # ê±°ë˜ ìœ í˜•ì— ë”°ë¥¸ í…Œì´ë¸” ë° í•„ë“œ ì„ íƒ
        if transaction_type == "sale":
            trans_table = Sale
            date_field = Sale.contract_date
            base_filter = and_(
                Sale.is_canceled == False,
                (Sale.is_deleted == False) | (Sale.is_deleted.is_(None)),
                Sale.contract_date.isnot(None),
                or_(Sale.remarks != "ë”ë¯¸", Sale.remarks.is_(None))
            )
        else:  # rent
            trans_table = Rent
            date_field = Rent.deal_date
            base_filter = and_(
                (Rent.is_deleted == False) | (Rent.is_deleted.is_(None)),
                Rent.deal_date.isnot(None),
                or_(Rent.remarks != "ë”ë¯¸", Rent.remarks.is_(None))
            )
        
        # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ê¸°ê°„ ì„¤ì • (min/max ì¿¼ë¦¬ ì œê±°)
        today = date.today()
        # í˜„ì¬ ë‹¬ì˜ ì²« ë‚  (í˜„ì¬ ë‹¬ ì œì™¸)
        current_month_start = date(today.year, today.month, 1)
        
        # í˜„ì¬ ê¸°ê°„: ìµœê·¼ current_period_months ê°œì›” (í˜„ì¬ ë‹¬ ì œì™¸)
        current_start = current_month_start - timedelta(days=current_period_months * 30)
        current_end = current_month_start  # í˜„ì¬ ë‹¬ì˜ ì²« ë‚  ì „ê¹Œì§€
        
        # í‰ê·  ê³„ì‚° ê¸°ê°„: current_start ì´ì „ average_period_months ê°œì›”
        average_start = current_start - timedelta(days=average_period_months * 30)
        average_end = current_start
        
        logger.info(
            f"ğŸ“… [Statistics RVOL] ë‚ ì§œ ë²”ìœ„ - "
            f"current_start: {current_start}, current_end: {current_end}, "
            f"average_start: {average_start}, average_end: {average_end}"
        )
        
        # ì›”ë³„ ì§‘ê³„ë¡œ ê°„ì†Œí™” (ì¼ë³„ ëŒ€ì‹  ì›”ë³„)
        # í‰ê·  ê¸°ê°„ ì›”ë³„ ê±°ë˜ëŸ‰
        average_volume_stmt = (
            select(
                extract('year', date_field).label('year'),
                extract('month', date_field).label('month'),
                func.count(trans_table.trans_id).label('count')
            )
            .where(
                and_(
                    base_filter,
                    date_field >= average_start,
                    date_field < average_end
                )
            )
            .group_by(extract('year', date_field), extract('month', date_field))
        )
        
        # í˜„ì¬ ê¸°ê°„ ì›”ë³„ ê±°ë˜ëŸ‰
        current_volume_stmt = (
            select(
                extract('year', date_field).label('year'),
                extract('month', date_field).label('month'),
                func.count(trans_table.trans_id).label('count')
            )
            .where(
                and_(
                    base_filter,
                    date_field >= current_start,
                    date_field < current_end  # í˜„ì¬ ë‹¬ ì œì™¸ (ë¯¸ë§Œìœ¼ë¡œ ë³€ê²½)
                )
            )
            .group_by(extract('year', date_field), extract('month', date_field))
        )
        
        # ë³‘ë ¬ ì‹¤í–‰
        average_result, current_result = await asyncio.gather(
            db.execute(average_volume_stmt),
            db.execute(current_volume_stmt)
        )
        
        average_rows = average_result.fetchall()
        current_rows = current_result.fetchall()
        
        # í‰ê·  ê±°ë˜ëŸ‰ ê³„ì‚°
        if average_rows:
            total_average = sum(row.count for row in average_rows)
            average_monthly_volume = total_average / len(average_rows)
        else:
            average_monthly_volume = 1  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        
        logger.info(
            f"ğŸ“Š [Statistics RVOL] í‰ê·  ê±°ë˜ëŸ‰ ê³„ì‚° - "
            f"average_monthly_volume: {average_monthly_volume}"
        )
        
        # RVOL ë°ì´í„° ìƒì„± (ì›”ë³„) - í˜„ì¬ ë‹¬ ì œì™¸
        rvol_data = []
        current_year = today.year
        current_month = today.month
        
        for row in current_rows:
            year = int(row.year)
            month = int(row.month)
            
            # í˜„ì¬ ë‹¬ ì œì™¸
            if year == current_year and month == current_month:
                continue
                
            count = row.count or 0
            
            # RVOL ê³„ì‚°
            rvol = count / average_monthly_volume if average_monthly_volume > 0 else 0
            
            rvol_data.append(
                RVOLDataPoint(
                    date=f"{year}-{month:02d}-01",
                    current_volume=count,
                    average_volume=round(average_monthly_volume, 2),
                    rvol=round(rvol, 2)
                )
            )
        
        # ë‚ ì§œìˆœ ì •ë ¬
        rvol_data.sort(key=lambda x: x.date)
        
        period_description = f"ìµœê·¼ {current_period_months}ê°œì›” vs ì§ì „ {average_period_months}ê°œì›”"
        
        response_data = RVOLResponse(
            success=True,
            data=rvol_data,
            period=period_description
        )
        
        # ìºì‹œì— ì €ì¥ (TTL: 6ì‹œê°„)
        if len(rvol_data) > 0:
            await set_to_cache(cache_key, response_data.dict(), ttl=STATISTICS_CACHE_TTL)
        
        logger.info(f"âœ… [Statistics RVOL] RVOL ë°ì´í„° ìƒì„± ì™„ë£Œ - ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(rvol_data)}")
        
        return response_data
        
    except Exception as e:
        logger.error(f"âŒ [Statistics RVOL] RVOL ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"RVOL ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/quadrant",
    response_model=QuadrantResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="4ë¶„ë©´ ë¶„ë¥˜ ì¡°íšŒ",
    description="""
    ë§¤ë§¤ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨ê³¼ ì „ì›”ì„¸ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨ì„ ê¸°ë°˜ìœ¼ë¡œ 4ë¶„ë©´ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    ### 4ë¶„ë©´ ë¶„ë¥˜
    - **xì¶•**: ë§¤ë§¤ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨
    - **yì¶•**: ì „ì›”ì„¸ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨
    
    ### í•´ì„
    1. **ë§¤ë§¤â†‘ / ì „ì›”ì„¸â†“**: ë§¤ìˆ˜ ì „í™˜ (ì‚¬ëŠ” ìª½ìœ¼ë¡œ ì´ë™)
    2. **ë§¤ë§¤â†“ / ì „ì›”ì„¸â†‘**: ì„ëŒ€ ì„ í˜¸/ê´€ë§ (ë¹Œë¦¬ëŠ” ìª½ìœ¼ë¡œ ì´ë™)
    3. **ë§¤ë§¤â†“ / ì „ì›”ì„¸â†“**: ì‹œì¥ ìœ„ì¶• (ì „ì²´ ìœ ë™ì„± ê²½ìƒ‰)
    4. **ë§¤ë§¤â†‘ / ì „ì›”ì„¸â†‘**: í™œì„±í™” (ìˆ˜ìš” ìì²´ê°€ ê°•í•¨, ì´ì‚¬/ê±°ë˜ ì¦ê°€)
    
    ### Query Parameters
    - `period_months`: ë¹„êµ ê¸°ê°„ (ê°œì›”, ê¸°ë³¸ê°’: 2, ìµœëŒ€: 6)
    """
)
async def get_quadrant(
    period_months: int = Query(2, ge=1, le=12, description="ë¹„êµ ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 12)"),
    db: AsyncSession = Depends(get_db)
):
    """
    4ë¶„ë©´ ë¶„ë¥˜ ì¡°íšŒ - ì„±ëŠ¥ ìµœì í™” ë²„ì „
    
    ì›”ë³„ ì§‘ê³„ë¡œ ê°„ì†Œí™”í•˜ì—¬ ë¹ ë¥¸ ì‘ë‹µ ì œê³µ
    """
    cache_key = build_cache_key("statistics", "quadrant_v2", str(period_months))
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        logger.info(f"âœ… [Statistics Quadrant] ìºì‹œì—ì„œ ë°˜í™˜")
        return cached_data
    
    try:
        logger.info(
            f"ğŸ” [Statistics Quadrant] 4ë¶„ë©´ ë¶„ë¥˜ ë°ì´í„° ì¡°íšŒ ì‹œì‘ - "
            f"period_months: {period_months}"
        )
        
        # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ê¸°ê°„ ì„¤ì •
        today = date.today()
        # í˜„ì¬ ë‹¬ì˜ ì²« ë‚  (í˜„ì¬ ë‹¬ ì œì™¸)
        current_month_start = date(today.year, today.month, 1)
        
        # ìµœê·¼ ê¸°ê°„ê³¼ ì´ì „ ê¸°ê°„ ì„¤ì • (í˜„ì¬ ë‹¬ ì œì™¸)
        recent_start = current_month_start - timedelta(days=period_months * 30)
        recent_end = current_month_start  # í˜„ì¬ ë‹¬ì˜ ì²« ë‚  ì „ê¹Œì§€
        
        previous_start = recent_start - timedelta(days=period_months * 30)
        previous_end = recent_start
        
        logger.info(
            f"ğŸ“… [Statistics Quadrant] ë‚ ì§œ ë²”ìœ„ - "
            f"previous_start: {previous_start}, previous_end: {previous_end}, "
            f"recent_start: {recent_start}, recent_end: {recent_end}"
        )
        
        # ì›”ë³„ ì§‘ê³„ (to_char ëŒ€ì‹  extract ì‚¬ìš© - ì¸ë±ìŠ¤ í™œìš© ê°€ëŠ¥)
        # ë§¤ë§¤ ê±°ë˜ëŸ‰: ì´ì „ ê¸°ê°„
        sale_previous_stmt = (
            select(
                extract('year', Sale.contract_date).label('year'),
                extract('month', Sale.contract_date).label('month'),
                func.count(Sale.trans_id).label('count')
            )
            .where(
                and_(
                    Sale.is_canceled == False,
                    (Sale.is_deleted == False) | (Sale.is_deleted.is_(None)),
                    Sale.contract_date.isnot(None),
                    Sale.contract_date >= previous_start,
                    Sale.contract_date < previous_end,
                    or_(Sale.remarks != "ë”ë¯¸", Sale.remarks.is_(None))
                )
            )
            .group_by(extract('year', Sale.contract_date), extract('month', Sale.contract_date))
        )
        
        # ë§¤ë§¤ ê±°ë˜ëŸ‰: ìµœê·¼ ê¸°ê°„
        sale_recent_stmt = (
            select(
                extract('year', Sale.contract_date).label('year'),
                extract('month', Sale.contract_date).label('month'),
                func.count(Sale.trans_id).label('count')
            )
            .where(
                and_(
                    Sale.is_canceled == False,
                    (Sale.is_deleted == False) | (Sale.is_deleted.is_(None)),
                    Sale.contract_date.isnot(None),
                    Sale.contract_date >= recent_start,
                    Sale.contract_date < recent_end,  # í˜„ì¬ ë‹¬ ì œì™¸ (ë¯¸ë§Œìœ¼ë¡œ ë³€ê²½)
                    or_(Sale.remarks != "ë”ë¯¸", Sale.remarks.is_(None))
                )
            )
            .group_by(extract('year', Sale.contract_date), extract('month', Sale.contract_date))
        )
        
        # ì „ì›”ì„¸ ê±°ë˜ëŸ‰: ì´ì „ ê¸°ê°„
        rent_previous_stmt = (
            select(
                extract('year', Rent.deal_date).label('year'),
                extract('month', Rent.deal_date).label('month'),
                func.count(Rent.trans_id).label('count')
            )
            .where(
                and_(
                    (Rent.is_deleted == False) | (Rent.is_deleted.is_(None)),
                    Rent.deal_date.isnot(None),
                    Rent.deal_date >= previous_start,
                    Rent.deal_date < previous_end,
                    or_(Rent.remarks != "ë”ë¯¸", Rent.remarks.is_(None))
                )
            )
            .group_by(extract('year', Rent.deal_date), extract('month', Rent.deal_date))
        )
        
        # ì „ì›”ì„¸ ê±°ë˜ëŸ‰: ìµœê·¼ ê¸°ê°„
        rent_recent_stmt = (
            select(
                extract('year', Rent.deal_date).label('year'),
                extract('month', Rent.deal_date).label('month'),
                func.count(Rent.trans_id).label('count')
            )
            .where(
                and_(
                    (Rent.is_deleted == False) | (Rent.is_deleted.is_(None)),
                    Rent.deal_date.isnot(None),
                    Rent.deal_date >= recent_start,
                    Rent.deal_date < recent_end,  # í˜„ì¬ ë‹¬ ì œì™¸ (ë¯¸ë§Œìœ¼ë¡œ ë³€ê²½)
                    or_(Rent.remarks != "ë”ë¯¸", Rent.remarks.is_(None))
                )
            )
            .group_by(extract('year', Rent.deal_date), extract('month', Rent.deal_date))
        )
        
        # ì¿¼ë¦¬ ë³‘ë ¬ ì‹¤í–‰ (ì„±ëŠ¥ ìµœì í™”)
        sale_previous_result, sale_recent_result, rent_previous_result, rent_recent_result = await asyncio.gather(
            db.execute(sale_previous_stmt),
            db.execute(sale_recent_stmt),
            db.execute(rent_previous_stmt),
            db.execute(rent_recent_stmt)
        )
        
        sale_previous_rows = sale_previous_result.fetchall()
        sale_recent_rows = sale_recent_result.fetchall()
        rent_previous_rows = rent_previous_result.fetchall()
        rent_recent_rows = rent_recent_result.fetchall()
        
        # ì´ì „ ê¸°ê°„ í‰ê·  ê³„ì‚°
        sale_previous_total = sum(row.count for row in sale_previous_rows) if sale_previous_rows else 0
        rent_previous_total = sum(row.count for row in rent_previous_rows) if rent_previous_rows else 0
        
        sale_previous_avg = sale_previous_total / len(sale_previous_rows) if sale_previous_rows else 1
        rent_previous_avg = rent_previous_total / len(rent_previous_rows) if rent_previous_rows else 1
        
        # ìµœê·¼ ê¸°ê°„ ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        sale_recent_dict = {f"{int(row.year)}-{int(row.month):02d}": row.count for row in sale_recent_rows}
        rent_recent_dict = {f"{int(row.year)}-{int(row.month):02d}": row.count for row in rent_recent_rows}
        
        # ëª¨ë“  ê¸°ê°„ ìˆ˜ì§‘ (í˜„ì¬ ë‹¬ ì œì™¸)
        all_periods = set(sale_recent_dict.keys()) | set(rent_recent_dict.keys())
        current_year = today.year
        current_month = today.month
        current_period_key = f"{current_year}-{current_month:02d}"
        
        # í˜„ì¬ ë‹¬ ì œì™¸
        all_periods.discard(current_period_key)
        
        quadrant_data = []
        quadrant_counts = {1: 0, 2: 0, 3: 0, 4: 0}
        
        for period in sorted(all_periods):
            sale_recent_count = sale_recent_dict.get(period, 0)
            rent_recent_count = rent_recent_dict.get(period, 0)
            
            # ë³€í™”ìœ¨ ê³„ì‚°
            sale_change_rate = ((sale_recent_count - sale_previous_avg) / sale_previous_avg * 100) if sale_previous_avg > 0 else 0
            rent_change_rate = ((rent_recent_count - rent_previous_avg) / rent_previous_avg * 100) if rent_previous_avg > 0 else 0
            
            # 4ë¶„ë©´ ë¶„ë¥˜
            quadrant_num, quadrant_label = calculate_quadrant(sale_change_rate, rent_change_rate)
            
            if quadrant_num > 0:
                quadrant_counts[quadrant_num] = quadrant_counts.get(quadrant_num, 0) + 1
            
            quadrant_data.append(
                QuadrantDataPoint(
                    date=period,
                    sale_volume_change_rate=round(sale_change_rate, 2),
                    rent_volume_change_rate=round(rent_change_rate, 2),
                    quadrant=quadrant_num,
                    quadrant_label=quadrant_label
                )
            )
        
        summary = {
            "total_periods": len(quadrant_data),
            "quadrant_distribution": quadrant_counts,
            "sale_previous_avg": round(sale_previous_avg, 2),
            "rent_previous_avg": round(rent_previous_avg, 2)
        }
        
        response_data = QuadrantResponse(
            success=True,
            data=quadrant_data,
            summary=summary
        )
        
        # ìºì‹œì— ì €ì¥ (TTL: 6ì‹œê°„)
        if len(quadrant_data) > 0:
            await set_to_cache(cache_key, response_data.dict(), ttl=STATISTICS_CACHE_TTL)
        
        logger.info(f"âœ… [Statistics Quadrant] 4ë¶„ë©´ ë¶„ë¥˜ ë°ì´í„° ìƒì„± ì™„ë£Œ - ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(quadrant_data)}")
        
        return response_data
        
    except Exception as e:
        logger.error(f"âŒ [Statistics Quadrant] 4ë¶„ë©´ ë¶„ë¥˜ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"4ë¶„ë©´ ë¶„ë¥˜ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/hpi",
    response_model=HPIResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="ì£¼íƒê°€ê²©ì§€ìˆ˜(HPI) ì¡°íšŒ",
    description="""
    ì£¼íƒê°€ê²©ì§€ìˆ˜(Housing Price Index, HPI)ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ### ì£¼íƒê°€ê²©ì§€ìˆ˜ë€?
    íŠ¹ì • ì‹œì ì˜ ì£¼íƒ ê°€ê²©ì„ ê¸°ì¤€(100)ìœ¼ë¡œ ì¡ê³ , ì´í›„ ê°€ê²©ì´ ì–¼ë§ˆë‚˜ ë³€í–ˆëŠ”ì§€ë¥¼ ìˆ˜ì¹˜í™”í•œ í†µê³„ ì§€í‘œì…ë‹ˆë‹¤.
    
    ### ì§€ìˆ˜ í•´ì„
    - **ì§€ìˆ˜ > 100**: ê¸°ì¤€ ì‹œì ë³´ë‹¤ ì§‘ê°’ì´ ì˜¬ëìŒ
    - **ì§€ìˆ˜ = 100**: ê¸°ì¤€ ì‹œì ê³¼ ë™ì¼
    - **ì§€ìˆ˜ < 100**: ê¸°ì¤€ ì‹œì ë³´ë‹¤ ì§‘ê°’ì´ ë‚´ë ¸ìŒ
    
    ### Query Parameters
    - `region_id`: ì§€ì—­ ID (ì„ íƒ, ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì „ì²´ ì§€ì—­ í‰ê· )
    - `index_type`: ì§€ìˆ˜ ìœ í˜• (APT: ì•„íŒŒíŠ¸, HOUSE: ë‹¨ë…ì£¼íƒ, ALL: ì „ì²´, ê¸°ë³¸ê°’: APT)
    - `months`: ì¡°íšŒ ê¸°ê°„ (ê°œì›”, ê¸°ë³¸ê°’: 24, ìµœëŒ€: 60)
    """
)
async def get_hpi(
    region_id: Optional[int] = Query(None, description="ì§€ì—­ ID (ì„ íƒ)"),
    index_type: str = Query("APT", description="ì§€ìˆ˜ ìœ í˜•: APT(ì•„íŒŒíŠ¸), HOUSE(ë‹¨ë…ì£¼íƒ), ALL(ì „ì²´)"),
    months: int = Query(24, ge=1, le=60, description="ì¡°íšŒ ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 60)"),
    db: AsyncSession = Depends(get_db)
):
    """
    ì£¼íƒê°€ê²©ì§€ìˆ˜(HPI) ì¡°íšŒ
    
    ì§€ì—­ë³„ ì£¼íƒê°€ê²©ì§€ìˆ˜ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    # ìœ íš¨í•œ index_type ê²€ì¦
    valid_index_types = ["APT", "HOUSE", "ALL"]
    if index_type not in valid_index_types:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ index_typeì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê°’: {', '.join(valid_index_types)}"
        )
    
    cache_key = build_cache_key(
        "statistics", "hpi", 
        str(region_id) if region_id else "all",
        index_type,
        str(months)
    )
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        logger.info(f"âœ… [Statistics HPI] ìºì‹œì—ì„œ ë°˜í™˜")
        return cached_data
    
    try:
        logger.info(
            f"ğŸ” [Statistics HPI] HPI ë°ì´í„° ì¡°íšŒ ì‹œì‘ - "
            f"region_id: {region_id}, index_type: {index_type}, months: {months}"
        )
        
        # ê¸°ì¤€ ë‚ ì§œ ê³„ì‚° (í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ìµœê·¼ monthsê°œì›”)
        today = date.today()
        # base_ymì€ YYYYMM í˜•ì‹ì´ë¯€ë¡œ, í˜„ì¬ ë…„ì›”ì„ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
        current_year = today.year
        current_month = today.month
        
        # ìµœì†Œ base_ym ê³„ì‚° (monthsê°œì›” ì „)
        # ì›” ë‹¨ìœ„ë¡œ ê³„ì‚°
        total_months = current_year * 12 + current_month - 1
        start_total_months = total_months - months + 1  # í˜„ì¬ ë‹¬ í¬í•¨
        start_year = start_total_months // 12
        start_month = (start_total_months % 12) + 1
        
        start_base_ym = f"{start_year:04d}{start_month:02d}"
        end_base_ym = f"{current_year:04d}{current_month:02d}"
        
        logger.info(
            f"ğŸ“… [Statistics HPI] ë‚ ì§œ ë²”ìœ„ - "
            f"start_base_ym: {start_base_ym}, end_base_ym: {end_base_ym}"
        )
        
        # ì¿¼ë¦¬ êµ¬ì„±
        # region_idê°€ ì§€ì •ëœ ê²½ìš°: íŠ¹ì • ì§€ì—­ë§Œ ì¡°íšŒ
        if region_id is not None:
            query = (
                select(
                    HouseScore.base_ym,
                    HouseScore.index_value,
                    HouseScore.index_change_rate,
                    HouseScore.index_type,
                    State.city_name.label('region_name')  # ì‹œë„ëª… ì‚¬ìš©
                )
                .join(State, HouseScore.region_id == State.region_id)
                .where(
                    and_(
                        HouseScore.region_id == region_id,
                        HouseScore.is_deleted == False,
                        HouseScore.index_type == index_type,
                        HouseScore.base_ym >= start_base_ym,
                        HouseScore.base_ym <= end_base_ym
                    )
                )
                .order_by(HouseScore.base_ym)
            )
        else:
            # region_idê°€ ì—†ëŠ” ê²½ìš°: ì‹œë„(city_name) ë ˆë²¨ë¡œ ê·¸ë£¹í™” (ì¸êµ¬ ì´ë™ ë°ì´í„°ì™€ ë™ì¼í•œ ë ˆë²¨)
            query = (
                select(
                    HouseScore.base_ym,
                    func.avg(HouseScore.index_value).label('index_value'),
                    func.avg(HouseScore.index_change_rate).label('index_change_rate'),
                    func.max(HouseScore.index_type).label('index_type'),
                    State.city_name.label('region_name')  # ì‹œë„ëª…ìœ¼ë¡œ ê·¸ë£¹í™”
                )
                .join(State, HouseScore.region_id == State.region_id)
                .where(
                    and_(
                        HouseScore.is_deleted == False,
                        State.is_deleted == False,
                        HouseScore.index_type == index_type,
                        HouseScore.base_ym >= start_base_ym,
                        HouseScore.base_ym <= end_base_ym
                    )
                )
                .group_by(HouseScore.base_ym, State.city_name)
                .order_by(HouseScore.base_ym, State.city_name)
            )
        
        result = await db.execute(query)
        rows = result.fetchall()
        
        logger.info(
            f"ğŸ“Š [Statistics HPI] ì¿¼ë¦¬ ê²°ê³¼ - "
            f"ì´ {len(rows)}ê±´ ì¡°íšŒë¨"
        )
        
        # ì‹œë„ë³„ ë°ì´í„° ê°œìˆ˜ í™•ì¸
        if rows:
            region_counts = {}
            for row in rows:
                region_name = row.region_name if hasattr(row, 'region_name') and row.region_name else "Unknown"
                region_counts[region_name] = region_counts.get(region_name, 0) + 1
            
            logger.info(
                f"ğŸ“‹ [Statistics HPI] ì‹œë„ë³„ ë°ì´í„° ê°œìˆ˜ - "
                f"{', '.join([f'{k}: {v}ê±´' for k, v in sorted(region_counts.items())])}"
            )
        
        # ë°ì´í„° í¬ì¸íŠ¸ ìƒì„±
        hpi_data = []
        for row in rows:
            base_ym = row.base_ym
            # YYYYMM -> YYYY-MM í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            year = base_ym[:4]
            month = base_ym[4:6]
            date_str = f"{year}-{month}"
            
            index_value = float(row.index_value) if row.index_value is not None else 0.0
            index_change_rate = float(row.index_change_rate) if row.index_change_rate is not None else None
            
            # region_name ì²˜ë¦¬: ì‹œë„ëª…(city_name) ì‚¬ìš©
            region_name = row.region_name if hasattr(row, 'region_name') and row.region_name else None
            
            hpi_data.append(
                HPIDataPoint(
                    date=date_str,
                    index_value=round(index_value, 2),
                    index_change_rate=round(index_change_rate, 2) if index_change_rate is not None else None,
                    region_name=region_name,
                    index_type=index_type
                )
            )
        
        # ë‚ ì§œìˆœ ì •ë ¬ (ì´ë¯¸ ì •ë ¬ë˜ì–´ ìˆì§€ë§Œ í™•ì‹¤íˆ)
        hpi_data.sort(key=lambda x: x.date)
        
        # ì§€ì—­ë³„/ë‚ ì§œë³„ ë°ì´í„° ê°œìˆ˜ í™•ì¸
        if hpi_data:
            date_counts = {}
            region_date_counts = {}
            for item in hpi_data:
                date_counts[item.date] = date_counts.get(item.date, 0) + 1
                if item.region_name:
                    key = f"{item.region_name}-{item.date}"
                    region_date_counts[key] = region_date_counts.get(key, 0) + 1
            
            logger.info(
                f"ğŸ“ˆ [Statistics HPI] ë°ì´í„° í¬ì¸íŠ¸ ìƒì„¸ - "
                f"ì´ {len(hpi_data)}ê±´, "
                f"ë‚ ì§œë³„ ê°œìˆ˜: {dict(sorted(date_counts.items())[:5])}... (ìµœì‹  5ê°œë§Œ í‘œì‹œ), "
                f"ì‹œë„ ìˆ˜: {len(set(item.region_name for item in hpi_data if item.region_name))}ê°œ"
            )
            
            # ê° ì‹œë„ë³„ ìµœì‹  ë°ì´í„° ìƒ˜í”Œ ë¡œê¹…
            latest_by_region = {}
            for item in reversed(hpi_data):  # ìµœì‹ ë¶€í„°
                if item.region_name and item.region_name not in latest_by_region:
                    latest_by_region[item.region_name] = item
            
            if latest_by_region:
                sample_regions = list(latest_by_region.items())[:5]  # ìµœëŒ€ 5ê°œë§Œ
                logger.info(
                    f"ğŸ“ [Statistics HPI] ì‹œë„ë³„ ìµœì‹  ë°ì´í„° ìƒ˜í”Œ - "
                    f"{', '.join([f'{r}: {d.date} {d.index_value}' for r, d in sample_regions])}"
                )
        
        region_desc = f"ì§€ì—­ ID {region_id}" if region_id else "ì „ì²´ ì§€ì—­ í‰ê· "
        period_desc = f"{months}ê°œì›” ({hpi_data[0].date if hpi_data else 'N/A'} ~ {hpi_data[-1].date if hpi_data else 'N/A'})"
        
        response_data = HPIResponse(
            success=True,
            data=hpi_data,
            region_id=region_id,
            index_type=index_type,
            period=f"{region_desc}, {index_type}, {period_desc}"
        )
        
        # ìºì‹œì— ì €ì¥ (TTL: 6ì‹œê°„)
        if len(hpi_data) > 0:
            await set_to_cache(cache_key, response_data.dict(), ttl=STATISTICS_CACHE_TTL)
        
        logger.info(f"âœ… [Statistics HPI] HPI ë°ì´í„° ìƒì„± ì™„ë£Œ - ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(hpi_data)}")
        
        return response_data
        
    except Exception as e:
        logger.error(f"âŒ [Statistics HPI] HPI ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"HPI ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/hpi/heatmap",
    response_model=HPIHeatmapResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="ì£¼íƒê°€ê²©ì§€ìˆ˜(HPI) íˆíŠ¸ë§µ ì¡°íšŒ",
    description="""
    ê´‘ì—­ì‹œ/íŠ¹ë³„ì‹œ/ë„ë³„ ì£¼íƒê°€ê²©ì§€ìˆ˜ë¥¼ íˆíŠ¸ë§µ í˜•ì‹ìœ¼ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ê° ë„/ì‹œì˜ ìµœì‹  HPI ê°’ì„ ë°˜í™˜í•˜ì—¬ ì§€ì—­ë³„ ê°€ê²© ì¶”ì´ë¥¼ í•œëˆˆì— ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    ### Query Parameters
    - `index_type`: ì§€ìˆ˜ ìœ í˜• (APT: ì•„íŒŒíŠ¸, HOUSE: ë‹¨ë…ì£¼íƒ, ALL: ì „ì²´, ê¸°ë³¸ê°’: APT)
    """
)
async def get_hpi_heatmap(
    index_type: str = Query("APT", description="ì§€ìˆ˜ ìœ í˜•: APT(ì•„íŒŒíŠ¸), HOUSE(ë‹¨ë…ì£¼íƒ), ALL(ì „ì²´)"),
    db: AsyncSession = Depends(get_db)
):
    """
    ì£¼íƒê°€ê²©ì§€ìˆ˜(HPI) íˆíŠ¸ë§µ ì¡°íšŒ
    
    ë„/ì‹œë³„ ìµœì‹  HPI ê°’ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    # ìœ íš¨í•œ index_type ê²€ì¦
    valid_index_types = ["APT", "HOUSE", "ALL"]
    if index_type not in valid_index_types:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ index_typeì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê°’: {', '.join(valid_index_types)}"
        )
    
    cache_key = build_cache_key("statistics", "hpi_heatmap", index_type)
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        logger.info(f"âœ… [Statistics HPI Heatmap] ìºì‹œì—ì„œ ë°˜í™˜")
        return cached_data
    
    try:
        logger.info(
            f"ğŸ” [Statistics HPI Heatmap] HPI íˆíŠ¸ë§µ ë°ì´í„° ì¡°íšŒ ì‹œì‘ - "
            f"index_type: {index_type}"
        )
        
        # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ìµœì‹  base_ym ì°¾ê¸°
        today = date.today()
        current_year = today.year
        current_month = today.month
        current_base_ym = f"{current_year:04d}{current_month:02d}"
        
        # ìµœì‹  base_ymë¶€í„° ì—­ìˆœìœ¼ë¡œ ì°¾ê¸° (ìµœëŒ€ 12ê°œì›” ì „ê¹Œì§€)
        found_base_ym = None
        for i in range(12):
            check_year = current_year
            check_month = current_month - i
            if check_month <= 0:
                check_year -= 1
                check_month += 12
            check_base_ym = f"{check_year:04d}{check_month:02d}"
            
            # í•´ë‹¹ base_ymì— ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
            check_query = (
                select(func.count(HouseScore.index_id))
                .where(
                    and_(
                        HouseScore.is_deleted == False,
                        HouseScore.index_type == index_type,
                        HouseScore.base_ym == check_base_ym
                    )
                )
            )
            check_result = await db.execute(check_query)
            count = check_result.scalar() or 0
            
            if count > 0:
                found_base_ym = check_base_ym
                break
        
        if not found_base_ym:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="HPI ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        logger.info(f"ğŸ“… [Statistics HPI Heatmap] ì‚¬ìš©í•  base_ym: {found_base_ym}")
        
        # ë„/ì‹œë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í‰ê·  HPI ê³„ì‚°
        query = (
            select(
                State.city_name,
                func.avg(HouseScore.index_value).label('index_value'),
                func.avg(HouseScore.index_change_rate).label('index_change_rate'),
                func.count(HouseScore.index_id).label('region_count')
            )
            .join(State, HouseScore.region_id == State.region_id)
            .where(
                and_(
                    HouseScore.is_deleted == False,
                    State.is_deleted == False,
                    HouseScore.index_type == index_type,
                    HouseScore.base_ym == found_base_ym
                )
            )
            .group_by(State.city_name)
            .order_by(State.city_name)
        )
        
        result = await db.execute(query)
        rows = result.fetchall()
        
        # ë°ì´í„° í¬ì¸íŠ¸ ìƒì„±
        heatmap_data = []
        for row in rows:
            city_name = row.city_name
            index_value = float(row.index_value) if row.index_value is not None else 0.0
            index_change_rate = float(row.index_change_rate) if row.index_change_rate is not None else None
            region_count = int(row.region_count) if row.region_count else 0
            
            heatmap_data.append(
                HPIHeatmapDataPoint(
                    city_name=city_name,
                    index_value=round(index_value, 2),
                    index_change_rate=round(index_change_rate, 2) if index_change_rate is not None else None,
                    base_ym=found_base_ym,
                    region_count=region_count
                )
            )
        
        # ë„/ì‹œëª… ìˆœìœ¼ë¡œ ì •ë ¬
        heatmap_data.sort(key=lambda x: x.city_name)
        
        response_data = HPIHeatmapResponse(
            success=True,
            data=heatmap_data,
            index_type=index_type,
            base_ym=found_base_ym
        )
        
        # ìºì‹œì— ì €ì¥ (TTL: 6ì‹œê°„)
        if len(heatmap_data) > 0:
            await set_to_cache(cache_key, response_data.dict(), ttl=STATISTICS_CACHE_TTL)
        
        logger.info(f"âœ… [Statistics HPI Heatmap] HPI íˆíŠ¸ë§µ ë°ì´í„° ìƒì„± ì™„ë£Œ - ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(heatmap_data)}")
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [Statistics HPI Heatmap] HPI íˆíŠ¸ë§µ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"HPI íˆíŠ¸ë§µ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/summary",
    response_model=StatisticsSummaryResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="í†µê³„ ìš”ì•½ ì¡°íšŒ",
    description="""
    RVOLê³¼ 4ë¶„ë©´ ë¶„ë¥˜ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¡°íšŒí•©ë‹ˆë‹¤.
    """
)
async def get_statistics_summary(
    transaction_type: str = Query("sale", description="ê±°ë˜ ìœ í˜•: sale(ë§¤ë§¤), rent(ì „ì›”ì„¸)"),
    current_period_months: int = Query(6, ge=1, le=12, description="í˜„ì¬ ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 12)"),
    average_period_months: int = Query(6, ge=1, le=12, description="í‰ê·  ê³„ì‚° ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 12)"),
    quadrant_period_months: int = Query(2, ge=1, le=12, description="4ë¶„ë©´ ë¹„êµ ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 12)"),
    db: AsyncSession = Depends(get_db)
):
    """
    í†µê³„ ìš”ì•½ ì¡°íšŒ
    
    RVOLê³¼ 4ë¶„ë©´ ë¶„ë¥˜ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    # RVOLê³¼ 4ë¶„ë©´ ë¶„ë¥˜ë¥¼ ë³‘ë ¬ë¡œ ì¡°íšŒ
    rvol_task = get_rvol(transaction_type, current_period_months, average_period_months, db)
    quadrant_task = get_quadrant(quadrant_period_months, db)
    
    rvol_response, quadrant_response = await asyncio.gather(rvol_task, quadrant_task)
    
    return StatisticsSummaryResponse(
        success=True,
        rvol=rvol_response,
        quadrant=quadrant_response
    )


@router.get(
    "/population-movements",
    response_model=PopulationMovementResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="ì¸êµ¬ ì´ë™ ë°ì´í„° ì¡°íšŒ",
    description="""
    ì§€ì—­ë³„ ì¸êµ¬ ì´ë™ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ### Query Parameters
    - `region_id`: ì§€ì—­ ID (ì„ íƒ, ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì „ì²´)
    - `start_ym`: ì‹œì‘ ë…„ì›” (YYYYMM, ê¸°ë³¸ê°’: ìµœê·¼ 12ê°œì›”)
    - `end_ym`: ì¢…ë£Œ ë…„ì›” (YYYYMM, ê¸°ë³¸ê°’: í˜„ì¬)
    """
)
async def get_population_movements(
    region_id: Optional[int] = Query(None, description="ì§€ì—­ ID (ì„ íƒ)"),
    start_ym: Optional[str] = Query(None, description="ì‹œì‘ ë…„ì›” (YYYYMM)"),
    end_ym: Optional[str] = Query(None, description="ì¢…ë£Œ ë…„ì›” (YYYYMM)"),
    db: AsyncSession = Depends(get_db)
):
    """
    ì¸êµ¬ ì´ë™ ë°ì´í„° ì¡°íšŒ
    """
    try:
        # ê¸°ë³¸ ê¸°ê°„ ì„¤ì • (ìµœê·¼ 12ê°œì›”)
        if not end_ym:
            end_date = datetime.now()
            end_ym = end_date.strftime("%Y%m")
        
        if not start_ym:
            start_date = datetime.now() - timedelta(days=365)
            start_ym = start_date.strftime("%Y%m")
        
        # ì¿¼ë¦¬ êµ¬ì„±: ì‹œë„ ë ˆë²¨ ë°ì´í„°ë§Œ ì¡°íšŒ (city_name ì‚¬ìš©)
        query = select(
            PopulationMovement,
            State.city_name  # ì‹œë„ëª… ì‚¬ìš© (ì˜ˆ: ì„œìš¸íŠ¹ë³„ì‹œ, ë¶€ì‚°ê´‘ì—­ì‹œ)
        ).join(
            State, PopulationMovement.region_id == State.region_id
        ).where(
            and_(
                PopulationMovement.base_ym >= start_ym,
                PopulationMovement.base_ym <= end_ym,
                PopulationMovement.is_deleted == False
            )
        )
        
        if region_id:
            query = query.where(PopulationMovement.region_id == region_id)
        
        query = query.order_by(PopulationMovement.base_ym.desc())
        
        result = await db.execute(query)
        rows = result.all()
        
        logger.info(
            f"ğŸ“Š [Statistics Population Movement] ì¸êµ¬ ì´ë™ ë°ì´í„° ì¡°íšŒ - "
            f"ì´ {len(rows)}ê±´ ì¡°íšŒë¨"
        )
        
        # ì§€ì—­ë³„ ë°ì´í„° ê°œìˆ˜ í™•ì¸
        if rows:
            region_counts = {}
            region_net_totals = {}  # ì§€ì—­ë³„ ìˆœì´ë™ í•©ê³„
            for movement, city_name in rows:
                region_name = city_name or "Unknown"
                region_counts[region_name] = region_counts.get(region_name, 0) + 1
                # ìˆœì´ë™ í•©ê³„ ê³„ì‚°
                if region_name not in region_net_totals:
                    region_net_totals[region_name] = 0
                region_net_totals[region_name] += movement.net_migration or 0
            
            logger.info(
                f"ğŸ“‹ [Statistics Population Movement] ì‹œë„ë³„ ë°ì´í„° ê°œìˆ˜ - "
                f"{', '.join([f'{k}: {v}ê±´' for k, v in sorted(region_counts.items())])}"
            )
            
            logger.info(
                f"ğŸ“Š [Statistics Population Movement] ì‹œë„ë³„ ìˆœì´ë™ í•©ê³„ - "
                f"{', '.join([f'{k}: {v}ëª…' for k, v in sorted(region_net_totals.items())])}"
            )
        
        data_points = []
        for movement, city_name in rows:
            # YYYYMM -> YYYY-MM ë³€í™˜
            year = movement.base_ym[:4]
            month = movement.base_ym[4:]
            date_str = f"{year}-{month}"
            
            data_points.append(PopulationMovementDataPoint(
                date=date_str,
                region_id=movement.region_id,
                region_name=city_name,  # ì‹œë„ëª… ë°˜í™˜
                in_migration=movement.in_migration,
                out_migration=movement.out_migration,
                net_migration=movement.net_migration
            ))
        
        return PopulationMovementResponse(
            success=True,
            data=data_points,
            period=f"{start_ym} ~ {end_ym}"
        )
        
    except Exception as e:
        logger.error(f"âŒ ì¸êµ¬ ì´ë™ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì¸êµ¬ ì´ë™ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        )
