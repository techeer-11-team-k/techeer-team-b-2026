"""
í†µê³„ ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸

ë‹´ë‹¹ ê¸°ëŠ¥:
- RVOL(ìƒëŒ€ ê±°ë˜ëŸ‰) ê³„ì‚° ë° ì¡°íšŒ
- 4ë¶„ë©´ ë¶„ë¥˜ (ë§¤ë§¤/ì „ì›”ì„¸ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨ ê¸°ë°˜)

ì„±ëŠ¥ ìµœì í™”:
- ê¸°ê°„ ì œí•œ: ìµœëŒ€ 2~3ê°œì›”
- ì›”ë³„ ì§‘ê³„ë¡œ ê°„ì†Œí™”
- ê¸´ ìºì‹œ TTL (6ì‹œê°„)
"""
import logging
import sys
import asyncio
from datetime import date, datetime, timedelta
from typing import Optional, List, Dict, Any
from fastapi import APIRouter, Depends, HTTPException, Query, status
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func, and_, or_, case, desc, text, extract
from sqlalchemy.orm import selectinload

from app.api.v1.deps import get_db
from app.models.sale import Sale
from app.models.rent import Rent
from app.models.apartment import Apartment
from app.models.state import State
from app.models.house_score import HouseScore
from app.models.population_movement import PopulationMovement
from app.schemas.statistics import (
    RVOLResponse,
    RVOLDataPoint,
    QuadrantResponse,
    QuadrantDataPoint,
    StatisticsSummaryResponse,
    HPIResponse,
    HPIDataPoint,
    HPIHeatmapResponse,
    HPIHeatmapDataPoint,
    PopulationMovementResponse,
    PopulationMovementDataPoint,
    PopulationMovementSankeyResponse,
    PopulationMovementSankeyDataPoint,
    CorrelationAnalysisResponse,
    HPIRegionTypeResponse,
    HPIRegionTypeDataPoint,
    TransactionVolumeResponse,
    TransactionVolumeDataPoint
)
from app.utils.cache import get_from_cache, set_to_cache, build_cache_key, delete_cache_pattern

# ë¡œê±° ì„¤ì • (Docker ë¡œê·¸ì— ì¶œë ¥ë˜ë„ë¡)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
if not logger.handlers:
    handler = logging.StreamHandler(sys.stdout)
    handler.setLevel(logging.INFO)
    formatter = logging.Formatter(
        '%(asctime)s | %(levelname)s | %(name)s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.propagate = True  # ë£¨íŠ¸ ë¡œê±°ë¡œë„ ì „íŒŒ

router = APIRouter()

# ìºì‹œ TTL: 6ì‹œê°„ (í†µê³„ ë°ì´í„°ëŠ” ìì£¼ ë³€í•˜ì§€ ì•ŠìŒ)
STATISTICS_CACHE_TTL = 21600


# ============================================================
# í—¬í¼ í•¨ìˆ˜
# ============================================================

def normalize_city_name(city_name: str) -> str:
    """
    ì‹œë„ëª…ì„ í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”
    
    Args:
        city_name: ì‹œë„ëª… (ì˜ˆ: "ì„œìš¸íŠ¹ë³„ì‹œ", "ë¶€ì‚°ê´‘ì—­ì‹œ")
    
    Returns:
        ì •ê·œí™”ëœ ì§€ì—­ëª… (ì˜ˆ: "ì„œìš¸", "ë¶€ì‚°")
    """
    mapping = {
        "ì„œìš¸íŠ¹ë³„ì‹œ": "ì„œìš¸",
        "ë¶€ì‚°ê´‘ì—­ì‹œ": "ë¶€ì‚°",
        "ëŒ€êµ¬ê´‘ì—­ì‹œ": "ëŒ€êµ¬",
        "ì¸ì²œê´‘ì—­ì‹œ": "ì¸ì²œ",
        "ê´‘ì£¼ê´‘ì—­ì‹œ": "ê´‘ì£¼",
        "ëŒ€ì „ê´‘ì—­ì‹œ": "ëŒ€ì „",
        "ìš¸ì‚°ê´‘ì—­ì‹œ": "ìš¸ì‚°",
        "ê²½ê¸°ë„": "ê²½ê¸°",
    }
    return mapping.get(city_name, city_name)


def normalize_metropolitan_region_name(city_name: str, region_name: str) -> str:
    """
    ìˆ˜ë„ê¶Œì˜ êµ¬ ë‹¨ìœ„ ì§€ì—­ëª…ì„ ì‹œ/êµ° ë‹¨ìœ„ë¡œ ì •ê·œí™”
    
    Args:
        city_name: ì‹œë„ëª… (ì˜ˆ: "ì„œìš¸íŠ¹ë³„ì‹œ", "ê²½ê¸°ë„", "ì¸ì²œê´‘ì—­ì‹œ")
        region_name: ì‹œêµ°êµ¬ëª… (ì˜ˆ: "ê°•ë‚¨êµ¬", "ìˆ˜ì›ì‹œ", "ê¶Œì„ êµ¬")
    
    Returns:
        ì •ê·œí™”ëœ ì‹œ/êµ°ëª… (ì˜ˆ: "ì„œìš¸", "ìˆ˜ì›", "ì¸ì²œ")
    """
    if not region_name:
        if city_name == "ì„œìš¸íŠ¹ë³„ì‹œ":
            return "ì„œìš¸"
        elif city_name == "ì¸ì²œê´‘ì—­ì‹œ":
            return "ì¸ì²œ"
        else:
            return city_name
    
    # êµ¬ ë‹¨ìœ„ë¥¼ ì‹œ/êµ° ë‹¨ìœ„ë¡œ ë§¤í•‘
    gu_to_city_map = {
        # ìˆ˜ì›ì‹œ êµ¬ë“¤
        "ê¶Œì„ êµ¬": "ìˆ˜ì›",
        "ì˜í†µêµ¬": "ìˆ˜ì›",
        "ì¥ì•ˆêµ¬": "ìˆ˜ì›",
        "íŒ”ë‹¬êµ¬": "ìˆ˜ì›",
        "ê¶Œì„ ": "ìˆ˜ì›",
        "ì˜í†µ": "ìˆ˜ì›",
        "ì¥ì•ˆ": "ìˆ˜ì›",
        "íŒ”ë‹¬": "ìˆ˜ì›",
        # ìš©ì¸ì‹œ êµ¬ë“¤ (ìˆ˜ì§€ëŠ” ìš©ì¸, ê¸°í¥ì€ ì‹œí¥ìœ¼ë¡œ ë§¤í•‘)
        "ìˆ˜ì§€êµ¬": "ìš©ì¸",
        "ì²˜ì¸êµ¬": "ìš©ì¸",
        "ìˆ˜ì§€": "ìš©ì¸",
        "ì²˜ì¸": "ìš©ì¸",
        # ê¸°í¥ â†’ ì‹œí¥ìœ¼ë¡œ ë§¤í•‘
        "ê¸°í¥êµ¬": "ì‹œí¥",
        "ê¸°í¥": "ì‹œí¥",
        # ì•ˆì‚°ì‹œ êµ¬ë“¤
        "ë‹¨ì›êµ¬": "ì•ˆì‚°",
        "ìƒë¡êµ¬": "ì•ˆì‚°",
        "ë‹¨ì›": "ì•ˆì‚°",
        "ìƒë¡": "ì•ˆì‚°",
        # ê³ ì–‘ì‹œ êµ¬ë“¤
        "ë•ì–‘êµ¬": "ê³ ì–‘",
        "ì¼ì‚°ë™êµ¬": "ê³ ì–‘",
        "ì¼ì‚°ì„œêµ¬": "ê³ ì–‘",
        "ë•ì–‘": "ê³ ì–‘",
        "ì¼ì‚°ë™": "ê³ ì–‘",
        "ì¼ì‚°ì„œ": "ê³ ì–‘",
        # ì•ˆì–‘ì‹œ êµ¬ë“¤
        "ë™ì•ˆêµ¬": "ì•ˆì–‘",
        "ë§Œì•ˆêµ¬": "ì•ˆì–‘",
        "ë™ì•ˆ": "ì•ˆì–‘",
        "ë§Œì•ˆ": "ì•ˆì–‘",
        # ì„±ë‚¨ì‹œ êµ¬ë“¤
        "ë¶„ë‹¹êµ¬": "ì„±ë‚¨",
        "ìˆ˜ì •êµ¬": "ì„±ë‚¨",
        "ì¤‘ì›êµ¬": "ì„±ë‚¨",
        "ë¶„ë‹¹": "ì„±ë‚¨",
        "ìˆ˜ì •": "ì„±ë‚¨",
        "ì¤‘ì›": "ì„±ë‚¨",
        # ë¶€ì²œì‹œ êµ¬ë“¤
        "ì†Œì‚¬êµ¬": "ë¶€ì²œ",
        "ì˜¤ì •êµ¬": "ë¶€ì²œ",
        "ì›ë¯¸êµ¬": "ë¶€ì²œ",
        "ì†Œì‚¬": "ë¶€ì²œ",
        "ì˜¤ì •": "ë¶€ì²œ",
        "ì›ë¯¸": "ë¶€ì²œ",
        # ë¶ˆì™„ì „í•œ ì´ë¦„ ë§¤í•‘
        "ë¦¬": "êµ¬ë¦¬",
        "í¬": "êµ°í¬",
    }
    
    # ì›ë³¸ region_nameì—ì„œ ì§ì ‘ ë§¤í•‘ í™•ì¸ (ë¶ˆì™„ì „í•œ ì´ë¦„ ì²˜ë¦¬)
    if region_name in gu_to_city_map:
        return gu_to_city_map[region_name]
    
    # "ë¶€ì²œ ì†Œì‚¬", "ë¶€ì²œ ì˜¤ì •", "ë¶€ì²œ ì›ë¯¸" ê°™ì€ í˜•ì‹ ì²˜ë¦¬
    if "ë¶€ì²œ" in region_name:
        # "ë¶€ì²œì‹œ ì†Œì‚¬êµ¬" ë˜ëŠ” "ë¶€ì²œ ì†Œì‚¬" í˜•ì‹ ì²˜ë¦¬
        parts = region_name.replace("ì‹œ", "").replace("êµ¬", "").split()
        if len(parts) > 1:
            gu_name = parts[1].strip()
            if gu_name in gu_to_city_map:
                return gu_to_city_map[gu_name]
        return "ë¶€ì²œ"
    
    # êµ¬ ë‹¨ìœ„ ë§¤í•‘ í™•ì¸
    normalized_region = region_name.replace("ì‹œ", "").replace("êµ°", "").replace("êµ¬", "").strip()
    
    # ì •ê·œí™”ëœ ì´ë¦„ìœ¼ë¡œ ë§¤í•‘ í™•ì¸
    if normalized_region in gu_to_city_map:
        return gu_to_city_map[normalized_region]
    
    # ì„œìš¸íŠ¹ë³„ì‹œì™€ ì¸ì²œê´‘ì—­ì‹œëŠ” ì‹œë„ëª…ë§Œ ì‚¬ìš©
    if city_name == "ì„œìš¸íŠ¹ë³„ì‹œ":
        return "ì„œìš¸"
    elif city_name == "ì¸ì²œê´‘ì—­ì‹œ":
        return "ì¸ì²œ"
    
    # ê²½ê¸°ë„: ì‹œ/êµ°ëª…ì—ì„œ "ì‹œ", "êµ°", "êµ¬" ì œê±°
    # ì´ë¯¸ ì •ê·œí™”ëœ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if normalized_region:
        return normalized_region
    
    return city_name


def normalize_metropolitan_region_name_without_fallback(city_name: str, region_name: str) -> str:
    """
    ìˆ˜ë„ê¶Œì˜ êµ¬ ë‹¨ìœ„ ì§€ì—­ëª…ì„ ì‹œ/êµ° ë‹¨ìœ„ë¡œ ì •ê·œí™” (ì˜ˆì™¸ì²˜ë¦¬ ì œì™¸ ë²„ì „)
    "ë¦¬", "í¬", "ê¸°í¥"ì€ ë§¤í•‘í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ë°˜í™˜
    
    Args:
        city_name: ì‹œë„ëª… (ì˜ˆ: "ì„œìš¸íŠ¹ë³„ì‹œ", "ê²½ê¸°ë„", "ì¸ì²œê´‘ì—­ì‹œ")
        region_name: ì‹œêµ°êµ¬ëª… (ì˜ˆ: "ê°•ë‚¨êµ¬", "ìˆ˜ì›ì‹œ", "ê¶Œì„ êµ¬")
    
    Returns:
        ì •ê·œí™”ëœ ì‹œ/êµ°ëª… (ì˜ˆ: "ì„œìš¸", "ìˆ˜ì›", "ì¸ì²œ") ë˜ëŠ” ì›ë³¸ ("ë¦¬", "í¬", "ê¸°í¥")
    """
    if not region_name:
        if city_name == "ì„œìš¸íŠ¹ë³„ì‹œ":
            return "ì„œìš¸"
        elif city_name == "ì¸ì²œê´‘ì—­ì‹œ":
            return "ì¸ì²œ"
        else:
            return city_name
    
    # êµ¬ ë‹¨ìœ„ë¥¼ ì‹œ/êµ° ë‹¨ìœ„ë¡œ ë§¤í•‘ (ë¦¬, í¬, ê¸°í¥ ì œì™¸)
    gu_to_city_map = {
        # ìˆ˜ì›ì‹œ êµ¬ë“¤
        "ê¶Œì„ êµ¬": "ìˆ˜ì›",
        "ì˜í†µêµ¬": "ìˆ˜ì›",
        "ì¥ì•ˆêµ¬": "ìˆ˜ì›",
        "íŒ”ë‹¬êµ¬": "ìˆ˜ì›",
        "ê¶Œì„ ": "ìˆ˜ì›",
        "ì˜í†µ": "ìˆ˜ì›",
        "ì¥ì•ˆ": "ìˆ˜ì›",
        "íŒ”ë‹¬": "ìˆ˜ì›",
        # ìš©ì¸ì‹œ êµ¬ë“¤ (ìˆ˜ì§€, ì²˜ì¸ë§Œ ìš©ì¸ìœ¼ë¡œ, ê¸°í¥ì€ ë§¤í•‘í•˜ì§€ ì•ŠìŒ)
        "ìˆ˜ì§€êµ¬": "ìš©ì¸",
        "ì²˜ì¸êµ¬": "ìš©ì¸",
        "ìˆ˜ì§€": "ìš©ì¸",
        "ì²˜ì¸": "ìš©ì¸",
        # ì•ˆì‚°ì‹œ êµ¬ë“¤
        "ë‹¨ì›êµ¬": "ì•ˆì‚°",
        "ìƒë¡êµ¬": "ì•ˆì‚°",
        "ë‹¨ì›": "ì•ˆì‚°",
        "ìƒë¡": "ì•ˆì‚°",
        # ê³ ì–‘ì‹œ êµ¬ë“¤
        "ë•ì–‘êµ¬": "ê³ ì–‘",
        "ì¼ì‚°ë™êµ¬": "ê³ ì–‘",
        "ì¼ì‚°ì„œêµ¬": "ê³ ì–‘",
        "ë•ì–‘": "ê³ ì–‘",
        "ì¼ì‚°ë™": "ê³ ì–‘",
        "ì¼ì‚°ì„œ": "ê³ ì–‘",
        # ì•ˆì–‘ì‹œ êµ¬ë“¤
        "ë™ì•ˆêµ¬": "ì•ˆì–‘",
        "ë§Œì•ˆêµ¬": "ì•ˆì–‘",
        "ë™ì•ˆ": "ì•ˆì–‘",
        "ë§Œì•ˆ": "ì•ˆì–‘",
        # ì„±ë‚¨ì‹œ êµ¬ë“¤
        "ë¶„ë‹¹êµ¬": "ì„±ë‚¨",
        "ìˆ˜ì •êµ¬": "ì„±ë‚¨",
        "ì¤‘ì›êµ¬": "ì„±ë‚¨",
        "ë¶„ë‹¹": "ì„±ë‚¨",
        "ìˆ˜ì •": "ì„±ë‚¨",
        "ì¤‘ì›": "ì„±ë‚¨",
        # ë¶€ì²œì‹œ êµ¬ë“¤
        "ì†Œì‚¬êµ¬": "ë¶€ì²œ",
        "ì˜¤ì •êµ¬": "ë¶€ì²œ",
        "ì›ë¯¸êµ¬": "ë¶€ì²œ",
        "ì†Œì‚¬": "ë¶€ì²œ",
        "ì˜¤ì •": "ë¶€ì²œ",
        "ì›ë¯¸": "ë¶€ì²œ",
    }
    
    # ì›ë³¸ region_name í™•ì¸
    normalized_region = region_name.replace("ì‹œ", "").replace("êµ°", "").replace("êµ¬", "").strip()
    
    # "ë¦¬", "í¬", "ê¸°í¥"ì€ ì˜ˆì™¸ì²˜ë¦¬ìš©ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if normalized_region == "ë¦¬" or normalized_region == "í¬" or normalized_region == "ê¸°í¥":
        return normalized_region
    
    # ì›ë³¸ region_nameì—ì„œ ì§ì ‘ ë§¤í•‘ í™•ì¸
    if region_name in gu_to_city_map:
        return gu_to_city_map[region_name]
    
    # "ë¶€ì²œ ì†Œì‚¬", "ë¶€ì²œ ì˜¤ì •", "ë¶€ì²œ ì›ë¯¸" ê°™ì€ í˜•ì‹ ì²˜ë¦¬
    if "ë¶€ì²œ" in region_name:
        parts = region_name.replace("ì‹œ", "").replace("êµ¬", "").split()
        if len(parts) > 1:
            gu_name = parts[1].strip()
            if gu_name in gu_to_city_map:
                return gu_to_city_map[gu_name]
        return "ë¶€ì²œ"
    
    # ì •ê·œí™”ëœ ì´ë¦„ìœ¼ë¡œ ë§¤í•‘ í™•ì¸
    if normalized_region in gu_to_city_map:
        return gu_to_city_map[normalized_region]
    
    # ì„œìš¸íŠ¹ë³„ì‹œì™€ ì¸ì²œê´‘ì—­ì‹œëŠ” ì‹œë„ëª…ë§Œ ì‚¬ìš©
    if city_name == "ì„œìš¸íŠ¹ë³„ì‹œ":
        return "ì„œìš¸"
    elif city_name == "ì¸ì²œê´‘ì—­ì‹œ":
        return "ì¸ì²œ"
    
    # ê²½ê¸°ë„: ì‹œ/êµ°ëª…ì—ì„œ "ì‹œ", "êµ°", "êµ¬" ì œê±°
    if normalized_region:
        return normalized_region
    
    return city_name


def get_region_type_filter(region_type: str):
    """
    ì§€ì—­ ìœ í˜•ì— ë”°ë¥¸ city_name í•„í„° ì¡°ê±´ ë°˜í™˜
    
    Args:
        region_type: ì§€ì—­ ìœ í˜• ("ì „êµ­", "ìˆ˜ë„ê¶Œ", "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ")
    
    Returns:
        SQLAlchemy í•„í„° ì¡°ê±´ (Noneì´ë©´ í•„í„° ì—†ìŒ)
    """
    if region_type == "ì „êµ­":
        return None
    elif region_type == "ìˆ˜ë„ê¶Œ":
        return State.city_name.in_(['ì„œìš¸íŠ¹ë³„ì‹œ', 'ê²½ê¸°ë„', 'ì¸ì²œê´‘ì—­ì‹œ'])
    elif region_type == "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ":
        return State.city_name.in_(['ë¶€ì‚°ê´‘ì—­ì‹œ', 'ëŒ€êµ¬ê´‘ì—­ì‹œ', 'ê´‘ì£¼ê´‘ì—­ì‹œ', 'ëŒ€ì „ê´‘ì—­ì‹œ', 'ìš¸ì‚°ê´‘ì—­ì‹œ'])
    else:
        raise ValueError(f"ìœ íš¨í•˜ì§€ ì•Šì€ region_type: {region_type}")


def calculate_quadrant(sale_change_rate: float, rent_change_rate: float) -> tuple[int, str]:
    """
    4ë¶„ë©´ ë¶„ë¥˜ ê³„ì‚°
    
    Args:
        sale_change_rate: ë§¤ë§¤ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨ (%)
        rent_change_rate: ì „ì›”ì„¸ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨ (%)
    
    Returns:
        (quadrant_number, quadrant_label) íŠœí”Œ
    """
    if sale_change_rate > 0 and rent_change_rate < 0:
        return (1, "ë§¤ìˆ˜ ì „í™˜")
    elif sale_change_rate < 0 and rent_change_rate > 0:
        return (2, "ì„ëŒ€ ì„ í˜¸/ê´€ë§")
    elif sale_change_rate < 0 and rent_change_rate < 0:
        return (3, "ì‹œì¥ ìœ„ì¶•")
    elif sale_change_rate > 0 and rent_change_rate > 0:
        return (4, "í™œì„±í™”")
    else:
        # ë³€í™”ìœ¨ì´ 0ì¸ ê²½ìš°ëŠ” ì¤‘ë¦½ìœ¼ë¡œ ì²˜ë¦¬
        if sale_change_rate == 0 and rent_change_rate == 0:
            return (0, "ì¤‘ë¦½")
        elif sale_change_rate == 0:
            return (2 if rent_change_rate > 0 else 3, "ì„ëŒ€ ì„ í˜¸/ê´€ë§" if rent_change_rate > 0 else "ì‹œì¥ ìœ„ì¶•")
        else:
            return (1 if sale_change_rate > 0 else 3, "ë§¤ìˆ˜ ì „í™˜" if sale_change_rate > 0 else "ì‹œì¥ ìœ„ì¶•")


@router.get(
    "/rvol",
    response_model=RVOLResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="RVOL(ìƒëŒ€ ê±°ë˜ëŸ‰) ì¡°íšŒ",
    description="""
    RVOL(Relative Volume)ì„ ê³„ì‚°í•˜ì—¬ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ### RVOL ê³„ì‚° ë°©ë²•
    - í˜„ì¬ ê±°ë˜ëŸ‰ì„ ê³¼ê±° ì¼ì • ê¸°ê°„ì˜ í‰ê·  ê±°ë˜ëŸ‰ìœ¼ë¡œ ë‚˜ëˆˆ ê°’
    - ì˜ˆ: ìµœê·¼ 2ê°œì›” ê±°ë˜ëŸ‰ Ã· ì§ì „ 2ê°œì›” í‰ê·  ê±°ë˜ëŸ‰
    
    ### í•´ì„
    - **RVOL > 1**: í‰ì†Œë³´ë‹¤ ê±°ë˜ê°€ í™œë°œí•¨ (í‰ê·  ì´ìƒ)
    - **RVOL = 1**: í‰ì†Œì™€ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì˜ ê±°ë˜ëŸ‰
    - **RVOL < 1**: í‰ì†Œë³´ë‹¤ ê±°ë˜ê°€ í•œì‚°í•¨ (í‰ê·  ì´í•˜)
    
    ### Query Parameters
    - `transaction_type`: ê±°ë˜ ìœ í˜• (sale: ë§¤ë§¤, rent: ì „ì›”ì„¸, ê¸°ë³¸ê°’: sale)
    - `current_period_months`: í˜„ì¬ ê¸°ê°„ (ê°œì›”, ê¸°ë³¸ê°’: 6, ìµœëŒ€: 6)
    - `average_period_months`: í‰ê·  ê³„ì‚° ê¸°ê°„ (ê°œì›”, ê¸°ë³¸ê°’: 6, ìµœëŒ€: 6)
    """
)
async def get_rvol(
    transaction_type: str = Query("sale", description="ê±°ë˜ ìœ í˜•: sale(ë§¤ë§¤), rent(ì „ì›”ì„¸)"),
    current_period_months: int = Query(6, ge=1, le=12, description="í˜„ì¬ ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 12)"),
    average_period_months: int = Query(6, ge=1, le=12, description="í‰ê·  ê³„ì‚° ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 12)"),
    db: AsyncSession = Depends(get_db)
):
    """
    RVOL(ìƒëŒ€ ê±°ë˜ëŸ‰) ì¡°íšŒ - ì„±ëŠ¥ ìµœì í™” ë²„ì „
    
    ì›”ë³„ ì§‘ê³„ë¡œ ê°„ì†Œí™”í•˜ì—¬ ë¹ ë¥¸ ì‘ë‹µ ì œê³µ
    """
    cache_key = build_cache_key(
        "statistics", "rvol_v2", transaction_type, 
        str(current_period_months), str(average_period_months)
    )
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        logger.info(f"âœ… [Statistics RVOL] ìºì‹œì—ì„œ ë°˜í™˜")
        return cached_data
    
    try:
        logger.info(
            f"ğŸ” [Statistics RVOL] RVOL ë°ì´í„° ì¡°íšŒ ì‹œì‘ - "
            f"transaction_type: {transaction_type}, "
            f"current_period_months: {current_period_months}, "
            f"average_period_months: {average_period_months}"
        )
        
        # ê±°ë˜ ìœ í˜•ì— ë”°ë¥¸ í…Œì´ë¸” ë° í•„ë“œ ì„ íƒ
        if transaction_type == "sale":
            trans_table = Sale
            date_field = Sale.contract_date
            base_filter = and_(
                Sale.is_canceled == False,
                (Sale.is_deleted == False) | (Sale.is_deleted.is_(None)),
                Sale.contract_date.isnot(None),
                or_(Sale.remarks != "ë”ë¯¸", Sale.remarks.is_(None))
            )
        else:  # rent
            trans_table = Rent
            date_field = Rent.deal_date
            base_filter = and_(
                (Rent.is_deleted == False) | (Rent.is_deleted.is_(None)),
                Rent.deal_date.isnot(None),
                or_(Rent.remarks != "ë”ë¯¸", Rent.remarks.is_(None))
            )
        
        # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ê¸°ê°„ ì„¤ì • (min/max ì¿¼ë¦¬ ì œê±°)
        today = date.today()
        # í˜„ì¬ ë‹¬ì˜ ì²« ë‚  (í˜„ì¬ ë‹¬ ì œì™¸)
        current_month_start = date(today.year, today.month, 1)
        
        # í˜„ì¬ ê¸°ê°„: ìµœê·¼ current_period_months ê°œì›” (í˜„ì¬ ë‹¬ ì œì™¸)
        current_start = current_month_start - timedelta(days=current_period_months * 30)
        current_end = current_month_start  # í˜„ì¬ ë‹¬ì˜ ì²« ë‚  ì „ê¹Œì§€
        
        # í‰ê·  ê³„ì‚° ê¸°ê°„: current_start ì´ì „ average_period_months ê°œì›”
        average_start = current_start - timedelta(days=average_period_months * 30)
        average_end = current_start
        
        logger.info(
            f"ğŸ“… [Statistics RVOL] ë‚ ì§œ ë²”ìœ„ - "
            f"current_start: {current_start}, current_end: {current_end}, "
            f"average_start: {average_start}, average_end: {average_end}"
        )
        
        # ì›”ë³„ ì§‘ê³„ë¡œ ê°„ì†Œí™” (ì¼ë³„ ëŒ€ì‹  ì›”ë³„)
        # í‰ê·  ê¸°ê°„ ì›”ë³„ ê±°ë˜ëŸ‰
        average_volume_stmt = (
            select(
                extract('year', date_field).label('year'),
                extract('month', date_field).label('month'),
                func.count(trans_table.trans_id).label('count')
            )
            .where(
                and_(
                    base_filter,
                    date_field >= average_start,
                    date_field < average_end
                )
            )
            .group_by(extract('year', date_field), extract('month', date_field))
        )
        
        # í˜„ì¬ ê¸°ê°„ ì›”ë³„ ê±°ë˜ëŸ‰
        current_volume_stmt = (
            select(
                extract('year', date_field).label('year'),
                extract('month', date_field).label('month'),
                func.count(trans_table.trans_id).label('count')
            )
            .where(
                and_(
                    base_filter,
                    date_field >= current_start,
                    date_field < current_end  # í˜„ì¬ ë‹¬ ì œì™¸ (ë¯¸ë§Œìœ¼ë¡œ ë³€ê²½)
                )
            )
            .group_by(extract('year', date_field), extract('month', date_field))
        )
        
        # ë³‘ë ¬ ì‹¤í–‰
        average_result, current_result = await asyncio.gather(
            db.execute(average_volume_stmt),
            db.execute(current_volume_stmt)
        )
        
        average_rows = average_result.fetchall()
        current_rows = current_result.fetchall()
        
        # í‰ê·  ê±°ë˜ëŸ‰ ê³„ì‚°
        if average_rows:
            total_average = sum(row.count for row in average_rows)
            average_monthly_volume = total_average / len(average_rows)
        else:
            average_monthly_volume = 1  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        
        logger.info(
            f"ğŸ“Š [Statistics RVOL] í‰ê·  ê±°ë˜ëŸ‰ ê³„ì‚° - "
            f"average_monthly_volume: {average_monthly_volume}"
        )
        
        # RVOL ë°ì´í„° ìƒì„± (ì›”ë³„) - í˜„ì¬ ë‹¬ ì œì™¸
        rvol_data = []
        current_year = today.year
        current_month = today.month
        
        for row in current_rows:
            year = int(row.year)
            month = int(row.month)
            
            # í˜„ì¬ ë‹¬ ì œì™¸
            if year == current_year and month == current_month:
                continue
                
            count = row.count or 0
            
            # RVOL ê³„ì‚°
            rvol = count / average_monthly_volume if average_monthly_volume > 0 else 0
            
            rvol_data.append(
                RVOLDataPoint(
                    date=f"{year}-{month:02d}-01",
                    current_volume=count,
                    average_volume=round(average_monthly_volume, 2),
                    rvol=round(rvol, 2)
                )
            )
        
        # ë‚ ì§œìˆœ ì •ë ¬
        rvol_data.sort(key=lambda x: x.date)
        
        period_description = f"ìµœê·¼ {current_period_months}ê°œì›” vs ì§ì „ {average_period_months}ê°œì›”"
        
        response_data = RVOLResponse(
            success=True,
            data=rvol_data,
            period=period_description
        )
        
        # ìºì‹œì— ì €ì¥ (TTL: 6ì‹œê°„)
        if len(rvol_data) > 0:
            await set_to_cache(cache_key, response_data.dict(), ttl=STATISTICS_CACHE_TTL)
        
        logger.info(f"âœ… [Statistics RVOL] RVOL ë°ì´í„° ìƒì„± ì™„ë£Œ - ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(rvol_data)}")
        
        return response_data
        
    except Exception as e:
        logger.error(f"âŒ [Statistics RVOL] RVOL ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"RVOL ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/quadrant",
    response_model=QuadrantResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="4ë¶„ë©´ ë¶„ë¥˜ ì¡°íšŒ",
    description="""
    ë§¤ë§¤ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨ê³¼ ì „ì›”ì„¸ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨ì„ ê¸°ë°˜ìœ¼ë¡œ 4ë¶„ë©´ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    ### 4ë¶„ë©´ ë¶„ë¥˜
    - **xì¶•**: ë§¤ë§¤ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨
    - **yì¶•**: ì „ì›”ì„¸ ê±°ë˜ëŸ‰ ë³€í™”ìœ¨
    
    ### í•´ì„
    1. **ë§¤ë§¤â†‘ / ì „ì›”ì„¸â†“**: ë§¤ìˆ˜ ì „í™˜ (ì‚¬ëŠ” ìª½ìœ¼ë¡œ ì´ë™)
    2. **ë§¤ë§¤â†“ / ì „ì›”ì„¸â†‘**: ì„ëŒ€ ì„ í˜¸/ê´€ë§ (ë¹Œë¦¬ëŠ” ìª½ìœ¼ë¡œ ì´ë™)
    3. **ë§¤ë§¤â†“ / ì „ì›”ì„¸â†“**: ì‹œì¥ ìœ„ì¶• (ì „ì²´ ìœ ë™ì„± ê²½ìƒ‰)
    4. **ë§¤ë§¤â†‘ / ì „ì›”ì„¸â†‘**: í™œì„±í™” (ìˆ˜ìš” ìì²´ê°€ ê°•í•¨, ì´ì‚¬/ê±°ë˜ ì¦ê°€)
    
    ### Query Parameters
    - `period_months`: ë¹„êµ ê¸°ê°„ (ê°œì›”, ê¸°ë³¸ê°’: 2, ìµœëŒ€: 6)
    """
)
async def get_quadrant(
    period_months: int = Query(2, ge=1, le=12, description="ë¹„êµ ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 12)"),
    db: AsyncSession = Depends(get_db)
):
    """
    4ë¶„ë©´ ë¶„ë¥˜ ì¡°íšŒ - ì„±ëŠ¥ ìµœì í™” ë²„ì „
    
    ì›”ë³„ ì§‘ê³„ë¡œ ê°„ì†Œí™”í•˜ì—¬ ë¹ ë¥¸ ì‘ë‹µ ì œê³µ
    """
    cache_key = build_cache_key("statistics", "quadrant_v2", str(period_months))
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        logger.info(f"âœ… [Statistics Quadrant] ìºì‹œì—ì„œ ë°˜í™˜")
        return cached_data
    
    try:
        logger.info(
            f"ğŸ” [Statistics Quadrant] 4ë¶„ë©´ ë¶„ë¥˜ ë°ì´í„° ì¡°íšŒ ì‹œì‘ - "
            f"period_months: {period_months}"
        )
        
        # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ê¸°ê°„ ì„¤ì •
        today = date.today()
        # í˜„ì¬ ë‹¬ì˜ ì²« ë‚  (í˜„ì¬ ë‹¬ ì œì™¸)
        current_month_start = date(today.year, today.month, 1)
        
        # ìµœê·¼ ê¸°ê°„ê³¼ ì´ì „ ê¸°ê°„ ì„¤ì • (í˜„ì¬ ë‹¬ ì œì™¸)
        recent_start = current_month_start - timedelta(days=period_months * 30)
        recent_end = current_month_start  # í˜„ì¬ ë‹¬ì˜ ì²« ë‚  ì „ê¹Œì§€
        
        previous_start = recent_start - timedelta(days=period_months * 30)
        previous_end = recent_start
        
        logger.info(
            f"ğŸ“… [Statistics Quadrant] ë‚ ì§œ ë²”ìœ„ - "
            f"previous_start: {previous_start}, previous_end: {previous_end}, "
            f"recent_start: {recent_start}, recent_end: {recent_end}"
        )
        
        # ì›”ë³„ ì§‘ê³„ (to_char ëŒ€ì‹  extract ì‚¬ìš© - ì¸ë±ìŠ¤ í™œìš© ê°€ëŠ¥)
        # ë§¤ë§¤ ê±°ë˜ëŸ‰: ì´ì „ ê¸°ê°„
        sale_previous_stmt = (
            select(
                extract('year', Sale.contract_date).label('year'),
                extract('month', Sale.contract_date).label('month'),
                func.count(Sale.trans_id).label('count')
            )
            .where(
                and_(
                    Sale.is_canceled == False,
                    (Sale.is_deleted == False) | (Sale.is_deleted.is_(None)),
                    Sale.contract_date.isnot(None),
                    Sale.contract_date >= previous_start,
                    Sale.contract_date < previous_end,
                    or_(Sale.remarks != "ë”ë¯¸", Sale.remarks.is_(None))
                )
            )
            .group_by(extract('year', Sale.contract_date), extract('month', Sale.contract_date))
        )
        
        # ë§¤ë§¤ ê±°ë˜ëŸ‰: ìµœê·¼ ê¸°ê°„
        sale_recent_stmt = (
            select(
                extract('year', Sale.contract_date).label('year'),
                extract('month', Sale.contract_date).label('month'),
                func.count(Sale.trans_id).label('count')
            )
            .where(
                and_(
                    Sale.is_canceled == False,
                    (Sale.is_deleted == False) | (Sale.is_deleted.is_(None)),
                    Sale.contract_date.isnot(None),
                    Sale.contract_date >= recent_start,
                    Sale.contract_date < recent_end,  # í˜„ì¬ ë‹¬ ì œì™¸ (ë¯¸ë§Œìœ¼ë¡œ ë³€ê²½)
                    or_(Sale.remarks != "ë”ë¯¸", Sale.remarks.is_(None))
                )
            )
            .group_by(extract('year', Sale.contract_date), extract('month', Sale.contract_date))
        )
        
        # ì „ì›”ì„¸ ê±°ë˜ëŸ‰: ì´ì „ ê¸°ê°„
        rent_previous_stmt = (
            select(
                extract('year', Rent.deal_date).label('year'),
                extract('month', Rent.deal_date).label('month'),
                func.count(Rent.trans_id).label('count')
            )
            .where(
                and_(
                    (Rent.is_deleted == False) | (Rent.is_deleted.is_(None)),
                    Rent.deal_date.isnot(None),
                    Rent.deal_date >= previous_start,
                    Rent.deal_date < previous_end,
                    or_(Rent.remarks != "ë”ë¯¸", Rent.remarks.is_(None))
                )
            )
            .group_by(extract('year', Rent.deal_date), extract('month', Rent.deal_date))
        )
        
        # ì „ì›”ì„¸ ê±°ë˜ëŸ‰: ìµœê·¼ ê¸°ê°„
        rent_recent_stmt = (
            select(
                extract('year', Rent.deal_date).label('year'),
                extract('month', Rent.deal_date).label('month'),
                func.count(Rent.trans_id).label('count')
            )
            .where(
                and_(
                    (Rent.is_deleted == False) | (Rent.is_deleted.is_(None)),
                    Rent.deal_date.isnot(None),
                    Rent.deal_date >= recent_start,
                    Rent.deal_date < recent_end,  # í˜„ì¬ ë‹¬ ì œì™¸ (ë¯¸ë§Œìœ¼ë¡œ ë³€ê²½)
                    or_(Rent.remarks != "ë”ë¯¸", Rent.remarks.is_(None))
                )
            )
            .group_by(extract('year', Rent.deal_date), extract('month', Rent.deal_date))
        )
        
        # ì¿¼ë¦¬ ë³‘ë ¬ ì‹¤í–‰ (ì„±ëŠ¥ ìµœì í™”)
        sale_previous_result, sale_recent_result, rent_previous_result, rent_recent_result = await asyncio.gather(
            db.execute(sale_previous_stmt),
            db.execute(sale_recent_stmt),
            db.execute(rent_previous_stmt),
            db.execute(rent_recent_stmt)
        )
        
        sale_previous_rows = sale_previous_result.fetchall()
        sale_recent_rows = sale_recent_result.fetchall()
        rent_previous_rows = rent_previous_result.fetchall()
        rent_recent_rows = rent_recent_result.fetchall()
        
        # ì´ì „ ê¸°ê°„ í‰ê·  ê³„ì‚°
        sale_previous_total = sum(row.count for row in sale_previous_rows) if sale_previous_rows else 0
        rent_previous_total = sum(row.count for row in rent_previous_rows) if rent_previous_rows else 0
        
        sale_previous_avg = sale_previous_total / len(sale_previous_rows) if sale_previous_rows else 1
        rent_previous_avg = rent_previous_total / len(rent_previous_rows) if rent_previous_rows else 1
        
        # ìµœê·¼ ê¸°ê°„ ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        sale_recent_dict = {f"{int(row.year)}-{int(row.month):02d}": row.count for row in sale_recent_rows}
        rent_recent_dict = {f"{int(row.year)}-{int(row.month):02d}": row.count for row in rent_recent_rows}
        
        # ëª¨ë“  ê¸°ê°„ ìˆ˜ì§‘ (í˜„ì¬ ë‹¬ ì œì™¸)
        all_periods = set(sale_recent_dict.keys()) | set(rent_recent_dict.keys())
        current_year = today.year
        current_month = today.month
        current_period_key = f"{current_year}-{current_month:02d}"
        
        # í˜„ì¬ ë‹¬ ì œì™¸
        all_periods.discard(current_period_key)
        
        quadrant_data = []
        quadrant_counts = {1: 0, 2: 0, 3: 0, 4: 0}
        
        for period in sorted(all_periods):
            sale_recent_count = sale_recent_dict.get(period, 0)
            rent_recent_count = rent_recent_dict.get(period, 0)
            
            # ë³€í™”ìœ¨ ê³„ì‚°
            sale_change_rate = ((sale_recent_count - sale_previous_avg) / sale_previous_avg * 100) if sale_previous_avg > 0 else 0
            rent_change_rate = ((rent_recent_count - rent_previous_avg) / rent_previous_avg * 100) if rent_previous_avg > 0 else 0
            
            # 4ë¶„ë©´ ë¶„ë¥˜
            quadrant_num, quadrant_label = calculate_quadrant(sale_change_rate, rent_change_rate)
            
            if quadrant_num > 0:
                quadrant_counts[quadrant_num] = quadrant_counts.get(quadrant_num, 0) + 1
            
            quadrant_data.append(
                QuadrantDataPoint(
                    date=period,
                    sale_volume_change_rate=round(sale_change_rate, 2),
                    rent_volume_change_rate=round(rent_change_rate, 2),
                    quadrant=quadrant_num,
                    quadrant_label=quadrant_label
                )
            )
        
        summary = {
            "total_periods": len(quadrant_data),
            "quadrant_distribution": quadrant_counts,
            "sale_previous_avg": round(sale_previous_avg, 2),
            "rent_previous_avg": round(rent_previous_avg, 2)
        }
        
        response_data = QuadrantResponse(
            success=True,
            data=quadrant_data,
            summary=summary
        )
        
        # ìºì‹œì— ì €ì¥ (TTL: 6ì‹œê°„)
        if len(quadrant_data) > 0:
            await set_to_cache(cache_key, response_data.dict(), ttl=STATISTICS_CACHE_TTL)
        
        logger.info(f"âœ… [Statistics Quadrant] 4ë¶„ë©´ ë¶„ë¥˜ ë°ì´í„° ìƒì„± ì™„ë£Œ - ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(quadrant_data)}")
        
        return response_data
        
    except Exception as e:
        logger.error(f"âŒ [Statistics Quadrant] 4ë¶„ë©´ ë¶„ë¥˜ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"4ë¶„ë©´ ë¶„ë¥˜ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/hpi",
    response_model=HPIResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="ì£¼íƒê°€ê²©ì§€ìˆ˜(HPI) ì¡°íšŒ",
    description="""
    ì£¼íƒê°€ê²©ì§€ìˆ˜(Housing Price Index, HPI)ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ### ì£¼íƒê°€ê²©ì§€ìˆ˜ë€?
    íŠ¹ì • ì‹œì ì˜ ì£¼íƒ ê°€ê²©ì„ ê¸°ì¤€(100)ìœ¼ë¡œ ì¡ê³ , ì´í›„ ê°€ê²©ì´ ì–¼ë§ˆë‚˜ ë³€í–ˆëŠ”ì§€ë¥¼ ìˆ˜ì¹˜í™”í•œ í†µê³„ ì§€í‘œì…ë‹ˆë‹¤.
    
    ### ì§€ìˆ˜ í•´ì„
    - **ì§€ìˆ˜ > 100**: ê¸°ì¤€ ì‹œì ë³´ë‹¤ ì§‘ê°’ì´ ì˜¬ëìŒ
    - **ì§€ìˆ˜ = 100**: ê¸°ì¤€ ì‹œì ê³¼ ë™ì¼
    - **ì§€ìˆ˜ < 100**: ê¸°ì¤€ ì‹œì ë³´ë‹¤ ì§‘ê°’ì´ ë‚´ë ¸ìŒ
    
    ### Query Parameters
    - `region_id`: ì§€ì—­ ID (ì„ íƒ, ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì „ì²´ ì§€ì—­ í‰ê· )
    - `index_type`: ì§€ìˆ˜ ìœ í˜• (APT: ì•„íŒŒíŠ¸, HOUSE: ë‹¨ë…ì£¼íƒ, ALL: ì „ì²´, ê¸°ë³¸ê°’: APT)
    - `months`: ì¡°íšŒ ê¸°ê°„ (ê°œì›”, ê¸°ë³¸ê°’: 24, ìµœëŒ€: 60)
    """
)
async def get_hpi(
    region_id: Optional[int] = Query(None, description="ì§€ì—­ ID (ì„ íƒ)"),
    index_type: str = Query("APT", description="ì§€ìˆ˜ ìœ í˜•: APT(ì•„íŒŒíŠ¸), HOUSE(ë‹¨ë…ì£¼íƒ), ALL(ì „ì²´)"),
    months: int = Query(24, ge=1, le=60, description="ì¡°íšŒ ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 60)"),
    db: AsyncSession = Depends(get_db)
):
    """
    ì£¼íƒê°€ê²©ì§€ìˆ˜(HPI) ì¡°íšŒ
    
    ì§€ì—­ë³„ ì£¼íƒê°€ê²©ì§€ìˆ˜ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    # ìœ íš¨í•œ index_type ê²€ì¦
    valid_index_types = ["APT", "HOUSE", "ALL"]
    if index_type not in valid_index_types:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ index_typeì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê°’: {', '.join(valid_index_types)}"
        )
    
    cache_key = build_cache_key(
        "statistics", "hpi", 
        str(region_id) if region_id else "all",
        index_type,
        str(months)
    )
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        logger.info(f"âœ… [Statistics HPI] ìºì‹œì—ì„œ ë°˜í™˜")
        return cached_data
    
    try:
        logger.info(
            f"ğŸ” [Statistics HPI] HPI ë°ì´í„° ì¡°íšŒ ì‹œì‘ - "
            f"region_id: {region_id}, index_type: {index_type}, months: {months}"
        )
        
        # ê¸°ì¤€ ë‚ ì§œ ê³„ì‚° (í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ìµœê·¼ monthsê°œì›”)
        today = date.today()
        # base_ymì€ YYYYMM í˜•ì‹ì´ë¯€ë¡œ, í˜„ì¬ ë…„ì›”ì„ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
        current_year = today.year
        current_month = today.month
        
        # ìµœì†Œ base_ym ê³„ì‚° (monthsê°œì›” ì „)
        # ì›” ë‹¨ìœ„ë¡œ ê³„ì‚°
        total_months = current_year * 12 + current_month - 1
        start_total_months = total_months - months + 1  # í˜„ì¬ ë‹¬ í¬í•¨
        start_year = start_total_months // 12
        start_month = (start_total_months % 12) + 1
        
        start_base_ym = f"{start_year:04d}{start_month:02d}"
        end_base_ym = f"{current_year:04d}{current_month:02d}"
        
        logger.info(
            f"ğŸ“… [Statistics HPI] ë‚ ì§œ ë²”ìœ„ - "
            f"start_base_ym: {start_base_ym}, end_base_ym: {end_base_ym}"
        )
        
        # ì¿¼ë¦¬ êµ¬ì„±
        # region_idê°€ ì§€ì •ëœ ê²½ìš°: íŠ¹ì • ì§€ì—­ë§Œ ì¡°íšŒ
        if region_id is not None:
            query = (
                select(
                    HouseScore.base_ym,
                    HouseScore.index_value,
                    HouseScore.index_change_rate,
                    HouseScore.index_type,
                    State.city_name.label('region_name')  # ì‹œë„ëª… ì‚¬ìš©
                )
                .join(State, HouseScore.region_id == State.region_id)
                .where(
                    and_(
                        HouseScore.region_id == region_id,
                        HouseScore.is_deleted == False,
                        HouseScore.index_type == index_type,
                        HouseScore.base_ym >= start_base_ym,
                        HouseScore.base_ym <= end_base_ym
                    )
                )
                .order_by(HouseScore.base_ym)
            )
        else:
            # region_idê°€ ì—†ëŠ” ê²½ìš°: ì‹œë„(city_name) ë ˆë²¨ë¡œ ê·¸ë£¹í™” (ì¸êµ¬ ì´ë™ ë°ì´í„°ì™€ ë™ì¼í•œ ë ˆë²¨)
            query = (
                select(
                    HouseScore.base_ym,
                    func.avg(HouseScore.index_value).label('index_value'),
                    func.avg(HouseScore.index_change_rate).label('index_change_rate'),
                    func.max(HouseScore.index_type).label('index_type'),
                    State.city_name.label('region_name')  # ì‹œë„ëª…ìœ¼ë¡œ ê·¸ë£¹í™”
                )
                .join(State, HouseScore.region_id == State.region_id)
                .where(
                    and_(
                        HouseScore.is_deleted == False,
                        State.is_deleted == False,
                        HouseScore.index_type == index_type,
                        HouseScore.base_ym >= start_base_ym,
                        HouseScore.base_ym <= end_base_ym
                    )
                )
                .group_by(HouseScore.base_ym, State.city_name)
                .order_by(HouseScore.base_ym, State.city_name)
            )
        
        result = await db.execute(query)
        rows = result.fetchall()
        
        logger.info(
            f"ğŸ“Š [Statistics HPI] ì¿¼ë¦¬ ê²°ê³¼ - "
            f"ì´ {len(rows)}ê±´ ì¡°íšŒë¨"
        )
        
        # ì‹œë„ë³„ ë°ì´í„° ê°œìˆ˜ í™•ì¸
        if rows:
            region_counts = {}
            for row in rows:
                region_name = row.region_name if hasattr(row, 'region_name') and row.region_name else "Unknown"
                region_counts[region_name] = region_counts.get(region_name, 0) + 1
            
            logger.info(
                f"ğŸ“‹ [Statistics HPI] ì‹œë„ë³„ ë°ì´í„° ê°œìˆ˜ - "
                f"{', '.join([f'{k}: {v}ê±´' for k, v in sorted(region_counts.items())])}"
            )
        
        # ë°ì´í„° í¬ì¸íŠ¸ ìƒì„±
        hpi_data = []
        for row in rows:
            base_ym = row.base_ym
            # YYYYMM -> YYYY-MM í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            year = base_ym[:4]
            month = base_ym[4:6]
            date_str = f"{year}-{month}"
            
            index_value = float(row.index_value) if row.index_value is not None else 0.0
            index_change_rate = float(row.index_change_rate) if row.index_change_rate is not None else None
            
            # region_name ì²˜ë¦¬: ì‹œë„ëª…(city_name) ì‚¬ìš©
            region_name = row.region_name if hasattr(row, 'region_name') and row.region_name else None
            
            hpi_data.append(
                HPIDataPoint(
                    date=date_str,
                    index_value=round(index_value, 2),
                    index_change_rate=round(index_change_rate, 2) if index_change_rate is not None else None,
                    region_name=region_name,
                    index_type=index_type
                )
            )
        
        # ë‚ ì§œìˆœ ì •ë ¬ (ì´ë¯¸ ì •ë ¬ë˜ì–´ ìˆì§€ë§Œ í™•ì‹¤íˆ)
        hpi_data.sort(key=lambda x: x.date)
        
        # ì§€ì—­ë³„/ë‚ ì§œë³„ ë°ì´í„° ê°œìˆ˜ í™•ì¸
        if hpi_data:
            date_counts = {}
            region_date_counts = {}
            for item in hpi_data:
                date_counts[item.date] = date_counts.get(item.date, 0) + 1
                if item.region_name:
                    key = f"{item.region_name}-{item.date}"
                    region_date_counts[key] = region_date_counts.get(key, 0) + 1
            
            logger.info(
                f"ğŸ“ˆ [Statistics HPI] ë°ì´í„° í¬ì¸íŠ¸ ìƒì„¸ - "
                f"ì´ {len(hpi_data)}ê±´, "
                f"ë‚ ì§œë³„ ê°œìˆ˜: {dict(sorted(date_counts.items())[:5])}... (ìµœì‹  5ê°œë§Œ í‘œì‹œ), "
                f"ì‹œë„ ìˆ˜: {len(set(item.region_name for item in hpi_data if item.region_name))}ê°œ"
            )
            
            # ê° ì‹œë„ë³„ ìµœì‹  ë°ì´í„° ìƒ˜í”Œ ë¡œê¹…
            latest_by_region = {}
            for item in reversed(hpi_data):  # ìµœì‹ ë¶€í„°
                if item.region_name and item.region_name not in latest_by_region:
                    latest_by_region[item.region_name] = item
            
            if latest_by_region:
                sample_regions = list(latest_by_region.items())[:5]  # ìµœëŒ€ 5ê°œë§Œ
                logger.info(
                    f"ğŸ“ [Statistics HPI] ì‹œë„ë³„ ìµœì‹  ë°ì´í„° ìƒ˜í”Œ - "
                    f"{', '.join([f'{r}: {d.date} {d.index_value}' for r, d in sample_regions])}"
                )
        
        region_desc = f"ì§€ì—­ ID {region_id}" if region_id else "ì „ì²´ ì§€ì—­ í‰ê· "
        period_desc = f"{months}ê°œì›” ({hpi_data[0].date if hpi_data else 'N/A'} ~ {hpi_data[-1].date if hpi_data else 'N/A'})"
        
        response_data = HPIResponse(
            success=True,
            data=hpi_data,
            region_id=region_id,
            index_type=index_type,
            period=f"{region_desc}, {index_type}, {period_desc}"
        )
        
        # ìºì‹œì— ì €ì¥ (TTL: 6ì‹œê°„)
        if len(hpi_data) > 0:
            await set_to_cache(cache_key, response_data.dict(), ttl=STATISTICS_CACHE_TTL)
        
        logger.info(f"âœ… [Statistics HPI] HPI ë°ì´í„° ìƒì„± ì™„ë£Œ - ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(hpi_data)}")
        
        return response_data
        
    except Exception as e:
        logger.error(f"âŒ [Statistics HPI] HPI ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"HPI ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/hpi/heatmap",
    response_model=HPIHeatmapResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="ì£¼íƒê°€ê²©ì§€ìˆ˜(HPI) íˆíŠ¸ë§µ ì¡°íšŒ",
    description="""
    ê´‘ì—­ì‹œ/íŠ¹ë³„ì‹œ/ë„ë³„ ì£¼íƒê°€ê²©ì§€ìˆ˜ë¥¼ íˆíŠ¸ë§µ í˜•ì‹ìœ¼ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ê° ë„/ì‹œì˜ ìµœì‹  HPI ê°’ì„ ë°˜í™˜í•˜ì—¬ ì§€ì—­ë³„ ê°€ê²© ì¶”ì´ë¥¼ í•œëˆˆì— ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    ### Query Parameters
    - `index_type`: ì§€ìˆ˜ ìœ í˜• (APT: ì•„íŒŒíŠ¸, HOUSE: ë‹¨ë…ì£¼íƒ, ALL: ì „ì²´, ê¸°ë³¸ê°’: APT)
    """
)
async def get_hpi_heatmap(
    index_type: str = Query("APT", description="ì§€ìˆ˜ ìœ í˜•: APT(ì•„íŒŒíŠ¸), HOUSE(ë‹¨ë…ì£¼íƒ), ALL(ì „ì²´)"),
    db: AsyncSession = Depends(get_db)
):
    """
    ì£¼íƒê°€ê²©ì§€ìˆ˜(HPI) íˆíŠ¸ë§µ ì¡°íšŒ
    
    ë„/ì‹œë³„ ìµœì‹  HPI ê°’ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    # ìœ íš¨í•œ index_type ê²€ì¦
    valid_index_types = ["APT", "HOUSE", "ALL"]
    if index_type not in valid_index_types:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ index_typeì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê°’: {', '.join(valid_index_types)}"
        )
    
    cache_key = build_cache_key("statistics", "hpi_heatmap", index_type)
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        logger.info(f"âœ… [Statistics HPI Heatmap] ìºì‹œì—ì„œ ë°˜í™˜")
        return cached_data
    
    try:
        logger.info(
            f"ğŸ” [Statistics HPI Heatmap] HPI íˆíŠ¸ë§µ ë°ì´í„° ì¡°íšŒ ì‹œì‘ - "
            f"index_type: {index_type}"
        )
        
        # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ìµœì‹  base_ym ì°¾ê¸°
        today = date.today()
        current_year = today.year
        current_month = today.month
        current_base_ym = f"{current_year:04d}{current_month:02d}"
        
        # ìµœì‹  base_ymë¶€í„° ì—­ìˆœìœ¼ë¡œ ì°¾ê¸° (ìµœëŒ€ 12ê°œì›” ì „ê¹Œì§€)
        found_base_ym = None
        for i in range(12):
            check_year = current_year
            check_month = current_month - i
            if check_month <= 0:
                check_year -= 1
                check_month += 12
            check_base_ym = f"{check_year:04d}{check_month:02d}"
            
            # í•´ë‹¹ base_ymì— ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
            check_query = (
                select(func.count(HouseScore.index_id))
                .where(
                    and_(
                        HouseScore.is_deleted == False,
                        HouseScore.index_type == index_type,
                        HouseScore.base_ym == check_base_ym
                    )
                )
            )
            check_result = await db.execute(check_query)
            count = check_result.scalar() or 0
            
            if count > 0:
                found_base_ym = check_base_ym
                break
        
        if not found_base_ym:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="HPI ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        logger.info(f"ğŸ“… [Statistics HPI Heatmap] ì‚¬ìš©í•  base_ym: {found_base_ym}")
        
        # ë„/ì‹œë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í‰ê·  HPI ê³„ì‚°
        query = (
            select(
                State.city_name,
                func.avg(HouseScore.index_value).label('index_value'),
                func.avg(HouseScore.index_change_rate).label('index_change_rate'),
                func.count(HouseScore.index_id).label('region_count')
            )
            .join(State, HouseScore.region_id == State.region_id)
            .where(
                and_(
                    HouseScore.is_deleted == False,
                    State.is_deleted == False,
                    HouseScore.index_type == index_type,
                    HouseScore.base_ym == found_base_ym
                )
            )
            .group_by(State.city_name)
            .order_by(State.city_name)
        )
        
        result = await db.execute(query)
        rows = result.fetchall()
        
        # ë°ì´í„° í¬ì¸íŠ¸ ìƒì„±
        heatmap_data = []
        for row in rows:
            city_name = row.city_name
            index_value = float(row.index_value) if row.index_value is not None else 0.0
            index_change_rate = float(row.index_change_rate) if row.index_change_rate is not None else None
            region_count = int(row.region_count) if row.region_count else 0
            
            heatmap_data.append(
                HPIHeatmapDataPoint(
                    city_name=city_name,
                    index_value=round(index_value, 2),
                    index_change_rate=round(index_change_rate, 2) if index_change_rate is not None else None,
                    base_ym=found_base_ym,
                    region_count=region_count
                )
            )
        
        # ë„/ì‹œëª… ìˆœìœ¼ë¡œ ì •ë ¬
        heatmap_data.sort(key=lambda x: x.city_name)
        
        response_data = HPIHeatmapResponse(
            success=True,
            data=heatmap_data,
            index_type=index_type,
            base_ym=found_base_ym
        )
        
        # ìºì‹œì— ì €ì¥ (TTL: 6ì‹œê°„)
        if len(heatmap_data) > 0:
            await set_to_cache(cache_key, response_data.dict(), ttl=STATISTICS_CACHE_TTL)
        
        logger.info(f"âœ… [Statistics HPI Heatmap] HPI íˆíŠ¸ë§µ ë°ì´í„° ìƒì„± ì™„ë£Œ - ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(heatmap_data)}")
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [Statistics HPI Heatmap] HPI íˆíŠ¸ë§µ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"HPI íˆíŠ¸ë§µ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/summary",
    response_model=StatisticsSummaryResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="í†µê³„ ìš”ì•½ ì¡°íšŒ",
    description="""
    RVOLê³¼ 4ë¶„ë©´ ë¶„ë¥˜ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¡°íšŒí•©ë‹ˆë‹¤.
    """
)
async def get_statistics_summary(
    transaction_type: str = Query("sale", description="ê±°ë˜ ìœ í˜•: sale(ë§¤ë§¤), rent(ì „ì›”ì„¸)"),
    current_period_months: int = Query(6, ge=1, le=12, description="í˜„ì¬ ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 12)"),
    average_period_months: int = Query(6, ge=1, le=12, description="í‰ê·  ê³„ì‚° ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 12)"),
    quadrant_period_months: int = Query(2, ge=1, le=12, description="4ë¶„ë©´ ë¹„êµ ê¸°ê°„ (ê°œì›”, ìµœëŒ€ 12)"),
    db: AsyncSession = Depends(get_db)
):
    """
    í†µê³„ ìš”ì•½ ì¡°íšŒ
    
    RVOLê³¼ 4ë¶„ë©´ ë¶„ë¥˜ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    # RVOLê³¼ 4ë¶„ë©´ ë¶„ë¥˜ë¥¼ ë³‘ë ¬ë¡œ ì¡°íšŒ
    rvol_task = get_rvol(transaction_type, current_period_months, average_period_months, db)
    quadrant_task = get_quadrant(quadrant_period_months, db)
    
    rvol_response, quadrant_response = await asyncio.gather(rvol_task, quadrant_task)
    
    return StatisticsSummaryResponse(
        success=True,
        rvol=rvol_response,
        quadrant=quadrant_response
    )


@router.get(
    "/population-movements",
    response_model=PopulationMovementResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="ì¸êµ¬ ì´ë™ ë°ì´í„° ì¡°íšŒ",
    description="""
    ì§€ì—­ë³„ ì¸êµ¬ ì´ë™ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ### Query Parameters
    - `region_id`: ì§€ì—­ ID (ì„ íƒ, ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì „ì²´)
    - `start_ym`: ì‹œì‘ ë…„ì›” (YYYYMM, ê¸°ë³¸ê°’: ìµœê·¼ 12ê°œì›”)
    - `end_ym`: ì¢…ë£Œ ë…„ì›” (YYYYMM, ê¸°ë³¸ê°’: í˜„ì¬)
    """
)
async def get_population_movements(
    region_id: Optional[int] = Query(None, description="ì§€ì—­ ID (ì„ íƒ)"),
    start_ym: Optional[str] = Query(None, description="ì‹œì‘ ë…„ì›” (YYYYMM)"),
    end_ym: Optional[str] = Query(None, description="ì¢…ë£Œ ë…„ì›” (YYYYMM)"),
    db: AsyncSession = Depends(get_db)
):
    """
    ì¸êµ¬ ì´ë™ ë°ì´í„° ì¡°íšŒ
    """
    try:
        # ê¸°ë³¸ ê¸°ê°„ ì„¤ì • (ìµœê·¼ 12ê°œì›”)
        if not end_ym:
            end_date = datetime.now()
            end_ym = end_date.strftime("%Y%m")
        
        if not start_ym:
            start_date = datetime.now() - timedelta(days=365)
            start_ym = start_date.strftime("%Y%m")
        
        # ì¿¼ë¦¬ êµ¬ì„±: ì‹œë„ ë ˆë²¨ ë°ì´í„°ë§Œ ì¡°íšŒ (city_name ì‚¬ìš©)
        query = select(
            PopulationMovement,
            State.city_name  # ì‹œë„ëª… ì‚¬ìš© (ì˜ˆ: ì„œìš¸íŠ¹ë³„ì‹œ, ë¶€ì‚°ê´‘ì—­ì‹œ)
        ).join(
            State, PopulationMovement.region_id == State.region_id
        ).where(
            and_(
                PopulationMovement.base_ym >= start_ym,
                PopulationMovement.base_ym <= end_ym,
                PopulationMovement.is_deleted == False
            )
        )
        
        if region_id:
            query = query.where(PopulationMovement.region_id == region_id)
        
        query = query.order_by(PopulationMovement.base_ym.desc())
        
        result = await db.execute(query)
        rows = result.all()
        
        logger.info(
            f"ğŸ“Š [Statistics Population Movement] ì¸êµ¬ ì´ë™ ë°ì´í„° ì¡°íšŒ - "
            f"ì´ {len(rows)}ê±´ ì¡°íšŒë¨"
        )
        
        # ì§€ì—­ë³„ ë°ì´í„° ê°œìˆ˜ í™•ì¸
        if rows:
            region_counts = {}
            region_net_totals = {}  # ì§€ì—­ë³„ ìˆœì´ë™ í•©ê³„
            for movement, city_name in rows:
                region_name = city_name or "Unknown"
                region_counts[region_name] = region_counts.get(region_name, 0) + 1
                # ìˆœì´ë™ í•©ê³„ ê³„ì‚°
                if region_name not in region_net_totals:
                    region_net_totals[region_name] = 0
                region_net_totals[region_name] += movement.net_migration or 0
            
            logger.info(
                f"ğŸ“‹ [Statistics Population Movement] ì‹œë„ë³„ ë°ì´í„° ê°œìˆ˜ - "
                f"{', '.join([f'{k}: {v}ê±´' for k, v in sorted(region_counts.items())])}"
            )
            
            logger.info(
                f"ğŸ“Š [Statistics Population Movement] ì‹œë„ë³„ ìˆœì´ë™ í•©ê³„ - "
                f"{', '.join([f'{k}: {v}ëª…' for k, v in sorted(region_net_totals.items())])}"
            )
        
        data_points = []
        for movement, city_name in rows:
            # YYYYMM -> YYYY-MM ë³€í™˜
            year = movement.base_ym[:4]
            month = movement.base_ym[4:]
            date_str = f"{year}-{month}"
            
            data_points.append(PopulationMovementDataPoint(
                date=date_str,
                region_id=movement.region_id,
                region_name=city_name,  # ì‹œë„ëª… ë°˜í™˜
                in_migration=movement.in_migration,
                out_migration=movement.out_migration,
                net_migration=movement.net_migration
            ))
        
        return PopulationMovementResponse(
            success=True,
            data=data_points,
            period=f"{start_ym} ~ {end_ym}"
        )
        
    except Exception as e:
        logger.error(f"âŒ ì¸êµ¬ ì´ë™ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì¸êµ¬ ì´ë™ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        )


# ============================================================
# ì£¼íƒ ìˆ˜ìš” í˜ì´ì§€ìš© ìƒˆë¡œìš´ API
# ============================================================

@router.get(
    "/hpi/by-region-type",
    response_model=HPIRegionTypeResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="ì§€ì—­ ìœ í˜•ë³„ ì£¼íƒ ê°€ê²© ì§€ìˆ˜ ì¡°íšŒ",
    description="""
    ì§€ì—­ ìœ í˜•ë³„ë¡œ ì£¼íƒ ê°€ê²© ì§€ìˆ˜ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ### Query Parameters
    - `region_type`: ì§€ì—­ ìœ í˜• (required)
    - `index_type`: ì§€ìˆ˜ ìœ í˜• (optional, ê¸°ë³¸ê°’: "APT")
    - `base_ym`: ê¸°ì¤€ ë…„ì›” (optional, ê¸°ë³¸ê°’: ìµœì‹ )
    """
)
async def get_hpi_by_region_type(
    region_type: str = Query(..., description="ì§€ì—­ ìœ í˜•: ì „êµ­, ìˆ˜ë„ê¶Œ, ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"),
    index_type: str = Query("APT", description="ì§€ìˆ˜ ìœ í˜•: APT(ì•„íŒŒíŠ¸), HOUSE(ë‹¨ë…ì£¼íƒ), ALL(ì „ì²´)"),
    base_ym: Optional[str] = Query(None, description="ê¸°ì¤€ ë…„ì›” (YYYYMM, ê¸°ë³¸ê°’: ìµœì‹ )"),
    db: AsyncSession = Depends(get_db)
):
    """
    ì§€ì—­ ìœ í˜•ë³„ ì£¼íƒ ê°€ê²© ì§€ìˆ˜ ì¡°íšŒ
    """
    # ìœ íš¨ì„± ê²€ì¦
    valid_region_types = ["ì „êµ­", "ìˆ˜ë„ê¶Œ", "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"]
    if region_type not in valid_region_types:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ region_typeì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê°’: {', '.join(valid_region_types)}"
        )
    
    valid_index_types = ["APT", "HOUSE", "ALL"]
    if index_type not in valid_index_types:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ index_typeì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê°’: {', '.join(valid_index_types)}"
        )
    
    cache_key = build_cache_key(
        "statistics", "hpi-by-region-type", region_type, index_type,
        base_ym if base_ym else "latest"
    )
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        logger.info(f"âœ… [Statistics HPI Region Type] ìºì‹œì—ì„œ ë°˜í™˜")
        return cached_data
    
    try:
        logger.info(
            f"ğŸ” [Statistics HPI Region Type] HPI ë°ì´í„° ì¡°íšŒ ì‹œì‘ - "
            f"region_type: {region_type}, index_type: {index_type}, base_ym: {base_ym}"
        )
        
        # base_ymì´ ì—†ìœ¼ë©´ ìµœì‹  ë°ì´í„° ì°¾ê¸°
        if not base_ym:
            today = date.today()
            current_year = today.year
            current_month = today.month
            
            # ìµœì‹  base_ym ì°¾ê¸° (ìµœëŒ€ 12ê°œì›” ì „ê¹Œì§€)
            found_base_ym = None
            for i in range(12):
                check_year = current_year
                check_month = current_month - i
                if check_month <= 0:
                    check_year -= 1
                    check_month += 12
                check_base_ym = f"{check_year:04d}{check_month:02d}"
                
                # í•´ë‹¹ base_ymì— ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
                check_query = (
                    select(func.count(HouseScore.index_id))
                    .where(
                        and_(
                            HouseScore.is_deleted == False,
                            HouseScore.index_type == index_type,
                            HouseScore.base_ym == check_base_ym
                        )
                    )
                )
                check_result = await db.execute(check_query)
                count = check_result.scalar() or 0
                
                if count > 0:
                    found_base_ym = check_base_ym
                    break
            
            if not found_base_ym:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail="HPI ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            
            base_ym = found_base_ym
        
        # ì§€ì—­ í•„í„° ì¡°ê±´
        region_filter = get_region_type_filter(region_type)
        
        # ìˆ˜ë„ê¶Œì˜ ê²½ìš° ì‹œ/êµ° ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”, ê·¸ ì™¸ëŠ” ì‹œë„ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”
        if region_type == "ìˆ˜ë„ê¶Œ":
            # ìˆ˜ë„ê¶Œ: ì‹œ/êµ° ë‹¨ìœ„ë¡œ ê·¸ë£¹í™” (ì„œìš¸íŠ¹ë³„ì‹œëŠ” "ì„œìš¸", ì¸ì²œê´‘ì—­ì‹œëŠ” "ì¸ì²œ", ê²½ê¸°ë„ëŠ” ì‹œ/êµ°ëª…)
            query = (
                select(
                    State.city_name,
                    State.region_name,
                    func.avg(HouseScore.index_value).label('index_value'),
                    func.avg(HouseScore.index_change_rate).label('index_change_rate'),
                    func.count(HouseScore.index_id).label('region_count')
                )
                .join(State, HouseScore.region_id == State.region_id)
                .where(
                    and_(
                        HouseScore.is_deleted == False,
                        State.is_deleted == False,
                        HouseScore.index_type == index_type,
                        HouseScore.base_ym == base_ym
                    )
                )
                .group_by(State.city_name, State.region_name)
            )
            
            # ì§€ì—­ í•„í„° ì ìš©
            if region_filter is not None:
                query = query.where(region_filter)
            
            query = query.order_by(State.city_name, State.region_name)
            
            result = await db.execute(query)
            rows = result.fetchall()
            
            # ì‘ë‹µ ë°ì´í„° ìƒì„±: ì‹œ/êµ° ë‹¨ìœ„
            # êµ¬ ë‹¨ìœ„ ë°ì´í„°ë¥¼ ì‹œ/êµ° ë‹¨ìœ„ë¡œ ì§‘ê³„
            region_data_map: Dict[str, Dict[str, Any]] = {}
            fallback_data_map: Dict[str, Dict[str, Any]] = {}  # ì˜ˆì™¸ì²˜ë¦¬ìš©: ë¦¬, í¬, ê¸°í¥ ë°ì´í„° ì €ì¥
            
            for row in rows:
                city_name = row.city_name
                region_name = row.region_name or ""
                
                # ì›ë³¸ region_name í™•ì¸ (ì˜ˆì™¸ì²˜ë¦¬ìš©)
                original_normalized = region_name.replace("ì‹œ", "").replace("êµ°", "").replace("êµ¬", "").strip()
                
                # êµ¬ ë‹¨ìœ„ë¥¼ ì‹œ/êµ° ë‹¨ìœ„ë¡œ ì •ê·œí™” (ì˜ˆì™¸ì²˜ë¦¬ ì œì™¸)
                normalized_name = normalize_metropolitan_region_name_without_fallback(city_name, region_name)
                
                # ë¶ˆì™„ì „í•œ ì´ë¦„ í•„í„°ë§ (1ê¸€ì ë˜ëŠ” ì´ìƒí•œ ë°ì´í„°)
                if len(normalized_name) <= 1 or normalized_name == "í¥":
                    continue
                
                # "ê²½ê¸°ë„" ê°™ì€ ë„ ë‹¨ìœ„ ë°ì´í„° ì œì™¸
                if normalized_name == "ê²½ê¸°ë„" or normalized_name == "ê²½ê¸°":
                    continue
                
                index_value = float(row.index_value or 0)
                index_change_rate = float(row.index_change_rate) if row.index_change_rate is not None else None
                region_count = row.region_count or 0
                
                # ì˜ˆì™¸ì²˜ë¦¬ìš© ë°ì´í„° ì €ì¥ (ë¦¬, í¬, ê¸°í¥)
                if original_normalized == "ë¦¬":
                    if "ë¦¬" not in fallback_data_map:
                        fallback_data_map["ë¦¬"] = {
                            "total_value": index_value * region_count,
                            "total_count": region_count,
                            "index_change_rate": index_change_rate
                        }
                    else:
                        fallback_data_map["ë¦¬"]["total_value"] += index_value * region_count
                        fallback_data_map["ë¦¬"]["total_count"] += region_count
                    continue
                elif original_normalized == "í¬":
                    if "í¬" not in fallback_data_map:
                        fallback_data_map["í¬"] = {
                            "total_value": index_value * region_count,
                            "total_count": region_count,
                            "index_change_rate": index_change_rate
                        }
                    else:
                        fallback_data_map["í¬"]["total_value"] += index_value * region_count
                        fallback_data_map["í¬"]["total_count"] += region_count
                    continue
                elif original_normalized == "ê¸°í¥":
                    if "ê¸°í¥" not in fallback_data_map:
                        fallback_data_map["ê¸°í¥"] = {
                            "total_value": index_value * region_count,
                            "total_count": region_count,
                            "index_change_rate": index_change_rate
                        }
                    else:
                        fallback_data_map["ê¸°í¥"]["total_value"] += index_value * region_count
                        fallback_data_map["ê¸°í¥"]["total_count"] += region_count
                    continue
                
                # ê°™ì€ ì‹œ/êµ°ì˜ ë°ì´í„°ë¥¼ ì§‘ê³„ (í‰ê· )
                if normalized_name not in region_data_map:
                    region_data_map[normalized_name] = {
                        "total_value": index_value * region_count,
                        "total_count": region_count,
                        "index_change_rate": index_change_rate
                    }
                else:
                    region_data_map[normalized_name]["total_value"] += index_value * region_count
                    region_data_map[normalized_name]["total_count"] += region_count
                    # index_change_rateëŠ” ì²« ë²ˆì§¸ ê°’ ì‚¬ìš© (ë˜ëŠ” í‰ê·  ê³„ì‚° ê°€ëŠ¥)
            
            # ì˜ˆì™¸ì²˜ë¦¬: êµ¬ë¦¬, êµ°í¬, ì‹œí¥ì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ ë¦¬, í¬, ê¸°í¥ ë°ì´í„° ì‚¬ìš©
            if "êµ¬ë¦¬" not in region_data_map and "ë¦¬" in fallback_data_map:
                region_data_map["êµ¬ë¦¬"] = fallback_data_map["ë¦¬"]
            if "êµ°í¬" not in region_data_map and "í¬" in fallback_data_map:
                region_data_map["êµ°í¬"] = fallback_data_map["í¬"]
            if "ì‹œí¥" not in region_data_map and "ê¸°í¥" in fallback_data_map:
                region_data_map["ì‹œí¥"] = fallback_data_map["ê¸°í¥"]
            
            # ì§‘ê³„ëœ ë°ì´í„°ë¥¼ ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            # í—ˆìš©ëœ ìˆ˜ë„ê¶Œ ì§€ì—­ ëª©ë¡
            allowed_metropolitan_regions = {
                "ì—°ì²œ", "í¬ì²œ", "íŒŒì£¼", "ì–‘ì£¼", "ë™ë‘ì²œ", "ê°€í‰", "ê³ ì–‘", "ì˜ì •ë¶€", 
                "ë‚¨ì–‘ì£¼", "ì–‘í‰", "ê¹€í¬", "ì„œìš¸", "êµ¬ë¦¬", "í•˜ë‚¨", "ì¸ì²œ", "ë¶€ì²œ", 
                "ê´‘ëª…", "ê³¼ì²œ", "ê´‘ì£¼", "ì‹œí¥", "ì•ˆì–‘", "ì„±ë‚¨", "ì´ì²œ", "ì—¬ì£¼", 
                "ì•ˆì‚°", "êµ°í¬", "ì˜ì™•", "ìš©ì¸", "í™”ì„±", "ìˆ˜ì›", "ì•ˆì„±", "ì˜¤ì‚°", "í‰íƒ"
            }
            
            hpi_data = []
            for normalized_name, data in region_data_map.items():
                # í—ˆìš©ëœ ì§€ì—­ë§Œ í¬í•¨
                if normalized_name not in allowed_metropolitan_regions:
                    continue
                
                avg_value = data["total_value"] / data["total_count"] if data["total_count"] > 0 else 0
                
                hpi_data.append(HPIRegionTypeDataPoint(
                    id=None,
                    name=normalized_name,
                    value=round(avg_value, 2),
                    index_change_rate=round(data["index_change_rate"], 2) if data["index_change_rate"] is not None else None
                ))
        else:
            # ì „êµ­, ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ: ì‹œë„ ë ˆë²¨ë¡œ ê·¸ë£¹í™”
            query = (
                select(
                    State.city_name,
                    func.avg(HouseScore.index_value).label('index_value'),
                    func.avg(HouseScore.index_change_rate).label('index_change_rate'),
                    func.count(HouseScore.index_id).label('region_count')
                )
                .join(State, HouseScore.region_id == State.region_id)
                .where(
                    and_(
                        HouseScore.is_deleted == False,
                        State.is_deleted == False,
                        HouseScore.index_type == index_type,
                        HouseScore.base_ym == base_ym
                    )
                )
                .group_by(State.city_name)
            )
            
            # ì§€ì—­ í•„í„° ì ìš©
            if region_filter is not None:
                query = query.where(region_filter)
            
            query = query.order_by(State.city_name)
            
            result = await db.execute(query)
            rows = result.fetchall()
            
            # ì‘ë‹µ ë°ì´í„° ìƒì„±: ì‹œë„ ë‹¨ìœ„
            hpi_data = []
            for row in rows:
                city_name = row.city_name
                normalized_name = normalize_city_name(city_name)
                index_value = float(row.index_value or 0)
                index_change_rate = float(row.index_change_rate) if row.index_change_rate is not None else None
                
                hpi_data.append(HPIRegionTypeDataPoint(
                    id=None,
                    name=normalized_name,
                    value=round(index_value, 2),
                    index_change_rate=round(index_change_rate, 2) if index_change_rate is not None else None
                ))
        
        response_data = HPIRegionTypeResponse(
            success=True,
            data=hpi_data,
            region_type=region_type,
            index_type=index_type,
            base_ym=base_ym
        )
        
        # ìºì‹œì— ì €ì¥
        if len(hpi_data) > 0:
            await set_to_cache(cache_key, response_data.dict(), ttl=STATISTICS_CACHE_TTL)
        
        logger.info(f"âœ… [Statistics HPI Region Type] HPI ë°ì´í„° ìƒì„± ì™„ë£Œ - ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(hpi_data)}")
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [Statistics HPI Region Type] HPI ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"HPI ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/transaction-volume",
    response_model=TransactionVolumeResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“Š Statistics (í†µê³„)"],
    summary="ê±°ë˜ëŸ‰ ì¡°íšŒ (ì›”ë³„ ë°ì´í„°)",
    description="""
    ì „êµ­, ìˆ˜ë„ê¶Œ, ì§€ë°©5ëŒ€ê´‘ì—­ì‹œì˜ ì›”ë³„ ê±°ë˜ëŸ‰ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    ### ì§€ì—­ ìœ í˜•
    - **ì „êµ­**: ì „ì²´ ì§€ì—­
    - **ìˆ˜ë„ê¶Œ**: ì„œìš¸íŠ¹ë³„ì‹œ, ê²½ê¸°ë„, ì¸ì²œê´‘ì—­ì‹œ
    - **ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ**: ë¶€ì‚°ê´‘ì—­ì‹œ, ëŒ€êµ¬ê´‘ì—­ì‹œ, ê´‘ì£¼ê´‘ì—­ì‹œ, ëŒ€ì „ê´‘ì—­ì‹œ, ìš¸ì‚°ê´‘ì—­ì‹œ
    
    ### ë°ì´í„° í˜•ì‹
    - ì›”ë³„ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤ (ì—°ë„, ì›”, ê±°ë˜ëŸ‰)
    - ì§€ë°©5ëŒ€ê´‘ì—­ì‹œì˜ ê²½ìš° `city_name` í•„ë“œê°€ í¬í•¨ë©ë‹ˆë‹¤
    - í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì—°ë„ë³„ ì§‘ê³„ ë˜ëŠ” ì›”ë³„ ë·°ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤
    
    ### Query Parameters
    - `region_type`: ì§€ì—­ ìœ í˜• (í•„ìˆ˜) - "ì „êµ­", "ìˆ˜ë„ê¶Œ", "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"
    - `transaction_type`: ê±°ë˜ ìœ í˜• (ì„ íƒ) - "sale"(ë§¤ë§¤), "rent"(ì „ì›”ì„¸), ê¸°ë³¸ê°’: "sale"
    - `max_years`: ìµœëŒ€ ì—°ë„ ìˆ˜ (ì„ íƒ) - 1~10, ê¸°ë³¸ê°’: 10
    """
)
async def get_transaction_volume(
    region_type: str = Query(..., description="ì§€ì—­ ìœ í˜•: ì „êµ­, ìˆ˜ë„ê¶Œ, ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"),
    transaction_type: str = Query("sale", description="ê±°ë˜ ìœ í˜•: sale(ë§¤ë§¤), rent(ì „ì›”ì„¸)"),
    max_years: int = Query(10, ge=1, le=10, description="ìµœëŒ€ ì—°ë„ ìˆ˜ (1~10)"),
    db: AsyncSession = Depends(get_db)
):
    """
    ê±°ë˜ëŸ‰ ì¡°íšŒ (ì›”ë³„ ë°ì´í„°)
    
    ìµœê·¼ Në…„ì¹˜ ì›”ë³„ ê±°ë˜ëŸ‰ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì—°ë„ë³„ ì§‘ê³„ ë˜ëŠ” ì›”ë³„ ë·°ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    # íŒŒë¼ë¯¸í„° ê²€ì¦
    if region_type not in ["ì „êµ­", "ìˆ˜ë„ê¶Œ", "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"]:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ region_type: {region_type}. í—ˆìš© ê°’: ì „êµ­, ìˆ˜ë„ê¶Œ, ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"
        )
    
    if transaction_type not in ["sale", "rent"]:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ transaction_type: {transaction_type}. í—ˆìš© ê°’: sale, rent"
        )
    
    # ìºì‹œ í‚¤ ìƒì„±
    cache_key = build_cache_key(
        "statistics", "volume", region_type, transaction_type, str(max_years)
    )
    
    # ìºì‹œì—ì„œ ì¡°íšŒ ì‹œë„ (ì¼ì‹œì ìœ¼ë¡œ ë¹„í™œì„±í™”í•˜ì—¬ DB ì§ì ‘ ì¡°íšŒ)
    # TODO: ì›ì¸ íŒŒì•… í›„ ì¬í™œì„±í™”
    cached_data = await get_from_cache(cache_key)
    if cached_data is not None:
        # ìºì‹œëœ ë°ì´í„°ì˜ ì—°ë„ ë²”ìœ„ í™•ì¸ (ë””ë²„ê¹…ìš©)
        if cached_data.get("data"):
            cached_years = sorted(set(int(item.get("year", 0)) for item in cached_data.get("data", [])), reverse=True)
            cached_data_count = len(cached_data.get("data", []))
            logger.warning(
                f"âš ï¸ [Statistics Transaction Volume] ìºì‹œ ë°œê²¬ (ë¬´ì‹œí•˜ê³  DB ì¡°íšŒ) - "
                f"region_type: {region_type}, "
                f"ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {cached_data_count}, "
                f"ì—°ë„ ë²”ìœ„: {cached_years[0] if cached_years else 'N/A'} ~ {cached_years[-1] if cached_years else 'N/A'}, "
                f"ìºì‹œ í‚¤: {cache_key}"
            )
            # ìºì‹œ ë¬´ì‹œí•˜ê³  DBì—ì„œ ì§ì ‘ ì¡°íšŒ (ë””ë²„ê¹…ìš©)
            # return cached_data
        else:
            logger.info(f"âœ… [Statistics Transaction Volume] ìºì‹œì—ì„œ ë°˜í™˜ (ë°ì´í„° ì—†ìŒ) - region_type: {region_type}, ìºì‹œ í‚¤: {cache_key}")
            return cached_data
    
    try:
        logger.info(
            f"ğŸ” [Statistics Transaction Volume] ê±°ë˜ëŸ‰ ë°ì´í„° ì¡°íšŒ ì‹œì‘ - "
            f"region_type: {region_type}, transaction_type: {transaction_type}, max_years: {max_years}"
        )
        
        # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì—°ë„ ë²”ìœ„ ê³„ì‚°
        current_date = date.today()
        start_year = current_date.year - max_years + 1
        start_date = date(start_year, 1, 1)
        end_date = current_date
        
        logger.info(
            f"ğŸ“… [Statistics Transaction Volume] ë‚ ì§œ ë²”ìœ„ ì„¤ì • - "
            f"start_date: {start_date}, end_date: {end_date}, "
            f"start_year: {start_year}, max_years: {max_years}"
        )
        
        # ê±°ë˜ ìœ í˜•ì— ë”°ë¥¸ í…Œì´ë¸” ë° í•„ë“œ ì„ íƒ
        if transaction_type == "sale":
            trans_table = Sale
            date_field = Sale.contract_date
            base_filter = and_(
                Sale.is_canceled == False,
                (Sale.is_deleted == False) | (Sale.is_deleted.is_(None)),
                Sale.contract_date.isnot(None),
                # remarks í•„í„°: í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ëª¨ë‘ 'ë”ë¯¸'ì´ë¯€ë¡œ ì¼ë‹¨ ì œê±°
                # TODO: ì‹¤ì œ ìš´ì˜ ë°ì´í„°ì—ì„œ ë”ë¯¸ ë°ì´í„° ì œì™¸ í•„ìš” ì‹œ ì¬í™œì„±í™”
                # or_(Sale.remarks != "ë”ë¯¸", Sale.remarks.is_(None)),
                Sale.contract_date >= start_date,
                Sale.contract_date <= end_date
            )
        else:  # rent
            trans_table = Rent
            date_field = Rent.deal_date
            base_filter = and_(
                (Rent.is_deleted == False) | (Rent.is_deleted.is_(None)),
                Rent.deal_date.isnot(None),
                # remarks í•„í„°: í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ëª¨ë‘ 'ë”ë¯¸'ì´ë¯€ë¡œ ì¼ë‹¨ ì œê±°
                # TODO: ì‹¤ì œ ìš´ì˜ ë°ì´í„°ì—ì„œ ë”ë¯¸ ë°ì´í„° ì œì™¸ í•„ìš” ì‹œ ì¬í™œì„±í™”
                # or_(Rent.remarks != "ë”ë¯¸", Rent.remarks.is_(None)),
                Rent.deal_date >= start_date,
                Rent.deal_date <= end_date
            )
        
        # ë””ë²„ê¹…: ì‹¤ì œ DBì— ì¡´ì¬í•˜ëŠ” ìµœì‹  ë°ì´í„° ì—°ë„ í™•ì¸ (í•„í„° ì „)
        max_date_query_all = select(
            func.max(date_field).label('max_date'),
            func.min(date_field).label('min_date'),
            func.count().label('total_count')
        ).select_from(trans_table).where(
            and_(
                trans_table.is_canceled == False if transaction_type == "sale" else True,
                (trans_table.is_deleted == False) | (trans_table.is_deleted.is_(None)),
                date_field.isnot(None)
            )
        )
        max_date_result_all = await db.execute(max_date_query_all)
        max_date_row_all = max_date_result_all.first()
        
        # ë””ë²„ê¹…: remarks ê°’ ë¶„í¬ í™•ì¸
        remarks_dist_query = select(
            trans_table.remarks,
            func.count().label('count')
        ).select_from(trans_table).where(
            and_(
                trans_table.is_canceled == False if transaction_type == "sale" else True,
                (trans_table.is_deleted == False) | (trans_table.is_deleted.is_(None)),
                date_field.isnot(None),
                date_field >= start_date,
                date_field <= end_date
            )
        ).group_by(trans_table.remarks).limit(10)
        remarks_dist_result = await db.execute(remarks_dist_query)
        remarks_dist_rows = remarks_dist_result.all()
        remarks_dist = {row.remarks or 'NULL': row.count for row in remarks_dist_rows}
        logger.info(
            f"ğŸ” [Statistics Transaction Volume] remarks ê°’ ë¶„í¬ í™•ì¸ - "
            f"{remarks_dist}"
        )
        
        # ë””ë²„ê¹…: í•„í„° ì ìš© í›„ ë°ì´í„° í™•ì¸ (remarks í•„í„° ì œì™¸)
        max_date_query_no_remarks = select(
            func.max(date_field).label('max_date'),
            func.min(date_field).label('min_date'),
            func.count().label('total_count')
        ).select_from(trans_table).where(
            and_(
                trans_table.is_canceled == False if transaction_type == "sale" else True,
                (trans_table.is_deleted == False) | (trans_table.is_deleted.is_(None)),
                date_field.isnot(None),
                date_field >= start_date,
                date_field <= end_date
            )
        )
        max_date_result_no_remarks = await db.execute(max_date_query_no_remarks)
        max_date_row_no_remarks = max_date_result_no_remarks.first()
        
        # ë””ë²„ê¹…: base_filter ì ìš© í›„ ë°ì´í„° í™•ì¸ (remarks í•„í„° ì œê±°ë¨)
        max_date_query = select(
            func.max(date_field).label('max_date'),
            func.min(date_field).label('min_date'),
            func.count().label('total_count')
        ).select_from(trans_table).where(
            and_(
                trans_table.is_canceled == False if transaction_type == "sale" else True,
                (trans_table.is_deleted == False) | (trans_table.is_deleted.is_(None)),
                date_field.isnot(None),
                # remarks í•„í„° ì œê±°ë¨ (í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ëª¨ë‘ 'ë”ë¯¸')
                date_field >= start_date,
                date_field <= end_date
            )
        )
        max_date_result = await db.execute(max_date_query)
        max_date_row = max_date_result.first()
        
        if max_date_row_all:
            logger.info(
                f"ğŸ” [Statistics Transaction Volume] DB ì „ì²´ ë°ì´í„° ë²”ìœ„ (í•„í„° ì „) - "
                f"ìµœì‹  ë‚ ì§œ: {max_date_row_all.max_date}, "
                f"ìµœ old ë‚ ì§œ: {max_date_row_all.min_date}, "
                f"ì „ì²´ ê±°ë˜ ìˆ˜: {max_date_row_all.total_count}"
            )
        
        if max_date_row_no_remarks:
            logger.info(
                f"ğŸ” [Statistics Transaction Volume] DB ë°ì´í„° ë²”ìœ„ (remarks í•„í„° ì œì™¸) - "
                f"ìµœì‹  ë‚ ì§œ: {max_date_row_no_remarks.max_date}, "
                f"ìµœ old ë‚ ì§œ: {max_date_row_no_remarks.min_date}, "
                f"ê±°ë˜ ìˆ˜: {max_date_row_no_remarks.total_count}"
            )
        
        if max_date_row and max_date_row.max_date:
            logger.info(
                f"ğŸ” [Statistics Transaction Volume] DB ì‹¤ì œ ë°ì´í„° ë²”ìœ„ (base_filter ì ìš©) - "
                f"ìµœì‹  ë‚ ì§œ: {max_date_row.max_date}, "
                f"ìµœ old ë‚ ì§œ: {max_date_row.min_date}, "
                f"í•„í„°ë§ëœ ê±°ë˜ ìˆ˜: {max_date_row.total_count}, "
                f"ë‚ ì§œ ë²”ìœ„: {start_date} ~ {end_date}"
            )
            
            # ë‚ ì§œ ë²”ìœ„ì™€ ì‹¤ì œ ë°ì´í„° ë²”ìœ„ ë¹„êµ
            if max_date_row.min_date and max_date_row.min_date > start_date:
                logger.warning(
                    f"âš ï¸ [Statistics Transaction Volume] ë‚ ì§œ ë²”ìœ„ ë¶ˆì¼ì¹˜ - "
                    f"ìš”ì²­í•œ ì‹œì‘ ë‚ ì§œ: {start_date}, "
                    f"ì‹¤ì œ ë°ì´í„° ìµœì†Œ ë‚ ì§œ: {max_date_row.min_date}, "
                    f"ì°¨ì´: {(max_date_row.min_date - start_date).days}ì¼"
                )
        
        # ì§€ì—­ í•„í„°ë§ ì¡°ê±´ ê°€ì ¸ì˜¤ê¸°
        region_filter = get_region_type_filter(region_type)
        
        # ë””ë²„ê¹…: ì‹¤ì œ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ìˆ˜ë„ê¶Œ, ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ)
        if region_type in ["ìˆ˜ë„ê¶Œ", "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"]:
            debug_query = select(
                extract('year', date_field).label('year'),
                func.count().label('count')
            ).select_from(
                trans_table
            ).join(
                Apartment, trans_table.apt_id == Apartment.apt_id
            ).join(
                State, Apartment.region_id == State.region_id
            ).where(
                and_(base_filter, region_filter)
            ).group_by(
                extract('year', date_field)
            ).order_by(
                desc(extract('year', date_field))
            )
            debug_result = await db.execute(debug_query)
            debug_rows = debug_result.all()
            debug_years = [int(row.year) for row in debug_rows[:10]]  # ìµœì‹  10ê°œ ì—°ë„
            logger.info(
                f"ğŸ” [Statistics Transaction Volume] {region_type} ì‹¤ì œ ë°ì´í„° ì—°ë„ í™•ì¸ - "
                f"ìµœì‹  ì—°ë„: {debug_years[0] if debug_years else 'N/A'}, "
                f"ì—°ë„ ëª©ë¡: {debug_years}, "
                f"ì´ {len(debug_rows)}ê°œ ì—°ë„ ë°ì´í„° ì¡´ì¬"
            )
            
            # JOIN ì „ ë°ì´í„° í™•ì¸ (ë””ë²„ê¹…ìš©) - í•­ìƒ ì‹¤í–‰
            # JOIN ì—†ì´ ê±°ë˜ ë°ì´í„°ë§Œ í™•ì¸
            no_join_query = select(
                extract('year', date_field).label('year'),
                func.count().label('count')
            ).select_from(
                trans_table
            ).where(
                base_filter
            ).group_by(
                extract('year', date_field)
            ).order_by(
                desc(extract('year', date_field))
            )
            no_join_result = await db.execute(no_join_query)
            no_join_rows = no_join_result.all()
            no_join_years = [int(row.year) for row in no_join_rows[:10]]
            
            if len(debug_rows) == 0 and len(no_join_rows) > 0:
                logger.warning(
                    f"âš ï¸ [Statistics Transaction Volume] {region_type} JOINìœ¼ë¡œ ì¸í•œ ë°ì´í„° ì†ì‹¤ í™•ì¸ - "
                    f"JOIN ì „: {len(no_join_rows)}ê°œ ì—°ë„ (ìµœì‹ : {no_join_years[0] if no_join_years else 'N/A'}), "
                    f"JOIN í›„: 0ê°œ ì—°ë„ (JOIN ì¡°ê±´ ë¬¸ì œ ê°€ëŠ¥ì„±)"
                )
            elif len(debug_rows) > 0:
                logger.info(
                    f"âœ… [Statistics Transaction Volume] {region_type} JOIN ì „/í›„ ë°ì´í„° ë¹„êµ - "
                    f"JOIN ì „: {len(no_join_rows)}ê°œ ì—°ë„, JOIN í›„: {len(debug_rows)}ê°œ ì—°ë„"
                )
        
        # ì¿¼ë¦¬ êµ¬ì„±
        if region_type == "ì „êµ­":
            # ì „êµ­: JOIN ì—†ì´ ê±°ë˜ í…Œì´ë¸”ë§Œ ì‚¬ìš©
            # ë””ë²„ê¹…: ì „êµ­ ì¿¼ë¦¬ ì „ì— base_filter ì ìš© ê²°ê³¼ í™•ì¸
            debug_national_query = select(
                extract('year', date_field).label('year'),
                func.count().label('count')
            ).select_from(
                trans_table
            ).where(
                base_filter
            ).group_by(
                extract('year', date_field)
            ).order_by(
                desc(extract('year', date_field))
            )
            debug_national_result = await db.execute(debug_national_query)
            debug_national_rows = debug_national_result.all()
            debug_national_years = [int(row.year) for row in debug_national_rows]
            logger.info(
                f"ğŸ” [Statistics Transaction Volume] ì „êµ­ base_filter ì ìš© í›„ ì—°ë„ í™•ì¸ - "
                f"ì—°ë„ ëª©ë¡: {debug_national_years[:10]}, "
                f"ì´ {len(debug_national_rows)}ê°œ ì—°ë„"
            )
            
            query = select(
                extract('year', date_field).label('year'),
                extract('month', date_field).label('month'),
                func.count().label('volume')
            ).select_from(
                trans_table
            ).where(
                base_filter
            ).group_by(
                extract('year', date_field),
                extract('month', date_field)
            ).order_by(
                desc(extract('year', date_field)),
                extract('month', date_field)
            )
        elif region_type == "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ":
            # ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ: city_name í¬í•¨í•˜ì—¬ ê·¸ë£¹í™”
            # ë””ë²„ê¹…: ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ JOIN ì „ ë°ì´í„° í™•ì¸
            debug_local_before_join = select(
                extract('year', date_field).label('year'),
                func.count().label('count')
            ).select_from(
                trans_table
            ).where(
                base_filter
            ).group_by(
                extract('year', date_field)
            ).order_by(
                desc(extract('year', date_field))
            )
            debug_local_before_result = await db.execute(debug_local_before_join)
            debug_local_before_rows = debug_local_before_result.all()
            debug_local_before_years = [int(row.year) for row in debug_local_before_rows]
            logger.info(
                f"ğŸ” [Statistics Transaction Volume] ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ JOIN ì „ ì—°ë„ í™•ì¸ - "
                f"ì—°ë„ ëª©ë¡: {debug_local_before_years[:10]}, "
                f"ì´ {len(debug_local_before_rows)}ê°œ ì—°ë„"
            )
            
            query = select(
                extract('year', date_field).label('year'),
                extract('month', date_field).label('month'),
                State.city_name.label('city_name'),
                func.count().label('volume')
            ).select_from(
                trans_table
            ).join(
                Apartment, trans_table.apt_id == Apartment.apt_id
            ).join(
                State, Apartment.region_id == State.region_id
            ).where(
                and_(base_filter, region_filter)
            ).group_by(
                extract('year', date_field),
                extract('month', date_field),
                State.city_name
            ).order_by(
                desc(extract('year', date_field)),
                extract('month', date_field),
                State.city_name
            )
        else:  # ìˆ˜ë„ê¶Œ
            # ìˆ˜ë„ê¶Œ: JOIN ì‚¬ìš©í•˜ì§€ë§Œ city_nameì€ ê·¸ë£¹í™”í•˜ì§€ ì•ŠìŒ
            query = select(
                extract('year', date_field).label('year'),
                extract('month', date_field).label('month'),
                func.count().label('volume')
            ).select_from(
                trans_table
            ).join(
                Apartment, trans_table.apt_id == Apartment.apt_id
            ).join(
                State, Apartment.region_id == State.region_id
            ).where(
                and_(base_filter, region_filter)
            ).group_by(
                extract('year', date_field),
                extract('month', date_field)
            ).order_by(
                desc(extract('year', date_field)),
                extract('month', date_field)
            )
        
        # ì¿¼ë¦¬ ì‹¤í–‰
        result = await db.execute(query)
        rows = result.all()
        
        logger.info(
            f"ğŸ“Š [Statistics Transaction Volume] ì¿¼ë¦¬ ê²°ê³¼ - "
            f"ì´ {len(rows)}ê°œ í–‰ ë°˜í™˜, region_type: {region_type}"
        )
        
        # ë°ì´í„°ê°€ ì—†ì„ ë•Œ ìƒì„¸ ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
        if len(rows) == 0 and region_type in ["ìˆ˜ë„ê¶Œ", "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"]:
            # JOIN ì—†ì´ ì „ì²´ ê±°ë˜ ë°ì´í„° í™•ì¸
            total_count_query = select(func.count()).select_from(trans_table).where(base_filter)
            total_result = await db.execute(total_count_query)
            total_count = total_result.scalar() or 0
            
            logger.warning(
                f"âš ï¸ [Statistics Transaction Volume] {region_type} ë°ì´í„° ì—†ìŒ - "
                f"base_filter ì ìš© í›„ ì „ì²´ ê±°ë˜ ìˆ˜: {total_count}, "
                f"JOIN í›„ ë°ì´í„°: 0ê°œ (JOIN ì¡°ê±´ ë¬¸ì œ ê°€ëŠ¥ì„± ë†’ìŒ)"
            )
        
        # ì—°ë„ë³„ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ë””ë²„ê¹…ìš©)
        if rows:
            years = sorted(set(int(row.year) for row in rows), reverse=True)
            logger.info(
                f"ğŸ“Š [Statistics Transaction Volume] DB ì¿¼ë¦¬ ê²°ê³¼ - "
                f"region_type: {region_type}, "
                f"ë°ì´í„° í–‰ ìˆ˜: {len(rows)}, "
                f"ì—°ë„ ë²”ìœ„: {years[0] if years else 'N/A'} ~ {years[-1] if years else 'N/A'}, "
                f"ì „ì²´ ì—°ë„: {years[:10] if len(years) <= 10 else years[:10] + ['...']}, "
                f"ìºì‹œ í‚¤: {cache_key}"
            )
        
        # ë°ì´í„° í¬ì¸íŠ¸ ìƒì„±
        data_points = []
        for row in rows:
            data_point = TransactionVolumeDataPoint(
                year=int(row.year),
                month=int(row.month),
                volume=int(row.volume),
                city_name=row.city_name if hasattr(row, 'city_name') and row.city_name else None
            )
            data_points.append(data_point)
        
        # ê¸°ê°„ ë¬¸ìì—´ ìƒì„±
        period_str = f"{start_date.strftime('%Y-%m')} ~ {end_date.strftime('%Y-%m')}"
        
        # ì‘ë‹µ ìƒì„±
        response_data = TransactionVolumeResponse(
            success=True,
            data=data_points,
            region_type=region_type,
            period=period_str,
            max_years=max_years
        )
        
        # ìºì‹œì— ì €ì¥
        if len(data_points) > 0:
            await set_to_cache(cache_key, response_data.dict(), ttl=STATISTICS_CACHE_TTL)
            logger.info(
                f"ğŸ’¾ [Statistics Transaction Volume] ìºì‹œ ì €ì¥ ì™„ë£Œ - "
                f"region_type: {region_type}, "
                f"ì—°ë„ ë²”ìœ„: {years[0] if years else 'N/A'} ~ {years[-1] if years else 'N/A'}"
            )
        
        logger.info(
            f"âœ… [Statistics Transaction Volume] ê±°ë˜ëŸ‰ ë°ì´í„° ìƒì„± ì™„ë£Œ - "
            f"ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(data_points)}, ê¸°ê°„: {period_str}"
        )
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(
            f"âŒ [Statistics Transaction Volume] ê±°ë˜ëŸ‰ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}",
            exc_info=True
        )
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ê±°ë˜ëŸ‰ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )
