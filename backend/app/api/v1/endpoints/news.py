"""
ë‰´ìŠ¤ API ì—”ë“œí¬ì¸íŠ¸

ë¶€ë™ì‚° ë‰´ìŠ¤ í¬ë¡¤ë§ APIë¥¼ ì œê³µí•©ë‹ˆë‹¤.
DB ì €ì¥ì´ ì—†ìœ¼ë¯€ë¡œ í¬ë¡¤ë§ ê²°ê³¼ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
ìºì‹±ì„ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤.
"""
import logging
from datetime import datetime, timedelta
from typing import Dict, Tuple, Any, Optional
from fastapi import APIRouter, HTTPException, status, Query, Depends
from sqlalchemy.ext.asyncio import AsyncSession

from app.services.news import news_service
from app.schemas.news import NewsListResponse, NewsDetailResponse, NewsResponse
from app.utils.news import generate_news_id, filter_news_by_location
from app.api.v1.deps import get_db
from app.crud.apartment import apartment as apartment_crud
from app.crud.state import state as state_crud

logger = logging.getLogger(__name__)

router = APIRouter()

# ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ìºì‹œ (í”„ë¡œë•ì…˜ì—ì„œëŠ” Redis ì‚¬ìš© ê¶Œì¥)
_cache: Dict[str, Tuple[Any, datetime]] = {}
CACHE_TTL = timedelta(minutes=5)  # 5ë¶„ê°„ ìºì‹œ ìœ ì§€


def get_cached_data(cache_key: str):
    """ìºì‹œì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    if cache_key in _cache:
        cached_data, cached_time = _cache[cache_key]
        if datetime.now() - cached_time < CACHE_TTL:
            logger.debug(f"ìºì‹œ íˆíŠ¸: {cache_key}")
            return cached_data
        else:
            # ìºì‹œ ë§Œë£Œ
            del _cache[cache_key]
            logger.debug(f"ìºì‹œ ë§Œë£Œ: {cache_key}")
    return None


def set_cached_data(cache_key: str, data: any):
    """ìºì‹œì— ë°ì´í„° ì €ì¥"""
    _cache[cache_key] = (data, datetime.now())
    logger.debug(f"ìºì‹œ ì €ì¥: {cache_key}")


@router.get(
    "",
    response_model=NewsListResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“° News (ë‰´ìŠ¤)"],
    summary="ë‰´ìŠ¤ ëª©ë¡ í¬ë¡¤ë§ ë° ì¡°íšŒ",
    description="""
    ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ë¶€ë™ì‚° ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    - DB ì €ì¥ ì—†ì´ ì‹¤ì‹œê°„ í¬ë¡¤ë§ ê²°ê³¼ë§Œ ë°˜í™˜
    - ìºì‹± ì ìš© (5ë¶„ TTL)
    - ì†ŒìŠ¤ë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜ ì œí•œ ê°€ëŠ¥
    
    íŒŒë¼ë¯¸í„°:
    - limit_per_source: ì†ŒìŠ¤ë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜ (í•„ìˆ˜)
    - apt_id: ì•„íŒŒíŠ¸ ID (ì„ íƒì )
    
    ë™ì‘ ë°©ì‹:
    - apt_idê°€ ì „ë‹¬ë˜ì§€ ì•Šì„ ê²½ìš°: ê¸°ì¡´ ê¸°ëŠ¥ê³¼ ê°™ì´ ê°œìˆ˜ë§Œí¼ í¬ë¡¤ë§í•˜ì—¬ ë°˜í™˜
    - apt_idê°€ ì „ë‹¬ë  ê²½ìš°: 
      * apartment í…Œì´ë¸”ê³¼ state í…Œì´ë¸”ì„ ì°¸ê³ í•´ì„œ ì‹œ, ë™, ì•„íŒŒíŠ¸ ì´ë¦„ ì•Œì•„ë‚´ê¸°
      * ê²€ìƒ‰ ë‹¨ê³„ë³„ë¡œ ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰:
        1. ì‹œ + ë¶€ë™ì‚°
        2. ì‹œ + ë™ + ë¶€ë™ì‚°
        3. ì‹œ + ë™ + ì•„íŒŒíŠ¸ì´ë¦„ + ë¶€ë™ì‚°
      * ê´€ë ¨ëœ ë‰´ìŠ¤ 5ê°œ ëª©ë¡ì„ ë°˜í™˜
      * ë°˜í™˜ê°’ì— ì‹œ, ë™, ì•„íŒŒíŠ¸ ì´ë¦„ í¬í•¨ (meta í•„ë“œ)
    """,
    responses={
        200: {"description": "í¬ë¡¤ë§ ì™„ë£Œ"},
        404: {"description": "ì•„íŒŒíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"},
        500: {"description": "í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"}
    }
)
async def get_news(
    limit_per_source: int = Query(20, ge=1, le=100, description="ì†ŒìŠ¤ë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜"),
    apt_id: Optional[int] = Query(None, description="ì•„íŒŒíŠ¸ ID (apartments.apt_id)"),
    db: AsyncSession = Depends(get_db)
):
    """ë‰´ìŠ¤ ëª©ë¡ í¬ë¡¤ë§ ë° ì¡°íšŒ"""
    try:
        si = None
        dong = None
        apartment_name = None
        region_name = None
        
        # apt_idê°€ ì „ë‹¬ëœ ê²½ìš°, ì•„íŒŒíŠ¸ ì •ë³´ ì¡°íšŒ
        if apt_id is not None:
            # 1. Apartments í…Œì´ë¸”ì—ì„œ region_idì™€ apt_name ì°¾ê¸°
            apartment = await apartment_crud.get(db, id=apt_id)
            if not apartment or apartment.is_deleted:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail=f"ì•„íŒŒíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: apt_id={apt_id}"
                )
            
            apartment_name = apartment.apt_name
            region_id = apartment.region_id
            
            # 2. States í…Œì´ë¸”ì—ì„œ region_name ì°¾ê¸°
            state = await state_crud.get(db, id=region_id)
            if not state or state.is_deleted:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail=f"ì§€ì—­ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: region_id={region_id}"
                )
            
            region_name = state.region_name
            city_name = state.city_name
            
            # 3. ì‹œëŠ” city_nameì—ì„œ, ë™ì€ region_nameì—ì„œ ê°€ì ¸ì˜¤ê¸°
            # city_nameì„ ì‹œë¡œ ì‚¬ìš© (ì˜ˆ: "ì„œìš¸íŠ¹ë³„ì‹œ" -> "ì„œìš¸ì‹œ")
            si = city_name
            if si:
                if "íŠ¹ë³„ì‹œ" in si:
                    si = si.replace("íŠ¹ë³„ì‹œ", "ì‹œ")
                elif "ê´‘ì—­ì‹œ" in si:
                    si = si.replace("ê´‘ì—­ì‹œ", "ì‹œ")
                elif not si.endswith("ì‹œ"):
                    si = si + "ì‹œ"
            
            # ë™ì€ region_nameì—ì„œ "ë™"ìœ¼ë¡œ ëë‚˜ëŠ” ê²½ìš°ë§Œ ì¶”ì¶œ (ì˜ˆ: "ì ì‹¤ë™")
            if region_name and region_name.endswith("ë™"):
                dong = region_name
            else:
                dong = None
        
        # ìºì‹œ í‚¤ ìƒì„± (apt_id ë˜ëŠ” ì§€ì—­ íŒŒë¼ë¯¸í„° í¬í•¨)
        if apt_id is not None:
            cache_key = f"news_list_{limit_per_source}_apt_{apt_id}"
        else:
            cache_key = f"news_list_{limit_per_source}"
        
        # ìºì‹œ í™•ì¸
        cached_result = get_cached_data(cache_key)
        if cached_result:
            return cached_result
        
        # ìºì‹œ ì—†ìœ¼ë©´ í¬ë¡¤ë§ ì‹¤í–‰
        crawled_news = await news_service.crawl_only(limit_per_source=limit_per_source)
        
        # apt_idê°€ ì „ë‹¬ëœ ê²½ìš°, ì§€ì—­ í•„í„°ë§ ì ìš©í•˜ì—¬ 5ê°œ ë°˜í™˜
        if apt_id is not None:
            filtered_news = filter_news_by_location(
                news_list=crawled_news,
                si=si,
                dong=dong,
                apartment=apartment_name
            )
        else:
            # apt_idê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ì²˜ëŸ¼ ëª¨ë“  ë‰´ìŠ¤ ë°˜í™˜
            filtered_news = crawled_news
        
        # í¬ë¡¤ë§ ê²°ê³¼ë¥¼ NewsResponse ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜ (ê°„ë‹¨í•œ í•´ì‹œ ID ì¶”ê°€)
        from app.schemas.news import NewsResponse
        news_list = [
            NewsResponse(
                id=generate_news_id(news["url"]),
                **{k: v for k, v in news.items() if k not in ["relevance_score", "matched_category"]}
            ) for news in filtered_news
        ]
        
        # ë©”íƒ€ ì •ë³´ êµ¬ì„±
        meta = {
            "total": len(news_list),
            "limit": len(news_list),
            "offset": 0
        }
        
        # apt_idê°€ ì „ë‹¬ëœ ê²½ìš° ë©”íƒ€ ì •ë³´ì— ì¶”ê°€
        if apt_id is not None:
            meta.update({
                "apt_id": apt_id,
                "apt_name": apartment_name,
                "si": si,
                "dong": dong
            })
        
        response = NewsListResponse(
            success=True,
            data=news_list,
            meta=meta
        )
        
        # ìºì‹œì— ì €ì¥
        set_cached_data(cache_key, response)
        
        return response
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/detail",
    response_model=NewsDetailResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“° News (ë‰´ìŠ¤)"],
    summary="ë‰´ìŠ¤ ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§",
    description="""
    íŠ¹ì • ë‰´ìŠ¤ URLì˜ ìƒì„¸ ë‚´ìš©ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    
    - ë‰´ìŠ¤ ëª©ë¡ì—ì„œ ë°›ì€ `url` í•„ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„¸ ì¡°íšŒ
    - ìºì‹± ì ìš© (5ë¶„ TTL)
    - ì œëª©, ë³¸ë¬¸, ì¸ë„¤ì¼ ë“± ìƒì„¸ ì •ë³´ ë°˜í™˜
    """,
    responses={
        200: {"description": "í¬ë¡¤ë§ ì™„ë£Œ"},
        400: {"description": "ì˜ëª»ëœ URL"},
        404: {"description": "ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"},
        500: {"description": "í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"}
    }
)
async def get_news_detail_by_url(
    url: str = Query(..., description="ë‰´ìŠ¤ ìƒì„¸ í˜ì´ì§€ URL (ë‰´ìŠ¤ ëª©ë¡ì—ì„œ ë°›ì€ url í•„ë“œ ì‚¬ìš©)")
):
    """ë‰´ìŠ¤ ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§"""
    try:
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = f"news_detail_{generate_news_id(url)}"
        
        # ìºì‹œ í™•ì¸
        cached_result = get_cached_data(cache_key)
        if cached_result:
            return cached_result
        
        # ìºì‹œ ì—†ìœ¼ë©´ í¬ë¡¤ë§ ì‹¤í–‰
        detail = await news_service.crawl_news_detail(url=url)
        
        if not detail:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ í¬ë¡¤ë§ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {url}"
            )
        
        # ê°„ë‹¨í•œ í•´ì‹œ ID ìƒì„±í•˜ì—¬ ì‘ë‹µ ìƒì„±
        news_response = NewsResponse(
            id=generate_news_id(url),
            **detail
        )
        
        response = NewsDetailResponse(
            success=True,
            data=news_response
        )
        
        # ìºì‹œì— ì €ì¥
        set_cached_data(cache_key, response)
        
        return response
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ ìƒì„¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )
