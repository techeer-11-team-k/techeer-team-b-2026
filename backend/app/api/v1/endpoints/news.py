"""
ë‰´ìŠ¤ API ì—”ë“œí¬ì¸íŠ¸

ë¶€ë™ì‚° ë‰´ìŠ¤ í¬ë¡¤ë§ APIë¥¼ ì œê³µí•©ë‹ˆë‹¤.
DB ì €ì¥ì´ ì—†ìœ¼ë¯€ë¡œ í¬ë¡¤ë§ ê²°ê³¼ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
ìºì‹±ì„ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤.
"""
import logging
from datetime import datetime, timedelta
from typing import Dict, Tuple, Any
from fastapi import APIRouter, HTTPException, status, Query

from app.services.news import news_service
from app.schemas.news import NewsListResponse, NewsDetailResponse, NewsResponse
from app.utils.news import generate_news_id

logger = logging.getLogger(__name__)

router = APIRouter()

# ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ìºì‹œ (í”„ë¡œë•ì…˜ì—ì„œëŠ” Redis ì‚¬ìš© ê¶Œì¥)
_cache: Dict[str, Tuple[Any, datetime]] = {}
CACHE_TTL = timedelta(minutes=5)  # 5ë¶„ê°„ ìºì‹œ ìœ ì§€


def get_cached_data(cache_key: str):
    """ìºì‹œì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    if cache_key in _cache:
        cached_data, cached_time = _cache[cache_key]
        if datetime.now() - cached_time < CACHE_TTL:
            logger.debug(f"ìºì‹œ íˆíŠ¸: {cache_key}")
            return cached_data
        else:
            # ìºì‹œ ë§Œë£Œ
            del _cache[cache_key]
            logger.debug(f"ìºì‹œ ë§Œë£Œ: {cache_key}")
    return None


def set_cached_data(cache_key: str, data: any):
    """ìºì‹œì— ë°ì´í„° ì €ì¥"""
    _cache[cache_key] = (data, datetime.now())
    logger.debug(f"ìºì‹œ ì €ì¥: {cache_key}")


@router.get(
    "",
    response_model=NewsListResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“° News (ë‰´ìŠ¤)"],
    summary="ë‰´ìŠ¤ ëª©ë¡ í¬ë¡¤ë§ ë° ì¡°íšŒ",
    description="""
    ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ë¶€ë™ì‚° ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    - DB ì €ì¥ ì—†ì´ ì‹¤ì‹œê°„ í¬ë¡¤ë§ ê²°ê³¼ë§Œ ë°˜í™˜
    - ìºì‹± ì ìš© (5ë¶„ TTL)
    - ì†ŒìŠ¤ë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜ ì œí•œ ê°€ëŠ¥
    """,
    responses={
        200: {"description": "í¬ë¡¤ë§ ì™„ë£Œ"},
        500: {"description": "í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"}
    }
)
async def get_news(
    limit_per_source: int = Query(20, ge=1, le=100, description="ì†ŒìŠ¤ë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜")
):
    """ë‰´ìŠ¤ ëª©ë¡ í¬ë¡¤ë§ ë° ì¡°íšŒ"""
    try:
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = f"news_list_{limit_per_source}"
        
        # ìºì‹œ í™•ì¸
        cached_result = get_cached_data(cache_key)
        if cached_result:
            return cached_result
        
        # ìºì‹œ ì—†ìœ¼ë©´ í¬ë¡¤ë§ ì‹¤í–‰
        crawled_news = await news_service.crawl_only(limit_per_source=limit_per_source)
        
        # í¬ë¡¤ë§ ê²°ê³¼ë¥¼ NewsResponse ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜ (ê°„ë‹¨í•œ í•´ì‹œ ID ì¶”ê°€)
        from app.schemas.news import NewsResponse
        news_list = [
            NewsResponse(
                id=generate_news_id(news["url"]),
                **news
            ) for news in crawled_news
        ]
        
        response = NewsListResponse(
            success=True,
            data=news_list,
            meta={
                "total": len(news_list),
                "limit": len(news_list),
                "offset": 0
            }
        )
        
        # ìºì‹œì— ì €ì¥
        set_cached_data(cache_key, response)
        
        return response
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/detail",
    response_model=NewsDetailResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“° News (ë‰´ìŠ¤)"],
    summary="ë‰´ìŠ¤ ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§",
    description="""
    íŠ¹ì • ë‰´ìŠ¤ URLì˜ ìƒì„¸ ë‚´ìš©ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    
    - ë‰´ìŠ¤ ëª©ë¡ì—ì„œ ë°›ì€ `url` í•„ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„¸ ì¡°íšŒ
    - ìºì‹± ì ìš© (5ë¶„ TTL)
    - ì œëª©, ë³¸ë¬¸, ì¸ë„¤ì¼ ë“± ìƒì„¸ ì •ë³´ ë°˜í™˜
    """,
    responses={
        200: {"description": "í¬ë¡¤ë§ ì™„ë£Œ"},
        400: {"description": "ì˜ëª»ëœ URL"},
        404: {"description": "ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"},
        500: {"description": "í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"}
    }
)
async def get_news_detail_by_url(
    url: str = Query(..., description="ë‰´ìŠ¤ ìƒì„¸ í˜ì´ì§€ URL (ë‰´ìŠ¤ ëª©ë¡ì—ì„œ ë°›ì€ url í•„ë“œ ì‚¬ìš©)")
):
    """ë‰´ìŠ¤ ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§"""
    try:
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = f"news_detail_{generate_news_id(url)}"
        
        # ìºì‹œ í™•ì¸
        cached_result = get_cached_data(cache_key)
        if cached_result:
            return cached_result
        
        # ìºì‹œ ì—†ìœ¼ë©´ í¬ë¡¤ë§ ì‹¤í–‰
        detail = await news_service.crawl_news_detail(url=url)
        
        if not detail:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ í¬ë¡¤ë§ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {url}"
            )
        
        # ê°„ë‹¨í•œ í•´ì‹œ ID ìƒì„±í•˜ì—¬ ì‘ë‹µ ìƒì„±
        news_response = NewsResponse(
            id=generate_news_id(url),
            **detail
        )
        
        response = NewsDetailResponse(
            success=True,
            data=news_response
        )
        
        # ìºì‹œì— ì €ì¥
        set_cached_data(cache_key, response)
        
        return response
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ ìƒì„¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


