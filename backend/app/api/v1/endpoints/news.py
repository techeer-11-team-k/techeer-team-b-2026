"""
ë‰´ìŠ¤ API ì—”ë“œí¬ì¸íŠ¸

ë¶€ë™ì‚° ë‰´ìŠ¤ í¬ë¡¤ë§ APIë¥¼ ì œê³µí•©ë‹ˆë‹¤.
DB ì €ì¥ì´ ì—†ìœ¼ë¯€ë¡œ í¬ë¡¤ë§ ê²°ê³¼ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
Redis ìºì‹±ì„ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤.
"""
import logging
from datetime import datetime, timedelta
from typing import Dict, Tuple, Any, Optional, List
from fastapi import APIRouter, HTTPException, status, Query, Depends
from sqlalchemy.ext.asyncio import AsyncSession

from app.services.news import news_service
from app.schemas.news import NewsListResponse, NewsDetailResponse, NewsResponse
from app.utils.news import generate_news_id, filter_news_by_location, filter_news_by_keywords
from app.api.v1.deps import get_db
from app.crud.apartment import apartment as apartment_crud
from app.crud.state import state as state_crud
from app.utils.cache import get_from_cache, set_to_cache

logger = logging.getLogger(__name__)

router = APIRouter()

# Redis ìºì‹œ TTL (ì´ˆ)
NEWS_CACHE_TTL = 600  # 10ë¶„ê°„ ìºì‹œ ìœ ì§€ (ë‰´ìŠ¤ëŠ” ì‹¤ì‹œê°„ì„±ì´ ëœ ì¤‘ìš”)


async def get_cached_news_data(cache_key: str):
    """Redisì—ì„œ ë‰´ìŠ¤ ìºì‹œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    try:
        cached = await get_from_cache(f"news:{cache_key}")
        if cached:
            logger.debug(f"âœ… ë‰´ìŠ¤ ìºì‹œ íˆíŠ¸: {cache_key}")
            return cached
    except Exception as e:
        logger.warning(f"âš ï¸ ë‰´ìŠ¤ ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
    return None


async def set_cached_news_data(cache_key: str, data: any):
    """Redisì— ë‰´ìŠ¤ ìºì‹œ ë°ì´í„° ì €ì¥"""
    try:
        await set_to_cache(f"news:{cache_key}", data, ttl=NEWS_CACHE_TTL)
        logger.debug(f"âœ… ë‰´ìŠ¤ ìºì‹œ ì €ì¥: {cache_key}")
    except Exception as e:
        logger.warning(f"âš ï¸ ë‰´ìŠ¤ ìºì‹œ ì €ì¥ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")


@router.get(
    "",
    response_model=NewsListResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“° News (ë‰´ìŠ¤)"],
    summary="ë‰´ìŠ¤ ëª©ë¡ í¬ë¡¤ë§ ë° ì¡°íšŒ",
    description="""
    ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ë¶€ë™ì‚° ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    - DB ì €ì¥ ì—†ì´ ì‹¤ì‹œê°„ í¬ë¡¤ë§ ê²°ê³¼ë§Œ ë°˜í™˜
    - ìºì‹± ì ìš© (5ë¶„ TTL)
    - ì†ŒìŠ¤ë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜ ì œí•œ ê°€ëŠ¥
    
    íŒŒë¼ë¯¸í„°:
    - limit_per_source: ì†ŒìŠ¤ë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜ (í•„ìˆ˜)
    - apt_id: ì•„íŒŒíŠ¸ ID (ì„ íƒì )
    - keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì„ íƒì , ì˜ˆ: ["ì„œìš¸ì‹œ", "ê°•ë‚¨êµ¬", "ì—­ì‚¼ë™"])
    
    ë™ì‘ ë°©ì‹:
    - apt_idê°€ ì „ë‹¬ë˜ì§€ ì•Šê³  keywordsë„ ì—†ëŠ” ê²½ìš°: ê¸°ì¡´ ê¸°ëŠ¥ê³¼ ê°™ì´ ê°œìˆ˜ë§Œí¼ í¬ë¡¤ë§í•˜ì—¬ ë°˜í™˜
    - apt_idê°€ ì „ë‹¬ë  ê²½ìš°: 
      * apartment í…Œì´ë¸”ê³¼ state í…Œì´ë¸”ì„ ì°¸ê³ í•´ì„œ ì‹œ, ë™, ì•„íŒŒíŠ¸ ì´ë¦„ ì•Œì•„ë‚´ê¸°
      * ê²€ìƒ‰ ë‹¨ê³„ë³„ë¡œ ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰:
        1. ì‹œ + ë¶€ë™ì‚°
        2. ì‹œ + ë™ + ë¶€ë™ì‚°
        3. ì‹œ + ë™ + ì•„íŒŒíŠ¸ì´ë¦„ + ë¶€ë™ì‚°
      * ê´€ë ¨ëœ ë‰´ìŠ¤ 5ê°œ ëª©ë¡ì„ ë°˜í™˜
      * ë°˜í™˜ê°’ì— ì‹œ, ë™, ì•„íŒŒíŠ¸ ì´ë¦„ í¬í•¨ (meta í•„ë“œ)
    - keywordsê°€ ì „ë‹¬ë  ê²½ìš°:
      * apt_idê°€ ìˆì–´ë„ keywordsë¥¼ ìš°ì„  ì‚¬ìš©
      * ê° í‚¤ì›Œë“œê°€ ë‰´ìŠ¤ ì œëª©ì´ë‚˜ ë³¸ë¬¸ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
      * ì œëª©ì— í¬í•¨ëœ í‚¤ì›Œë“œëŠ” ë†’ì€ ì ìˆ˜, ë³¸ë¬¸ì— í¬í•¨ëœ í‚¤ì›Œë“œëŠ” ë‚®ì€ ì ìˆ˜
      * ê´€ë ¨ì„± ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ìµœëŒ€ 5ê°œ ë‰´ìŠ¤ ë°˜í™˜
      * ë°˜í™˜ê°’ì— keywords í¬í•¨ (meta í•„ë“œ)
    """,
    responses={
        200: {"description": "í¬ë¡¤ë§ ì™„ë£Œ"},
        404: {"description": "ì•„íŒŒíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"},
        500: {"description": "í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"}
    }
)
async def get_news(
    limit_per_source: int = Query(20, ge=1, le=100, description="ì†ŒìŠ¤ë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜"),
    apt_id: Optional[int] = Query(None, description="ì•„íŒŒíŠ¸ ID (apartments.apt_id)"),
    keywords: Optional[List[str]] = Query(None, description="ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì„ íƒì , ì˜ˆ: ['ì„œìš¸ì‹œ', 'ê°•ë‚¨êµ¬', 'ì—­ì‚¼ë™'])"),
    db: AsyncSession = Depends(get_db)
):
    """ë‰´ìŠ¤ ëª©ë¡ í¬ë¡¤ë§ ë° ì¡°íšŒ"""
    try:
        search_keywords = keywords if keywords else []  # í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        apartment_name = None
        region_name = None
        si_value = None
        dong_value = None
        
        # í‚¤ì›Œë“œê°€ ì „ë‹¬ë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ apt_idë¡œë¶€í„° ì •ë³´ ì¶”ì¶œ
        has_keywords = search_keywords and len(search_keywords) > 0
        
        # apt_idê°€ ì „ë‹¬ë˜ê³  í‚¤ì›Œë“œê°€ ì—†ëŠ” ê²½ìš°, ì•„íŒŒíŠ¸ ì •ë³´ ì¡°íšŒ
        if apt_id is not None and not has_keywords:
            # 1. Apartments í…Œì´ë¸”ì—ì„œ region_idì™€ apt_name ì°¾ê¸°
            apartment = await apartment_crud.get(db, id=apt_id)
            if not apartment or apartment.is_deleted:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail=f"ì•„íŒŒíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: apt_id={apt_id}"
                )
            
            apartment_name = apartment.apt_name
            region_id = apartment.region_id
            
            # 2. States í…Œì´ë¸”ì—ì„œ region_name ì°¾ê¸°
            state = await state_crud.get(db, id=region_id)
            if not state or state.is_deleted:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail=f"ì§€ì—­ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: region_id={region_id}"
                )
            
            region_name = state.region_name
            city_name = state.city_name
            
            # 3. ì‹œëŠ” city_nameì—ì„œ, ë™ì€ region_nameì—ì„œ ê°€ì ¸ì˜¤ê¸°
            # city_nameì„ ì‹œë¡œ ì‚¬ìš© (ì˜ˆ: "ì„œìš¸íŠ¹ë³„ì‹œ" -> "ì„œìš¸ì‹œ")
            si_value = city_name
            if si_value:
                if "íŠ¹ë³„ì‹œ" in si_value:
                    si_value = si_value.replace("íŠ¹ë³„ì‹œ", "ì‹œ")
                elif "ê´‘ì—­ì‹œ" in si_value:
                    si_value = si_value.replace("ê´‘ì—­ì‹œ", "ì‹œ")
                elif not si_value.endswith("ì‹œ"):
                    si_value = si_value + "ì‹œ"
            
            # ë™ì€ region_nameì—ì„œ "ë™"ìœ¼ë¡œ ëë‚˜ëŠ” ê²½ìš°ë§Œ ì¶”ì¶œ (ì˜ˆ: "ì ì‹¤ë™")
            if region_name and region_name.endswith("ë™"):
                dong_value = region_name
            else:
                dong_value = None
        
        # ìºì‹œ í‚¤ ìƒì„± (apt_id ë˜ëŠ” keywords í¬í•¨)
        if has_keywords:
            # í‚¤ì›Œë“œë¥¼ ì •ë ¬í•˜ì—¬ ë™ì¼í•œ í‚¤ì›Œë“œ ì¡°í•©ì— ëŒ€í•´ ê°™ì€ ìºì‹œ ì‚¬ìš©
            sorted_keywords = sorted([kw.strip() for kw in search_keywords if kw and kw.strip()])
            keywords_str = "_".join(sorted_keywords)
            cache_key = f"news_list_{limit_per_source}_keywords_{keywords_str}"
        elif apt_id is not None:
            cache_key = f"news_list_{limit_per_source}_apt_{apt_id}"
        else:
            cache_key = f"news_list_{limit_per_source}"
        
        # ìºì‹œ í™•ì¸ (Redis)
        cached_result = await get_cached_news_data(cache_key)
        if cached_result:
            # Pydantic ëª¨ë¸ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
            return NewsListResponse(**cached_result)
        
        # ìºì‹œ ì—†ìœ¼ë©´ í¬ë¡¤ë§ ì‹¤í–‰
        logger.info(f"âŒ ë‰´ìŠ¤ ìºì‹œ ë¯¸ìŠ¤ - í¬ë¡¤ë§ ì‹œì‘: {cache_key}")
        crawled_news = await news_service.crawl_only(limit_per_source=limit_per_source)
        
        # í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§, apt_idë§Œ ìˆìœ¼ë©´ ì§€ì—­ ê¸°ë°˜ í•„í„°ë§
        if has_keywords:
            # í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§
            filtered_news = filter_news_by_keywords(
                news_list=crawled_news,
                keywords=search_keywords
            )
        elif apt_id is not None:
            # apt_id ê¸°ë°˜ ì§€ì—­ í•„í„°ë§
            filtered_news = filter_news_by_location(
                news_list=crawled_news,
                si=si_value,
                dong=dong_value,
                apartment=apartment_name
            )
        else:
            # í‚¤ì›Œë“œë„ ì—†ê³  apt_idë„ ì—†ìœ¼ë©´ ê¸°ì¡´ì²˜ëŸ¼ ëª¨ë“  ë‰´ìŠ¤ ë°˜í™˜
            filtered_news = crawled_news
        
        # í¬ë¡¤ë§ ê²°ê³¼ë¥¼ NewsResponse ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜ (ê°„ë‹¨í•œ í•´ì‹œ ID ì¶”ê°€)
        from app.schemas.news import NewsResponse
        news_list = [
            NewsResponse(
                id=generate_news_id(news["url"]),
                **{k: v for k, v in news.items() if k not in ["relevance_score", "matched_category"]}
            ) for news in filtered_news
        ]
        
        # ë©”íƒ€ ì •ë³´ êµ¬ì„±
        meta = {
            "total": len(news_list),
            "limit": len(news_list),
            "offset": 0
        }
        
        # í‚¤ì›Œë“œê°€ ìˆëŠ” ê²½ìš° ë©”íƒ€ ì •ë³´ì— ì¶”ê°€
        if has_keywords:
            meta["keywords"] = search_keywords
        
        # apt_idê°€ ì „ë‹¬ëœ ê²½ìš° ë©”íƒ€ ì •ë³´ì— ì¶”ê°€ (í‚¤ì›Œë“œë³´ë‹¤ ìš°ì„ ìˆœìœ„ ë‚®ìŒ)
        if apt_id is not None:
            apt_meta = {
                "apt_id": apt_id,
                "apt_name": apartment_name
            }
            # í‚¤ì›Œë“œê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ si, dong ì¶”ê°€ (apt_idë¡œë¶€í„° ì¶”ì¶œí•œ ê°’)
            if not has_keywords:
                if si_value:
                    apt_meta["si"] = si_value
                if dong_value:
                    apt_meta["dong"] = dong_value
            meta.update(apt_meta)
        
        response = NewsListResponse(
            success=True,
            data=news_list,
            meta=meta
        )
        
        # Redis ìºì‹œì— ì €ì¥ (dictë¡œ ë³€í™˜)
        await set_cached_news_data(cache_key, response.dict())
        
        return response
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get(
    "/detail",
    response_model=NewsDetailResponse,
    status_code=status.HTTP_200_OK,
    tags=["ğŸ“° News (ë‰´ìŠ¤)"],
    summary="ë‰´ìŠ¤ ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§",
    description="""
    íŠ¹ì • ë‰´ìŠ¤ URLì˜ ìƒì„¸ ë‚´ìš©ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    
    - ë‰´ìŠ¤ ëª©ë¡ì—ì„œ ë°›ì€ `url` í•„ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„¸ ì¡°íšŒ
    - ìºì‹± ì ìš© (5ë¶„ TTL)
    - ì œëª©, ë³¸ë¬¸, ì¸ë„¤ì¼ ë“± ìƒì„¸ ì •ë³´ ë°˜í™˜
    """,
    responses={
        200: {"description": "í¬ë¡¤ë§ ì™„ë£Œ"},
        400: {"description": "ì˜ëª»ëœ URL"},
        404: {"description": "ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"},
        500: {"description": "í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"}
    }
)
async def get_news_detail_by_url(
    url: str = Query(..., description="ë‰´ìŠ¤ ìƒì„¸ í˜ì´ì§€ URL (ë‰´ìŠ¤ ëª©ë¡ì—ì„œ ë°›ì€ url í•„ë“œ ì‚¬ìš©)")
):
    """ë‰´ìŠ¤ ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§"""
    try:
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = f"detail_{generate_news_id(url)}"
        
        # Redis ìºì‹œ í™•ì¸
        cached_result = await get_cached_news_data(cache_key)
        if cached_result:
            return NewsDetailResponse(**cached_result)
        
        # ìºì‹œ ì—†ìœ¼ë©´ í¬ë¡¤ë§ ì‹¤í–‰
        logger.info(f"âŒ ë‰´ìŠ¤ ìƒì„¸ ìºì‹œ ë¯¸ìŠ¤ - í¬ë¡¤ë§ ì‹œì‘: {url}")
        detail = await news_service.crawl_news_detail(url=url)
        
        if not detail:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ í¬ë¡¤ë§ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {url}"
            )
        
        # ê°„ë‹¨í•œ í•´ì‹œ ID ìƒì„±í•˜ì—¬ ì‘ë‹µ ìƒì„±
        news_response = NewsResponse(
            id=generate_news_id(url),
            **detail
        )
        
        response = NewsDetailResponse(
            success=True,
            data=news_response
        )
        
        # Redis ìºì‹œì— ì €ì¥
        await set_cached_news_data(cache_key, response.dict())
        
        return response
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ ìƒì„¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )
