"""
ì¸êµ¬ ì´ë™ ë°ì´í„° ìˆ˜ì§‘ ì„œë¹„ìŠ¤

KOSIS í†µê³„ì²­ APIì—ì„œ ì¸êµ¬ ì´ë™ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.
"""
import logging
import sys
from typing import Dict, Any, Optional
import httpx
from datetime import datetime

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, and_

from app.models.state import State
from app.models.population_movement import PopulationMovement
from app.core.config import settings
from app.services.data_collection.base import DataCollectionServiceBase

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# í•¸ë“¤ëŸ¬ê°€ ì—†ìœ¼ë©´ ì¶”ê°€
if not logger.handlers:
    handler = logging.StreamHandler(sys.stdout)
    handler.setLevel(logging.INFO)
    formatter = logging.Formatter(
        '%(asctime)s | %(levelname)s | %(name)s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.propagate = False


class PopulationMovementCollectionService(DataCollectionServiceBase):
    """
    ì¸êµ¬ ì´ë™ ë°ì´í„° ìˆ˜ì§‘ ì„œë¹„ìŠ¤
    """

    async def collect_population_movements(
        self,
        db: AsyncSession,
        start_prd_de: str = "202401",
        end_prd_de: str = "202511"
    ) -> Dict[str, Any]:
        """
        KOSIS í†µê³„ì²­ APIì—ì„œ ì¸êµ¬ ì´ë™ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ì €ì¥
        
        Args:
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            start_prd_de: ì‹œì‘ ê¸°ê°„ (YYYYMM)
            end_prd_de: ì¢…ë£Œ ê¸°ê°„ (YYYYMM)
        
        Returns:
            ì €ì¥ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not settings.KOSIS_API_KEY:
            raise ValueError("KOSIS_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            # KOSIS API í˜¸ì¶œ
            kosis_url = "https://kosis.kr/openapi/Param/statisticsParameterData.do"
            params = {
                "method": "getList",
                "apiKey": settings.KOSIS_API_KEY,
                "itmId": "T10+T20+T25+T30+T31+T32+T40+T50+",
                "objL1": "00+11+26+27+28+29+30+31+36+41+51+43+44+52+46+47+48+50+",
                "objL2": "",
                "objL3": "",
                "objL4": "",
                "objL5": "",
                "objL6": "",
                "objL7": "",
                "objL8": "",
                "format": "json",
                "jsonVD": "Y",
                "prdSe": "M",
                "startPrdDe": start_prd_de,
                "endPrdDe": end_prd_de,
                "orgId": "101",
                "tblId": "DT_1B26001_A01"
            }
            
            # API URLê³¼ íŒŒë¼ë¯¸í„° ë¡œê·¸ ì¶œë ¥ (ë¯¼ê° ì •ë³´ ì œì™¸)
            safe_params = {k: (v if k != "apiKey" else "***") for k, v in params.items()}
            logger.info(f"ğŸ“¡ KOSIS API í˜¸ì¶œ ì‹œì‘: {start_prd_de} ~ {end_prd_de}")
            logger.info(f"   ğŸŒ API URL: {kosis_url}")
            logger.info(f"   ğŸ“‹ API íŒŒë¼ë¯¸í„°: {safe_params}")
            
            async with httpx.AsyncClient(timeout=60.0) as client:
                # ì‹¤ì œ í˜¸ì¶œë  URL ìƒì„± (ë””ë²„ê¹…ìš©)
                from urllib.parse import urlencode
                actual_url = f"{kosis_url}?{urlencode(params)}"
                logger.info(f"   ğŸ”— ì‹¤ì œ API í˜¸ì¶œ URL: {actual_url[:200]}...")  # URLì´ ê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²˜ìŒ 200ìë§Œ
                
                response = await client.get(kosis_url, params=params)
                
                # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸
                logger.info(f"   ğŸ“Š HTTP ì‘ë‹µ ìƒíƒœ: {response.status_code}")
                
                response.raise_for_status()
                raw_data = response.json()
                
                # ì‘ë‹µ ë‚´ìš© í™•ì¸
                logger.info(f"   ğŸ“¦ ì‘ë‹µ ë°ì´í„° íƒ€ì…: {type(raw_data)}")
            
            # KOSIS API ì‘ë‹µ êµ¬ì¡° ì²˜ë¦¬
            # ì‘ë‹µì´ dictì¸ ê²½ìš° ë‚´ë¶€ì—ì„œ ë¦¬ìŠ¤íŠ¸ ì°¾ê¸°
            if isinstance(raw_data, dict):
                logger.info(f"   ğŸ“‹ API ì‘ë‹µ íƒ€ì…: dict, í‚¤ ëª©ë¡: {list(raw_data.keys())}")
                
                # ì˜¤ë¥˜ ì‘ë‹µ í™•ì¸
                if "err" in raw_data or "errMsg" in raw_data:
                    err_code = raw_data.get("err", "N/A")
                    err_msg = raw_data.get("errMsg", "N/A")
                    logger.error(f"   âŒ KOSIS API ì˜¤ë¥˜ ì‘ë‹µ: err={err_code}, errMsg={err_msg}")
                    raise ValueError(f"KOSIS API ì˜¤ë¥˜: {err_code} - {err_msg}")
                
                # ë‹¤ì–‘í•œ ê°€ëŠ¥í•œ í‚¤ ì‹œë„
                if "StatisticSearch" in raw_data:
                    stat_search = raw_data["StatisticSearch"]
                    if isinstance(stat_search, dict):
                        data = stat_search.get("row", [])
                    elif isinstance(stat_search, list):
                        data = stat_search
                    else:
                        data = []
                elif "row" in raw_data:
                    data = raw_data["row"] if isinstance(raw_data["row"], list) else []
                elif "data" in raw_data:
                    data = raw_data["data"] if isinstance(raw_data["data"], list) else []
                elif "list" in raw_data:
                    data = raw_data["list"] if isinstance(raw_data["list"], list) else []
                elif len(raw_data) == 1:
                    # ë”•ì…”ë„ˆë¦¬ì— ê°’ì´ í•˜ë‚˜ì¸ ê²½ìš° ê·¸ ê°’ì„ ì‹œë„
                    first_value = list(raw_data.values())[0]
                    if isinstance(first_value, list):
                        data = first_value
                    elif isinstance(first_value, dict) and "row" in first_value:
                        data = first_value["row"] if isinstance(first_value["row"], list) else []
                    else:
                        data = []
                else:
                    # dictì˜ ëª¨ë“  ê°’ì´ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
                    list_values = [v for v in raw_data.values() if isinstance(v, list)]
                    if list_values:
                        # ì²« ë²ˆì§¸ ë¦¬ìŠ¤íŠ¸ ê°’ ì‚¬ìš©
                        data = list_values[0]
                    else:
                        logger.warning(f"   âš ï¸ dict ì‘ë‹µì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ, ëª¨ë“  ê°’: {list(raw_data.keys())}")
                        # ë””ë²„ê¹…: raw_dataì˜ ì¼ë¶€ ì¶œë ¥
                        logger.debug(f"   ğŸ” raw_data ë‚´ìš© ìƒ˜í”Œ: {str(raw_data)[:500]}")
                        data = []
            elif isinstance(raw_data, list):
                data = raw_data
            else:
                logger.warning(f"   âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ íƒ€ì…: {type(raw_data)}")
                data = []
            
            data_count = len(data) if isinstance(data, list) else 0
            logger.info(f"âœ… KOSIS API í˜¸ì¶œ ì„±ê³µ: {data_count}ê±´ì˜ ë°ì´í„° ìˆ˜ì‹ ")
            
            # ë°ì´í„° íƒ€ì… ë° ìƒ˜í”Œ í™•ì¸
            if isinstance(data, list) and len(data) > 0:
                sample_item = data[0]
                logger.info(f"   ğŸ“Š ë°ì´í„° ìƒ˜í”Œ: C1={sample_item.get('C1')}, C1_NM={sample_item.get('C1_NM')}, ITM_ID={sample_item.get('ITM_ID')}, PRD_DE={sample_item.get('PRD_DE')}")
            else:
                logger.warning(f"   âš ï¸ ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆê±°ë‚˜ ë¹„ì–´ìˆìŒ: type={type(data)}, len={len(data) if isinstance(data, list) else 'N/A'}")
            
            # ë°ì´í„° íŒŒì‹± ë° ì €ì¥
            saved_count = 0
            updated_count = 0
            
            # C1 ì½”ë“œì™€ ì§€ì—­ëª… ë§¤í•‘ (KOSIS ì§€ì—­ ì½”ë“œ)
            # 00=ì „êµ­, 11=ì„œìš¸, 26=ë¶€ì‚°, 27=ëŒ€êµ¬, 28=ì¸ì²œ, 29=ê´‘ì£¼, 30=ëŒ€ì „, 31=ìš¸ì‚°
            # 36=ì„¸ì¢…, 41=ê²½ê¸°, 51=ê°•ì›, 43=ì¶©ë¶, 44=ì¶©ë‚¨, 52=ì „ë¶, 46=ì „ë‚¨, 47=ê²½ë¶, 48=ê²½ë‚¨, 50=ì œì£¼
            
            # ì‹œë„ ë ˆë²¨ ë°ì´í„°ë§Œ ì €ì¥: ê° ì‹œë„ë³„ë¡œ ëŒ€í‘œ region_id í•˜ë‚˜ë§Œ ì„ íƒ
            city_map = {
                "ì„œìš¸íŠ¹ë³„ì‹œ": "11", "ë¶€ì‚°ê´‘ì—­ì‹œ": "26", "ëŒ€êµ¬ê´‘ì—­ì‹œ": "27", "ì¸ì²œê´‘ì—­ì‹œ": "28",
                "ê´‘ì£¼ê´‘ì—­ì‹œ": "29", "ëŒ€ì „ê´‘ì—­ì‹œ": "30", "ìš¸ì‚°ê´‘ì—­ì‹œ": "31", "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ": "36",
                "ê²½ê¸°ë„": "41", "ê°•ì›íŠ¹ë³„ìì¹˜ë„": "51", "ì¶©ì²­ë¶ë„": "43", "ì¶©ì²­ë‚¨ë„": "44",
                "ì „ë¶íŠ¹ë³„ìì¹˜ë„": "52", "ì „ë¼ë‚¨ë„": "46", "ê²½ìƒë¶ë„": "47", "ê²½ìƒë‚¨ë„": "48", "ì œì£¼íŠ¹ë³„ìì¹˜ë„": "50"
            }
            
            # ê° ì‹œë„ë³„ë¡œ ì²« ë²ˆì§¸ region_idë§Œ ì„ íƒ (ì‹œë„ ë ˆë²¨ ì§‘ê³„ìš©)
            states_result = await db.execute(select(State).where(State.is_deleted == False))
            states_list = states_result.scalars().all()
            
            logger.info(f"   ğŸ“ DBì—ì„œ ì¡°íšŒëœ ì§€ì—­ ìˆ˜: {len(states_list)}ê°œ")
            
            # ê° ì‹œë„ë³„ë¡œ ê°€ì¥ ì‘ì€ region_id í•˜ë‚˜ë§Œ ì„ íƒ
            region_code_map: Dict[str, List[int]] = {}
            for state in states_list:
                if state.city_name in city_map:
                    code = city_map[state.city_name]
                    if code not in region_code_map:
                        # ê° ì‹œë„ë³„ë¡œ ì²« ë²ˆì§¸ region_idë§Œ ì €ì¥ (ì‹œë„ ë ˆë²¨ ì§‘ê³„)
                        region_code_map[code] = [state.region_id]
            
            logger.info(f"   ğŸ”— ì‹œë„ ë ˆë²¨ ë§¤í•‘ ìƒì„±: {len(region_code_map)}ê°œ ì‹œë„ (ê° ì‹œë„ë‹¹ 1ê°œ region_id)")
            for code, region_ids in sorted(region_code_map.items()):
                logger.info(f"      C1={code}: region_id={region_ids[0]} (ì‹œë„ ë ˆë²¨ ì§‘ê³„)")
            
            # ë°ì´í„°ë¥¼ PRD_DE(ê¸°ê°„)ì™€ C1(ì§€ì—­)ë³„ë¡œ ê·¸ë£¹í™”
            grouped_data: Dict[str, Dict[str, Dict[str, int]]] = {}  # {PRD_DE: {C1: {ITM_ID: DT}}}
            c1_codes_in_data = set()
            prd_des_in_data = set()
            
            if isinstance(data, list):
                processed_count = 0
                for item in data:
                    prd_de = item.get("PRD_DE", "")
                    c1 = item.get("C1", "")
                    itm_id = item.get("ITM_ID", "")
                    dt_str = item.get("DT", "0")
                    
                    if c1:
                        c1_codes_in_data.add(c1)
                    if prd_de:
                        prd_des_in_data.add(prd_de)
                    
                    try:
                        dt_value = int(dt_str) if dt_str else 0
                    except (ValueError, TypeError):
                        dt_value = 0
                    
                    if prd_de and c1 and itm_id:
                        if prd_de not in grouped_data:
                            grouped_data[prd_de] = {}
                        if c1 not in grouped_data[prd_de]:
                            grouped_data[prd_de][c1] = {}
                        grouped_data[prd_de][c1][itm_id] = dt_value
                        processed_count += 1
                
                logger.info(f"   ğŸ“¦ ë°ì´í„° ê·¸ë£¹í™” ì™„ë£Œ: {processed_count}ê±´ ì²˜ë¦¬, {len(prd_des_in_data)}ê°œ ê¸°ê°„, {len(c1_codes_in_data)}ê°œ ì§€ì—­ ì½”ë“œ")
                logger.info(f"      ì§€ì—­ ì½”ë“œ ëª©ë¡: {sorted(c1_codes_in_data)}")
            else:
                logger.warning(f"   âš ï¸ ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜: type={type(data)}")
            
            # ê¸°ì¡´ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¡°íšŒ (ì„±ëŠ¥ ìµœì í™”)
            logger.info(f"   ğŸ” ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ ì¤‘...")
            existing_result = await db.execute(
                select(PopulationMovement).where(
                    PopulationMovement.is_deleted == False
                )
            )
            existing_movements = existing_result.scalars().all()
            
            # ê¸°ì¡´ ë°ì´í„°ë¥¼ (region_id, base_ym) íŠœí”Œì„ í‚¤ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            existing_map: Dict[tuple, PopulationMovement] = {}
            for movement in existing_movements:
                key = (movement.region_id, movement.base_ym)
                existing_map[key] = movement
            
            logger.info(f"   ğŸ“‹ ê¸°ì¡´ ë°ì´í„° {len(existing_map)}ê±´ ì¡°íšŒ ì™„ë£Œ")
            
            # ê° ì§€ì—­ë³„ë¡œ ë°ì´í„° ì €ì¥ (C1="00" ì „êµ­ ë°ì´í„°ëŠ” ì œì™¸)
            matched_regions_count = 0
            skipped_no_match_count = 0
            total_operations = 0
            
            # ì§„í–‰ ìƒí™© ì¶”ì 
            total_prd_de_count = len([prd_de for prd_de, regions in grouped_data.items() if any(c1 != "00" for c1 in regions.keys())])
            processed_prd_de_count = 0
            
            for prd_de, regions in grouped_data.items():
                processed_prd_de_count += 1
                if processed_prd_de_count % 10 == 0 or processed_prd_de_count == total_prd_de_count:
                    logger.info(f"   â³ ì§„í–‰ ì¤‘: {processed_prd_de_count}/{total_prd_de_count} ê¸°ê°„ ì²˜ë¦¬ ì¤‘... (í˜„ì¬: {prd_de})")
                
                for c1_code, items in regions.items():
                    # C1="00" (ì „êµ­) ë°ì´í„°ëŠ” ìŠ¤í‚µ
                    if c1_code == "00":
                        continue
                    
                    # ìˆœì´ë™ ê³„ì‚°: T25(ìˆœì´ë™)ë¥¼ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ T10(ì´ì „ì…) - T20(ì´ì „ì¶œ)
                    # T25=ìˆœì´ë™, T10=ì´ì „ì…, T20=ì´ì „ì¶œ
                    if "T25" in items:
                        # T25ê°€ ìˆìœ¼ë©´ ì§ì ‘ ì‚¬ìš©
                        net_migration = items.get("T25", 0)
                        # ì „ì…/ì „ì¶œì€ T10, T20 ì‚¬ìš© (ì—†ìœ¼ë©´ 0)
                        in_migration = items.get("T10", 0)
                        out_migration = items.get("T20", 0)
                    else:
                        # T25ê°€ ì—†ìœ¼ë©´ ê³„ì‚°
                        in_migration = items.get("T10", 0)
                        out_migration = items.get("T20", 0)
                        net_migration = in_migration - out_migration
                    
                    # í•´ë‹¹ ì§€ì—­ ì½”ë“œì— ë§¤í•‘ëœ region_idë“¤ ì°¾ê¸°
                    if c1_code in region_code_map:
                        matched_regions_count += len(region_code_map[c1_code])
                        for region_id in region_code_map[c1_code]:
                            total_operations += 1
                            key = (region_id, prd_de)
                            
                            if key in existing_map:
                                # ì—…ë°ì´íŠ¸
                                existing_data = existing_map[key]
                                existing_data.in_migration = in_migration
                                existing_data.out_migration = out_migration
                                existing_data.net_migration = net_migration
                                existing_data.updated_at = datetime.utcnow()
                                updated_count += 1
                            else:
                                # ìƒˆë¡œ ìƒì„±
                                new_movement = PopulationMovement(
                                    region_id=region_id,
                                    base_ym=prd_de,
                                    in_migration=in_migration,
                                    out_migration=out_migration,
                                    net_migration=net_migration,
                                    movement_type="TOTAL",
                                    data_source="KOSIS"
                                )
                                db.add(new_movement)
                                saved_count += 1
                    else:
                        skipped_no_match_count += 1
            
            logger.info(f"   ğŸ’¾ ì €ì¥ ì¤€ë¹„ ì™„ë£Œ: ì´ {total_operations}ê°œ ì‘ì—… (ì‹ ê·œ {saved_count}ê±´, ì—…ë°ì´íŠ¸ {updated_count}ê±´ ì˜ˆì •)")
            logger.info(f"   ğŸ“Š ë§¤ì¹­ í†µê³„: ë§¤ì¹­ëœ ì§€ì—­ {matched_regions_count}ê°œ, ë§¤í•‘ ì‹¤íŒ¨ {skipped_no_match_count}ê°œ")
            
            await db.commit()
            
            logger.info(f"âœ… ì¸êµ¬ ì´ë™ ë°ì´í„° ì €ì¥ ì™„ë£Œ: ì‹ ê·œ {saved_count}ê±´, ì—…ë°ì´íŠ¸ {updated_count}ê±´")
            
            return {
                "success": True,
                "message": f"ì¸êµ¬ ì´ë™ ë°ì´í„° ì €ì¥ ì™„ë£Œ: ì‹ ê·œ {saved_count}ê±´, ì—…ë°ì´íŠ¸ {updated_count}ê±´",
                "saved_count": saved_count,
                "updated_count": updated_count,
                "period": f"{start_prd_de} ~ {end_prd_de}"
            }
            
        except Exception as e:
            await db.rollback()
            logger.error(f"âŒ ì¸êµ¬ ì´ë™ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {str(e)}", exc_info=True)
            raise

    async def collect_population_movement_matrix(
        self,
        db: AsyncSession,
        start_prd_de: str = "202401",
        end_prd_de: str = "202511"
    ) -> Dict[str, Any]:
        """
        KOSIS í†µê³„ì²­ APIì—ì„œ ì¸êµ¬ ì´ë™ ë§¤íŠ¸ë¦­ìŠ¤(ì¶œë°œì§€->ë„ì°©ì§€) ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ì €ì¥
        
        Args:
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            start_prd_de: ì‹œì‘ ê¸°ê°„ (YYYYMM)
            end_prd_de: ì¢…ë£Œ ê¸°ê°„ (YYYYMM)
        
        Returns:
            ì €ì¥ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not settings.KOSIS_API_KEY:
            raise ValueError("KOSIS_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        from app.models.population_movement_matrix import PopulationMovementMatrix
        
        try:
            # KOSIS API í˜¸ì¶œ
            kosis_url = "https://kosis.kr/openapi/Param/statisticsParameterData.do"
            params = {
                "method": "getList",
                "apiKey": settings.KOSIS_API_KEY,
                "itmId": "T70+",  # ì´ë™ììˆ˜
                "objL1": "ALL",   # ì „ì¶œì§€ (Source)
                "objL2": "ALL",   # ì „ì…ì§€ (Target)
                "objL3": "",
                "objL4": "",
                "objL5": "",
                "objL6": "",
                "objL7": "",
                "objL8": "",
                "format": "json",
                "jsonVD": "Y",
                "prdSe": "M",     # ì›”ë³„
                "startPrdDe": start_prd_de,
                "endPrdDe": end_prd_de,
                "orgId": "101",
                "tblId": "DT_1B26003_A01" # ì „ì¶œì§€/ì „ì…ì§€(ì‹œë„)ë³„ ì´ë™ììˆ˜
            }
            
            # API URLê³¼ íŒŒë¼ë¯¸í„° ë¡œê·¸ ì¶œë ¥ (ë¯¼ê° ì •ë³´ ì œì™¸)
            safe_params = {k: (v if k != "apiKey" else "***") for k, v in params.items()}
            logger.info(f"ğŸ“¡ KOSIS Matrix API í˜¸ì¶œ ì‹œì‘: {start_prd_de} ~ {end_prd_de}")
            logger.info(f"   ğŸ“‹ API íŒŒë¼ë¯¸í„°: {safe_params}")
            
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.get(kosis_url, params=params)
                logger.info(f"   ğŸ“Š HTTP ì‘ë‹µ ìƒíƒœ: {response.status_code}")
                response.raise_for_status()
                raw_data = response.json()
            
            # ë°ì´í„° íŒŒì‹±
            data = []
            if isinstance(raw_data, list):
                data = raw_data
            elif isinstance(raw_data, dict):
                # êµ¬ì¡°ì— ë”°ë¼ ë°ì´í„° ì¶”ì¶œ (ì´ì „ ë©”ì„œë“œì™€ ìœ ì‚¬í•œ ë¡œì§)
                if "StatisticSearch" in raw_data and "row" in raw_data["StatisticSearch"]:
                    data = raw_data["StatisticSearch"]["row"]
                elif "data" in raw_data:
                    data = raw_data["data"]
            
            if not isinstance(data, list):
                logger.warning(f"   âš ï¸ ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜: type={type(data)}")
                data = []
                
            logger.info(f"âœ… KOSIS Matrix API í˜¸ì¶œ ì„±ê³µ: {len(data)}ê±´ì˜ ë°ì´í„° ìˆ˜ì‹ ")
            
            # C1(ì „ì¶œì§€), C2(ì „ì…ì§€) ì½”ë“œ ë§¤í•‘
            # KOSIS ì½”ë“œ -> Region ID (State í…Œì´ë¸”)
            # 00=ì „êµ­, 11=ì„œìš¸, 26=ë¶€ì‚°, 27=ëŒ€êµ¬, 28=ì¸ì²œ, 29=ê´‘ì£¼, 30=ëŒ€ì „, 31=ìš¸ì‚°
            # 36=ì„¸ì¢…, 41=ê²½ê¸°, 51=ê°•ì›, 43=ì¶©ë¶, 44=ì¶©ë‚¨, 52=ì „ë¶, 46=ì „ë‚¨, 47=ê²½ë¶, 48=ê²½ë‚¨, 50=ì œì£¼
            
            kosis_city_map = {
                "11": "ì„œìš¸íŠ¹ë³„ì‹œ", "26": "ë¶€ì‚°ê´‘ì—­ì‹œ", "27": "ëŒ€êµ¬ê´‘ì—­ì‹œ", "28": "ì¸ì²œê´‘ì—­ì‹œ",
                "29": "ê´‘ì£¼ê´‘ì—­ì‹œ", "30": "ëŒ€ì „ê´‘ì—­ì‹œ", "31": "ìš¸ì‚°ê´‘ì—­ì‹œ", "36": "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ",
                "41": "ê²½ê¸°ë„", "51": "ê°•ì›íŠ¹ë³„ìì¹˜ë„", "42": "ê°•ì›íŠ¹ë³„ìì¹˜ë„", # 42ëŠ” êµ¬ ì½”ë“œì¼ ìˆ˜ ìˆìŒ
                "43": "ì¶©ì²­ë¶ë„", "44": "ì¶©ì²­ë‚¨ë„", "52": "ì „ë¶íŠ¹ë³„ìì¹˜ë„", "45": "ì „ë¶íŠ¹ë³„ìì¹˜ë„", # 45ëŠ” êµ¬ ì½”ë“œ
                "46": "ì „ë¼ë‚¨ë„", "47": "ê²½ìƒë¶ë„", "48": "ê²½ìƒë‚¨ë„", "50": "ì œì£¼íŠ¹ë³„ìì¹˜ë„"
            }
            
            # DBì—ì„œ State ì •ë³´ ë¡œë“œ to get region_id
            states_result = await db.execute(select(State).where(State.is_deleted == False))
            states_list = states_result.scalars().all()
            
            # City Name -> Region ID (Representative)
            city_to_region_id = {}
            for state in states_list:
                if state.city_name not in city_to_region_id:
                    city_to_region_id[state.city_name] = state.region_id
            
            # KOSIS Code -> Region ID
            code_to_region_id = {}
            for code, city_name in kosis_city_map.items():
                if city_name in city_to_region_id:
                    code_to_region_id[code] = city_to_region_id[city_name]
            
            logger.info(f"   ğŸ”— ì§€ì—­ ë§¤í•‘ ì¤€ë¹„ ì™„ë£Œ: {len(code_to_region_id)}ê°œ ì½”ë“œ ë§¤í•‘")

            # ë°ì´í„° ì²˜ë¦¬ ë° ì €ì¥
            saved_count = 0
            updated_count = 0
            skipped_count = 0
            
            # ê¸°ì¡´ ë°ì´í„° ì¡°íšŒë¥¼ ìœ„í•œ í‚¤ ì…‹ ì¤€ë¹„ (Batch Updateë¥¼ ìœ„í•¨)
            # ë³µí•©í‚¤: (base_ym, from_region_id, to_region_id)
            
            processed_data = []
            
            for item in data:
                prd_de = item.get("PRD_DE")
                c1 = item.get("C1") # ì „ì¶œì§€
                c2 = item.get("C2") # ì „ì…ì§€
                dt = item.get("DT") # ì´ë™ììˆ˜
                
                # ì „êµ­(00) ë°ì´í„°ëŠ” ì œì™¸ (ìˆœìˆ˜ ì§€ì—­ê°„ ì´ë™ë§Œ)
                if c1 == "00" or c2 == "00":
                    continue
                
                # ë™ì¼ ì§€ì—­ ì´ë™ ì œì™¸ (ì˜µì…˜, ì¼ë‹¨ í¬í•¨í•  ìˆ˜ë„ ìˆìœ¼ë‚˜ Sankeyì—ì„œëŠ” ë³´í†µ ì œì™¸í•˜ê±°ë‚˜ Loopë¡œ í‘œì‹œ)
                # ì‚¬ìš©ìê°€ "ì§€ì—­ë³„ êµ¬ë³„ë˜ëŠ” ìƒ‰"ì„ ì›í•˜ë¯€ë¡œ íƒ€ ì§€ì—­ ì´ë™ì´ ì¤‘ìš”
                
                if c1 in code_to_region_id and c2 in code_to_region_id:
                    from_id = code_to_region_id[c1]
                    to_id = code_to_region_id[c2]
                    try:
                        count = int(dt)
                    except:
                        count = 0
                    
                    processed_data.append({
                        "base_ym": prd_de,
                        "from_region_id": from_id,
                        "to_region_id": to_id,
                        "movement_count": count
                    })
                else:
                    skipped_count += 1

            logger.info(f"   ğŸ“¦ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {len(processed_data)}ê±´ ìœ íš¨, {skipped_count}ê±´ ìŠ¤í‚µ")

            # Upsert Logic (Delete existing for the period then Insert, or Check and Update)
            # Considering volume, deleting for the specific months and re-inserting might be cleaner 
            # but let's try to update individually or bulk insert if empty.
            
            # For simplicity and robustness, let's use merge (upsert) logic
            for row in processed_data:
                # Check exist
                stmt = select(PopulationMovementMatrix).where(
                    and_(
                        PopulationMovementMatrix.base_ym == row["base_ym"],
                        PopulationMovementMatrix.from_region_id == row["from_region_id"],
                        PopulationMovementMatrix.to_region_id == row["to_region_id"]
                    )
                )
                result = await db.execute(stmt)
                existing = result.scalar_one_or_none()
                
                if existing:
                    existing.movement_count = row["movement_count"]
                    existing.updated_at = datetime.utcnow()
                    updated_count += 1
                else:
                    new_matrix = PopulationMovementMatrix(
                        base_ym=row["base_ym"],
                        from_region_id=row["from_region_id"],
                        to_region_id=row["to_region_id"],
                        movement_count=row["movement_count"]
                    )
                    db.add(new_matrix)
                    saved_count += 1
            
            await db.commit()
            
            logger.info(f"âœ… ì¸êµ¬ ì´ë™ ë§¤íŠ¸ë¦­ìŠ¤ ì €ì¥ ì™„ë£Œ: ì‹ ê·œ {saved_count}ê±´, ì—…ë°ì´íŠ¸ {updated_count}ê±´")
            
            return {
                "success": True,
                "saved_count": saved_count,
                "updated_count": updated_count
            }
            
        except Exception as e:
            await db.rollback()
            logger.error(f"âŒ ì¸êµ¬ ì´ë™ ë§¤íŠ¸ë¦­ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {str(e)}", exc_info=True)
            raise
